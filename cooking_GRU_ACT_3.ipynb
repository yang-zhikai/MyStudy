{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cooking_GRU_ACT 3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yang-zhikai/MyStudy/blob/master/cooking_GRU_ACT_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X933XOt5z5nZ"
      },
      "source": [
        " pip install ipdb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD8OCd6KyMrl"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wT4zQDNnTrKq"
      },
      "source": [
        "!/opt/bin/nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L--KoDwLewGF"
      },
      "source": [
        "#处理工具"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wpnf5of0fiq"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: UTF-8 -*-\n",
        "################################################################################\n",
        "# Project:  Extracting Action Sequences Based on Deep Reinforcement Learning\n",
        "# Module:   utils\n",
        "# Author:   Wenfeng Feng \n",
        "# Time:     2017.12\n",
        "################################################################################\n",
        "\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg') # do not require GUI\n",
        "from ipdb import set_trace\n",
        "\n",
        "\n",
        "def str2bool(v):\n",
        "    \"\"\"\n",
        "    Transfrom string command/argument to bool\n",
        "    \"\"\"\n",
        "    return v.lower() in (\"yes\", \"true\", \"t\", \"1\")\n",
        "\n",
        "\n",
        "def timeit(f):\n",
        "    \"\"\"\n",
        "    Return time cost of function f\n",
        "    \"\"\"\n",
        "    def timed(*args, **kwargs):\n",
        "        start_time = time.time()\n",
        "        result = f(*args, **kwargs)\n",
        "        end_time = time.time()\n",
        "\n",
        "        print(\"   [-] %s : %2.5f sec\" % (f.__name__, end_time - start_time))\n",
        "        return result\n",
        "    return timed\n",
        "\n",
        "\n",
        "def get_time():\n",
        "    \"\"\"\n",
        "    Get global time as string\n",
        "    \"\"\"\n",
        "    return time.strftime(\"%Y-%m-%d_%H:%M:%S\", time.gmtime())\n",
        "\n",
        "\n",
        "def save_pkl(obj, path):\n",
        "    \"\"\"\n",
        "    Save pickle file\n",
        "    \"\"\"\n",
        "    with open(path, 'wb') as f:\n",
        "        pickle.dump(obj, f)\n",
        "\n",
        "\n",
        "def load_pkl(path):\n",
        "    \"\"\"\n",
        "    Load pickle file\n",
        "    \"\"\"\n",
        "    with open(path, 'rb') as f:\n",
        "        obj = pickle.load(f)\n",
        "        return obj\n",
        "\n",
        "\n",
        "def print_args(args, output_file=''):\n",
        "    \"\"\"\n",
        "    Print all arguments of an argparse instance\n",
        "    \"\"\"\n",
        "    print('\\n Arguments:')\n",
        "    for k, v in sorted(args.__dict__.items(), key=lambda x:x[0]):\n",
        "        print('{}: {}'.format(k, v))\n",
        "    if output_file:\n",
        "        output_file.write('\\n Arguments:\\n')\n",
        "        for k, v in sorted(args.__dict__.items(), key=lambda x:x[0]):\n",
        "            output_file.write('{}: {}\\n'.format(k, v))\n",
        "        output_file.write('\\n')\n",
        "\n",
        "\n",
        "def pos_tagging(agent_mode, domain):\n",
        "    \"\"\"\n",
        "    Part-of-speech tagging, using StanfordPOSTagger\n",
        "    \"\"\"\n",
        "    # import ipdb\n",
        "    from tqdm import tqdm\n",
        "    from nltk.tag import StanfordPOSTagger\n",
        "    # NB! you should download the Stanford postagger and set the following two directories\n",
        "    # e.g. pos_model = 'stanford/postagger/models/english-bidirectional-distsim.tagger'\n",
        "    # pos_jar = 'stanford/postagger/stanford-postagger.jar'\n",
        "    pos_model = ''\n",
        "    pos_jar = ''\n",
        "    pos_tagger = StanfordPOSTagger(pos_model, pos_jar)\n",
        "    \n",
        "    if agent_mode == 'act':\n",
        "        indata = pickle.load(open('/content/drive/My Drive/domain.pkl' % domain, 'r'))\n",
        "    else:\n",
        "        _, __, indata = pickle.load(open('/content/drive/My Drive/EASDRL/data/refined_%s_data.pkl' % domain, 'r'))\n",
        "\n",
        "    # ipdb.set_trace()\n",
        "    # pos_data = []\n",
        "    pos_data = load_pkl('/content/drive/My Drive/EASDRL/data/%s_%s_pos.pkl' % (domain, agent_mode))\n",
        "    try:\n",
        "        for i in range(118, len(indata)):\n",
        "            pos_text = []\n",
        "            for j in range(len(indata[i])):\n",
        "                print('Text %d/%d Sent %d/%d' % (i+1, len(indata), j+1, len(indata[i])))\n",
        "                if len(indata[i][j]) == 0:\n",
        "                    continue\n",
        "                last_sent = [w.lower() for w in indata[i][j]['last_sent']]\n",
        "                this_sent = [w.lower() for w in indata[i][j]['this_sent']]\n",
        "                pos_sent = [pos_tagger.tag(last_sent), pos_tagger.tag(this_sent)]\n",
        "                pos_text.append(pos_sent)\n",
        "            pos_data.append(pos_text)\n",
        "    except:\n",
        "        print('Error! i=%d, j=%d' % (i, j))\n",
        "    save_pkl(pos_data, 'data/%s_%s_pos.pkl' % (domain, agent_mode))\n",
        "\n",
        "def transfer(infile,output_file):\n",
        "    indata = load_pkl(infile)[-1]\n",
        "    data = []\n",
        "    log = {'wrong_last_sent': 0, 'act_reference_1': 0, 'related_act_reference_1': 0,\n",
        "            'obj_reference_1': 0, 'non-obj_reference_1': 0}\n",
        "    for i in range(len(indata)):\n",
        "        words = []  # all words of a text\n",
        "        sents = []  # all sentences of a text\n",
        "        word2sent = {}  # transfer from a word index to its sentence index\n",
        "        text_acts = []  # all actions of a text\n",
        "        sent_acts = []  # actions of each sentence\n",
        "        reference_related_acts = False\n",
        "        for j in range(len(indata[i])):\n",
        "            # labeling error: empty sentence\n",
        "            if len(indata[i][j]) == 0:  \n",
        "                print('%s, len(indata[%d][%d]) == 0'%(self.domain, i, j))\n",
        "                continue\n",
        "            last_sent = indata[i][j]['last_sent']\n",
        "            this_sent = indata[i][j]['this_sent']\n",
        "            acts = indata[i][j]['acts']\n",
        "            \n",
        "            # labeling error: mis-matched sentences\n",
        "            if j > 0 and len(last_sent) != len(indata[i][j-1]['this_sent']):\n",
        "                b1 = len(last_sent)\n",
        "                b2 = len(indata[i][j-1]['this_sent'])\n",
        "                for k in range(len(acts)):\n",
        "                    ai = acts[k]['act_idx']\n",
        "                    new_act_type = acts[k]['act_type']\n",
        "                    new_act_idx = ai - b1 + b2\n",
        "                    new_obj_idxs = [[],[]]\n",
        "                    for l in range(2):\n",
        "                        for oi in acts[k]['obj_idxs'][l]:\n",
        "                            if oi == -1:\n",
        "                                new_obj_idxs[l].append(oi)\n",
        "                            else:\n",
        "                                new_obj_idxs[l].append(oi - b1 + b2)\n",
        "                        assert len(new_obj_idxs[l]) == len(acts[k]['obj_idxs'][l])\n",
        "                    new_related_acts = []\n",
        "                    acts[k] = {'act_idx': new_act_idx, 'obj_idxs': new_obj_idxs,\n",
        "                            'act_type': new_act_type, 'related_acts': new_related_acts}\n",
        "                last_sent = indata[i][j-1]['this_sent']\n",
        "                log['wrong_last_sent'] += 1\n",
        "\n",
        "            sent = last_sent + this_sent\n",
        "            last_sent_bias = len(last_sent)\n",
        "            # pronoun resolution, find the source noun of a pronoun\n",
        "            reference_obj_flag = False  \n",
        "            tmp_acts = []\n",
        "            for k in range(len(acts)):\n",
        "                act_idx = acts[k]['act_idx']\n",
        "                obj_idxs = acts[k]['obj_idxs']\n",
        "                tmp_act_idx = act_idx - last_sent_bias\n",
        "                if tmp_act_idx < 0:\n",
        "                    log['act_reference_1'] += 1\n",
        "                \n",
        "                tmp_obj_idxs = [[],[]]\n",
        "                for l in range(2):\n",
        "                    for oi in obj_idxs[l]:\n",
        "                        if oi == -1:\n",
        "                            tmp_obj_idxs[l].append(oi)\n",
        "                        else:\n",
        "                            tmp_obj_idxs[l].append(oi - last_sent_bias)\n",
        "                            if oi - last_sent_bias < 0:\n",
        "                                reference_obj_flag = True\n",
        "                    assert len(tmp_obj_idxs[l]) == len(obj_idxs[l])\n",
        "                tmp_act_type = acts[k]['act_type']\n",
        "                tmp_related_acts = []\n",
        "                if len(acts[k]['related_acts']) > 0:\n",
        "                    for idx in acts[k]['related_acts']:\n",
        "                        tmp_related_acts.append(idx - last_sent_bias)\n",
        "                        if idx - last_sent_bias < 0:\n",
        "                            reference_related_acts = True\n",
        "                            log['related_act_reference_1'] += 1\n",
        "                    assert len(tmp_related_acts) == len(acts[k]['related_acts'])\n",
        "                tmp_acts.append({'act_idx': tmp_act_idx, 'obj_idxs': tmp_obj_idxs,\n",
        "                            'act_type': tmp_act_type, 'related_acts': tmp_related_acts})\n",
        "            # assert len(tmp_acts) == len(acts)\n",
        "            # labeling error: wrong word index in the first sentence \n",
        "            if j == 0:\n",
        "                if reference_obj_flag:\n",
        "                    log['obj_reference_1'] += 1\n",
        "                    for ii in range(len(words), len(words)+len(last_sent)):\n",
        "                        word2sent[ii] = len(sents)\n",
        "                    words.extend(last_sent)\n",
        "                    sents.append(last_sent)\n",
        "                    sent_acts.append({})\n",
        "                else:\n",
        "                    if len(last_sent) > 0:\n",
        "                        log['non-obj_reference_1'] += 1\n",
        "                        last_sent = []\n",
        "                        last_sent_bias = len(last_sent)\n",
        "                        sent = last_sent + this_sent\n",
        "                        acts = tmp_acts\n",
        "\n",
        "            \n",
        "            for ii in range(len(words), len(words)+len(this_sent)):\n",
        "                word2sent[ii] = len(sents)\n",
        "            all_word_bias = len(words)\n",
        "            words.extend(this_sent)\n",
        "            sents.append(this_sent)\n",
        "            sent_acts.append(acts)\n",
        "            all_acts_of_cur_sent = update_acts(words, sent, last_sent_bias, all_word_bias, tmp_acts)\n",
        "            text_acts.extend(all_acts_of_cur_sent)\n",
        "\n",
        "        # assert len(word2sent) == len(words)\n",
        "        # assert len(sents) == len(sent_acts)\n",
        "        data.append({'words': words, 'acts': text_acts, 'sent_acts': sent_acts,\n",
        "                    'sents': sents, 'word2sent': word2sent})\n",
        "    upper_bound, lower_bound = compute_context_len(data)\n",
        "    print('\\nupper_bound: {}\\tlower_bound: {}\\nlog history: {}\\n'.format(upper_bound, lower_bound, log))\n",
        "    save_pkl(data, outfile)\n",
        "\n",
        "\n",
        "\n",
        "def update_acts(words, sent, last_sent_bias, all_word_bias, tmp_acts):\n",
        "    \"\"\"\n",
        "    Add all actions of the current sentence to text_acts\n",
        "    \"\"\"\n",
        "    # all indices of the words in the current sentences need to add a last_sent_bias\n",
        "    all_acts_of_cur_sent = []\n",
        "    for k in range(len(tmp_acts)):\n",
        "        act_idx = tmp_acts[k]['act_idx']\n",
        "        obj_idxs = tmp_acts[k]['obj_idxs']\n",
        "        text_act_idx = act_idx + all_word_bias\n",
        "        # labeling error: mis-matched word index\n",
        "        if sent[act_idx + last_sent_bias] != words[act_idx + all_word_bias]:\n",
        "            print(sent[act_idx + last_sent_bias], words[act_idx + all_word_bias])\n",
        "        text_obj_idxs = [[],[]]\n",
        "        for l in range(2):\n",
        "            for oi in obj_idxs[l]:\n",
        "                if oi == -1:\n",
        "                    text_obj_idxs[l].append(-1)\n",
        "                else:\n",
        "                    text_obj_idxs[l].append(oi + all_word_bias)\n",
        "                    if sent[oi + last_sent_bias] != words[oi + all_word_bias]:\n",
        "                        ipdb.set_trace()\n",
        "                        print(sent[oi + last_sent_bias], words[oi + all_word_bias])\n",
        "            # assert len(text_obj_idxs[l]) == len(obj_idxs[l])\n",
        "        text_act_type = tmp_acts[k]['act_type']\n",
        "        text_related_acts = []\n",
        "        if len(tmp_acts[k]['related_acts']) > 0:\n",
        "            for idx in tmp_acts[k]['related_acts']:\n",
        "                text_related_acts.append(idx + all_word_bias)\n",
        "            # assert len(text_related_acts) == len(tmp_acts[k]['related_acts'])\n",
        "        acts = {'act_idx': text_act_idx, 'obj_idxs': text_obj_idxs,\n",
        "                'act_type': text_act_type, 'related_acts': text_related_acts}\n",
        "        all_acts_of_cur_sent.append(acts)\n",
        "    return all_acts_of_cur_sent\n",
        "\n",
        "\n",
        "\n",
        "def compute_context_len(data):\n",
        "    \"\"\"\n",
        "    Compute the length of context for action argument extractor\n",
        "    the upper_bound/lower_bound indicate how far/near between the action name and its arguments\n",
        "    the difference between them is used to control the context_len\n",
        "    e.g. context_len = 2 * upper_bound\n",
        "    \"\"\"\n",
        "    upper_bound = 0\n",
        "    lower_bound = 0\n",
        "    for d in data:\n",
        "        for n in range(len(d['acts'])):\n",
        "            act = d['acts'][n]['act_idx']\n",
        "            objs = d['acts'][n]['obj_idxs']\n",
        "            for l in range(2):\n",
        "                for obj in objs[l]:\n",
        "                    if obj == -1:\n",
        "                        continue\n",
        "                    if obj - act < lower_bound:\n",
        "                        lower_bound = obj - act\n",
        "                    if obj - act > upper_bound:\n",
        "                        upper_bound = obj - act\n",
        "    return upper_bound, lower_bound\n",
        "\n",
        "\n",
        "\n",
        "def plot_results(results, domain, filename):\n",
        "    \"\"\"\n",
        "    Plot training results, called by main\n",
        "    \"\"\"\n",
        "    print('\\nSave results to %s' % filename)\n",
        "    fontsize = 20\n",
        "    if isinstance(results, list):\n",
        "        plt.figure()\n",
        "        plt.plot(range(len(results)), results, label='loss')\n",
        "        plt.title('domain: %s' % domain)\n",
        "        plt.xlabel('episodes', fontsize=fontsize)\n",
        "        plt.legend(loc='best', fontsize=fontsize)\n",
        "        plt.xticks(fontsize=fontsize)  \n",
        "        plt.yticks(fontsize=fontsize) \n",
        "        plt.savefig(filename, format='pdf')\n",
        "        print('Success\\n')\n",
        "\n",
        "    else:\n",
        "        plt.figure(figsize=(16, 20)) # , dpi=300\n",
        "        plt.subplot(311)\n",
        "        x = range(len(results['rec']))\n",
        "        plt.plot(x, results['rec'], label='rec')\n",
        "        plt.plot(x, results['pre'], label='pre')\n",
        "        plt.plot(x, results['f1'], label='f1')\n",
        "        plt.title('domain: %s' % domain, fontsize=fontsize)\n",
        "        plt.xlabel('episodes', fontsize=fontsize)\n",
        "        plt.legend(loc='best', fontsize=fontsize)\n",
        "        plt.xticks(fontsize=fontsize)  \n",
        "        plt.yticks(fontsize=fontsize) \n",
        "\n",
        "        plt.subplot(312)\n",
        "        plt.plot(range(len(results['rw'])), results['rw'], label='reward')\n",
        "        plt.xlabel('episodes', fontsize=fontsize)\n",
        "        plt.legend(loc='best', fontsize=fontsize)\n",
        "        plt.xticks(fontsize=fontsize)  \n",
        "        plt.yticks(fontsize=fontsize) \n",
        "\n",
        "        if 'loss' in results:\n",
        "            plt.subplot(313)\n",
        "            plt.plot(range(len(results['loss'])), results['loss'], label='loss')\n",
        "            plt.xlabel('episodes', fontsize=fontsize)\n",
        "            plt.legend(loc='best', fontsize=fontsize)\n",
        "            plt.xticks(fontsize=fontsize)  \n",
        "            plt.yticks(fontsize=fontsize) \n",
        "        \n",
        "        plt.subplots_adjust(wspace=0.5,hspace=0.5)\n",
        "        plt.savefig(filename, format='pdf')\n",
        "        print('Success\\n')\n",
        "\n",
        "\n",
        "\n",
        "def ten_fold_split_ind(num_data, fname, k, random=True):\n",
        "    \"\"\"\n",
        "    Split data for 10-fold-cross-validation\n",
        "    Split randomly or sequentially\n",
        "    Retutn the indecies of splited data\n",
        "    \"\"\"\n",
        "    print('Getting tenfold indices ...')\n",
        "    if os.path.exists(fname):\n",
        "        with open(fname, 'rb') as f:\n",
        "            print('Loading tenfold indices from %s\\n' % fname)\n",
        "            indices = pickle.load(f)\n",
        "            return indices\n",
        "    n = num_data/k\n",
        "    indices = []\n",
        "\n",
        "    if random:\n",
        "        tmp_inds = np.arange(num_data)\n",
        "        np.random.shuffle(tmp_inds)\n",
        "        for i in range(k):\n",
        "            if i == k - 1:\n",
        "                indices.append(tmp_inds[i*n: ])\n",
        "            else:\n",
        "                indices.append(tmp_inds[i*n: (i+1)*n])\n",
        "    else:\n",
        "        for i in range(k):\n",
        "            indices.append(range(i*n, (i+1)*n))\n",
        "\n",
        "    with open(fname, 'wb') as f:\n",
        "        pickle.dump(indices, f)\n",
        "    return indices\n",
        "\n",
        "\n",
        "\n",
        "def index2data(indices, data):\n",
        "    \"\"\"\n",
        "    Obtain k-fold data according to given indices\n",
        "    \"\"\"\n",
        "    print('Spliting data according to indices ...')\n",
        "    folds = {'train': [], 'valid': []}\n",
        "    if type(data) == dict:\n",
        "        keys = data.keys()\n",
        "        print('data.keys: {}'.format(keys))\n",
        "        num_data = len(data[keys[0]])\n",
        "        for i in range(len(indices)):\n",
        "            valid_data = {}\n",
        "            train_data = {}\n",
        "            for k in keys:\n",
        "                valid_data[k] = []\n",
        "                train_data[k] = []\n",
        "            for ind in range(num_data):\n",
        "                for k in keys:\n",
        "                    if ind in indices[i]:\n",
        "                        valid_data[k].append(data[k][ind])\n",
        "                    else:\n",
        "                        train_data[k].append(data[k][ind])\n",
        "            folds['train'].append(train_data)\n",
        "            folds['valid'].append(valid_data)\n",
        "    else:\n",
        "        num_data = len(data)\n",
        "        for i in range(len(indices)):\n",
        "            valid_data = []\n",
        "            train_data = []\n",
        "            for ind in range(num_data):\n",
        "                if ind in indices[i]:\n",
        "                    valid_data.append(data[ind])\n",
        "                else:\n",
        "                    train_data.append(data[ind])\n",
        "            folds['train'].append(train_data)\n",
        "            folds['valid'].append(valid_data)\n",
        "\n",
        "    return folds\n",
        "\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     import ipdb\n",
        "#     infile = '/content/drive/My Drive/EASDRL/data/online_test/online_labeled_text.pkl' # is online-labeled data\n",
        "#     outfile = 'online_labeled_text_data.pkl'\n",
        "#     transfer(infile, outfile)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ohw-dcHe4yQ"
      },
      "source": [
        "#构建数据存储"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_8k92Ad4uMj"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: UTF-8 -*-\n",
        "################################################################################\n",
        "# Project:  Extracting Action Sequences Based on Deep Reinforcement Learning\n",
        "# Module:   ReplayMemory\n",
        "# Author:   Wenfeng Feng \n",
        "# Time:     2017.12\n",
        "################################################################################\n",
        "\n",
        "import ipdb\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "class ReplayMemory:\n",
        "    \"\"\"\n",
        "    Replay memory\n",
        "    \"\"\"\n",
        "    def __init__(self, args, agent_mode):\n",
        "        print('Initializing ReplayMemory...')\n",
        "        self.size = args.replay_size\n",
        "        if agent_mode == 'act':\n",
        "            self.word_dim = args.word_dim\n",
        "            self.num_words = args.num_words\n",
        "            self.state_dim = 3\n",
        "        elif agent_mode == 'arg':\n",
        "            self.word_dim = args.word_dim + args.dis_dim\n",
        "            self.num_words = args.context_len\n",
        "            self.state_dim = 4\n",
        "        \n",
        "\n",
        "        self.actions = np.zeros(self.size, dtype = np.uint8)\n",
        "        self.rewards = np.zeros(self.size, dtype = np.float16)\n",
        "        self.states = np.zeros([self.size, self.num_words, self.state_dim], dtype=np.float16)\n",
        "        self.terminals = np.zeros(self.size, dtype = np.bool)\n",
        "        self.priority = args.priority\n",
        "        self.positive_rate = args.positive_rate\n",
        "        self.batch_size = args.batch_size\n",
        "        self.count = 0\n",
        "        self.current = 0\n",
        "\n",
        "        if args.load_replay:\n",
        "            self.load(args.save_replay_name)\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the replay memory\n",
        "        \"\"\"\n",
        "        print('Reset the replay memory')\n",
        "        self.actions *= 0\n",
        "        self.rewards *= 0.0\n",
        "        self.states *= 0.0\n",
        "        self.terminals *= False\n",
        "        self.count = 0\n",
        "        self.current = 0\n",
        "\n",
        "        \n",
        "    def add(self, action, reward, state, terminal):\n",
        "        \"\"\"\n",
        "        Add action, reward, state, and terminal to replay memory\n",
        "        \"\"\"\n",
        "        self.actions[self.current] = action\n",
        "        self.rewards[self.current] = reward\n",
        "        self.states[self.current] = state\n",
        "        self.terminals[self.current] = terminal\n",
        "        self.count = max(self.count, self.current + 1)  \n",
        "        self.current = (self.current + 1) % self.size\n",
        "\n",
        "\n",
        "    def getMinibatch(self):\n",
        "        \"\"\"\n",
        "        Get a mini-batch of samples\n",
        "        Memory must include poststate, prestate and history\n",
        "        Sampling random indexes or with priority\n",
        "        \"\"\"\n",
        "        prestates = np.zeros([self.batch_size, self.num_words, self.state_dim])\n",
        "        poststates = np.zeros([self.batch_size, self.num_words, self.state_dim])\n",
        "        if self.priority:\n",
        "            pos_amount =  int(self.positive_rate*self.batch_size) \n",
        "\n",
        "        indexes = []\n",
        "        count_pos = 0\n",
        "        count_neg = 0\n",
        "        count_circle = 0 \n",
        "        max_circles = 10 * self.batch_size # max times for choosing positive samples or nagative samples\n",
        "        while len(indexes) < self.batch_size:\n",
        "            # find random index \n",
        "            while True:\n",
        "                # sample one index (ignore states wraping over) \n",
        "                index = np.random.randint(1, self.count - 1)\n",
        "                # NB! poststate (last state) can be terminal state!\n",
        "                if self.terminals[index - 1]:\n",
        "                    continue\n",
        "                # use prioritized replay trick\n",
        "                if self.priority:\n",
        "                    if count_circle < max_circles:\n",
        "                        # if num_pos is already enough but current ind is also pos sample, continue\n",
        "                        if (count_pos >= pos_amount) and (self.rewards[index] > 0):\n",
        "                            count_circle += 1\n",
        "                            continue\n",
        "                        # elif num_nag is already enough but current ind is also nag sample, continue\n",
        "                        elif (count_neg >= self.batch_size - pos_amount) and (self.rewards[index] < 0): \n",
        "                            count_circle += 1\n",
        "                            continue\n",
        "                    if self.rewards[index] > 0:\n",
        "                        count_pos += 1\n",
        "                    else:\n",
        "                        count_neg += 1\n",
        "                break\n",
        "            \n",
        "            prestates[len(indexes)] = self.states[index - 1]\n",
        "            indexes.append(index)\n",
        "\n",
        "        # copy actions, rewards and terminals with direct slicing\n",
        "        actions = self.actions[indexes]  \n",
        "        rewards = self.rewards[indexes]\n",
        "        terminals = self.terminals[indexes]\n",
        "        poststates = self.states[indexes]\n",
        "        return prestates, actions, rewards, poststates, terminals\n",
        "\n",
        "\n",
        "    def save(self, fname, size):\n",
        "        \"\"\"\n",
        "        Save replay memory\n",
        "        \"\"\"\n",
        "        if size > self.size:\n",
        "            size = self.size\n",
        "        databag = {}\n",
        "        databag['actions'] = self.actions[: size]\n",
        "        databag['rewards'] = self.rewards[: size]\n",
        "        databag['states'] = self.states[: size]\n",
        "        databag['terminals'] = self.terminals[: size]\n",
        "        with open(fname, 'wb') as f:\n",
        "            print('Try to save replay memory ...')\n",
        "            pickle.dump(databag, f)\n",
        "            print('Replay memory is successfully saved as %s' % fname)\n",
        "\n",
        "\n",
        "    def load(self, fname):\n",
        "        \"\"\"\n",
        "        Load replay memory\n",
        "        \"\"\"\n",
        "        if not os.path.exists(fname):\n",
        "            print(\"%s doesn't exist!\" % fname)\n",
        "            return\n",
        "        with open(fname, 'rb') as f:\n",
        "            print('Loading replay memory from %s ...' % fname)\n",
        "            databag = pickle.load(f)\n",
        "            size = len(databag['states'])\n",
        "            self.states[: size] = databag['states']\n",
        "            self.actions[: size] = databag['actions']\n",
        "            self.rewards[: size] = databag['rewards']\n",
        "            self.terminals[: size] = databag['terminals']\n",
        "            self.count = size\n",
        "            self.current = size\n",
        "          "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcc564MoezX-"
      },
      "source": [
        "#构建智能体"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hk9bynvzks8"
      },
      "source": [
        "import ipdb\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "class Agent:\n",
        "    \"\"\"\n",
        "    Reinforcement Learning Agent\n",
        "    \"\"\"\n",
        "    def __init__(self, environment, replay_memory, deep_q_network, args):\n",
        "        print('Initializing the Agent...')\n",
        "        self.env = environment\n",
        "        self.mem = replay_memory\n",
        "        self.net = deep_q_network\n",
        "        self.agent_mode = args.agent_mode\n",
        "        self.num_words = args.num_words\n",
        "        self.batch_size = args.batch_size\n",
        "        self.num_actions = args.num_actions\n",
        "\n",
        "        self.exp_rate_start = args.exploration_rate_start\n",
        "        self.exp_rate_end = args.exploration_rate_end\n",
        "        self.exp_decay_steps = args.exploration_decay_steps\n",
        "        self.exploration_rate_test = args.exploration_rate_test\n",
        "        self.total_train_steps = args.start_epoch * args.train_steps\n",
        "\n",
        "        self.train_frequency = args.train_frequency\n",
        "        self.train_repeat = args.train_repeat\n",
        "        self.target_steps = args.target_steps\n",
        "        self.random_play = args.random_play\n",
        "        self.display_training_result = args.display_training_result\n",
        "        \n",
        "        self.steps = 0  # use to decrease the reward during time steps \n",
        "        self.filter_act_ind = args.filter_act_ind \n",
        "\n",
        "    \n",
        "    def _restart(self, train_flag, init=False):\n",
        "        \"\"\"\n",
        "        Restart the environment and self.steps (time step)\n",
        "        where self.steps indicate the index of the current word\n",
        "        \"\"\"\n",
        "        self.steps = 0\n",
        "        self.env.restart(train_flag, init)\n",
        "\n",
        "\n",
        "    def _explorationRate(self):\n",
        "        \"\"\"\n",
        "        Calculate decaying exploration rate\n",
        "        \"\"\"\n",
        "        if self.total_train_steps < self.exp_decay_steps:\n",
        "            return self.exp_rate_start - self.total_train_steps * \\\n",
        "            (self.exp_rate_start - self.exp_rate_end) / self.exp_decay_steps\n",
        "        else:\n",
        "            return self.exp_rate_end\n",
        " \n",
        "\n",
        "    def step(self, exploration_rate):\n",
        "        \"\"\"\n",
        "        Go a step forward\n",
        "        \"\"\"\n",
        "        # exploration rate determines the probability of random moves\n",
        "        if random.random() < exploration_rate:\n",
        "            action = np.random.randint(self.num_actions)\n",
        "        else:\n",
        "            # otherwise choose action with highest Q-value\n",
        "            current_state = self.env.getState()\n",
        "            qvalues = self.net.predict(current_state)\n",
        "            action = np.argmax(qvalues[0])\n",
        "           \n",
        "            \n",
        "            \n",
        "        # perform the action  \n",
        "        reward = self.env.act(action, self.steps)\n",
        "        state = self.env.getState()\n",
        "        terminal = self.env.isTerminal()\n",
        "        \n",
        "        results = []\n",
        "        self.steps += 1\n",
        "        # at a terminal time step, compute the scores of training results\n",
        "        if terminal:\n",
        "            results = self.compute_f1(self.display_training_result)\n",
        "            self.steps = 0\n",
        "            # give a bonus to the terminal actions, it could be further fine-tuned \n",
        "            reward += 2   \n",
        "\n",
        "        return action, reward, state, terminal, results\n",
        "\n",
        "    def train(self, train_steps, train_episodes, restart_init):\n",
        "        '''\n",
        "        Perform a number of steps for training\n",
        "        '''\n",
        "        #ipdb.set_trace()\n",
        "        print('\\n\\n Training... ')\n",
        "        trained_texts = 0\n",
        "        ep_loss, ep_rewards = [], []\n",
        "        ep_results = {'rec': [], 'pre': [], 'f1': [], 'loss': [], 'rw': []}\n",
        "        if restart_init:\n",
        "            self._restart(train_flag=True, init=True)\n",
        "        for i in range(train_steps):\n",
        "            if self.random_play:\n",
        "                action, reward, state, terminal, results = self.step(1)\n",
        "               \n",
        "                \n",
        "            else:\n",
        "                action, reward, state, terminal, results = self.step(self._explorationRate())\n",
        "                \n",
        "                # set_trace()\n",
        "                self.mem.add(action, reward, state, terminal)\n",
        "                \n",
        "\n",
        "                # Update target network every target_steps steps\n",
        "                if self.target_steps and i % self.target_steps == 0:\n",
        "                    self.net.update_target_network()\n",
        "\n",
        "                # train after every train_frequency steps\n",
        "                if self.mem.count > self.mem.batch_size and i % self.train_frequency == 0:\n",
        "                    # train for train_repeat times\n",
        "                    for j in range(self.train_repeat):\n",
        "                        # sample minibatch\n",
        "                        minibatch = self.mem.getMinibatch()\n",
        "                       \n",
        "                        prestates, actions, rewards, poststates, terminals=minibatch\n",
        "                        \n",
        "                      \n",
        "                        # train the network\n",
        "                        loss = self.net.train(minibatch)\n",
        "                        \n",
        "                        ep_loss.append(loss)\n",
        "                      \n",
        "            # increase number of training steps for epsilon decay\n",
        "            ep_rewards.append(reward)\n",
        "            self.total_train_steps += 1\n",
        "            if terminal:\n",
        "                if len(ep_loss) > 0:\n",
        "                    avg_loss = sum(ep_loss) / len(ep_loss)\n",
        "                    max_loss = max(ep_loss)\n",
        "                    min_loss = min(ep_loss)\n",
        "                    print('max_loss: {:>6.6f}\\t min_loss: {:>6.6f}\\t avg_loss: {:>6.6f}'.format(\n",
        "                          max_loss, min_loss, avg_loss))\n",
        "                    ep_results['loss'].append(avg_loss)\n",
        "                \n",
        "                ep_results['rec'].append(results[-3])\n",
        "                ep_results['pre'].append(results[-2])\n",
        "                ep_results['f1'].append(results[-1])\n",
        "                ep_results['rw'].append(sum(ep_rewards)/len(ep_rewards))\n",
        "                ep_loss, ep_rewards = [], []\n",
        "                trained_texts += 1\n",
        "                self._restart(train_flag=True)\n",
        "                \n",
        "            if self.env.train_epoch_end_flag or trained_texts >= train_episodes:\n",
        "                print(\"loss list{}\".format(ep_results['loss']))\n",
        "                break\n",
        "\n",
        "        return ep_results\n",
        "    \n",
        "\n",
        "    def test(self, test_steps, outfile):\n",
        "        '''\n",
        "        Perform many steps until all test texts are used\n",
        "        '''\n",
        "        print('\\n\\n Testing ...')\n",
        "        t_f1 = 0\n",
        "        t_rec = 0\n",
        "        t_pre = 0\n",
        "        t_total_acts = 0\n",
        "        t_right_acts = 0\n",
        "        t_tagged_acts = 0\n",
        "        cumulative_reward = 0\n",
        "        self._restart(train_flag=False, init=True)\n",
        "        for test_step in range(test_steps):\n",
        "            if self.random_play:\n",
        "                a, r, s, t, rs = self.step(1)\n",
        "            else:\n",
        "                a, r, s, t, rs = self.step(self.exploration_rate_test)\n",
        "            cumulative_reward += r\n",
        "            if t:\n",
        "                t_total_acts += rs[0] \n",
        "                t_right_acts += rs[1]\n",
        "                t_tagged_acts += rs[2]\n",
        "                self._restart(train_flag=False)    \n",
        "                \n",
        "            if self.env.valid_epoch_end_flag:\n",
        "                break   \n",
        "\n",
        "        average_reward = cumulative_reward / (test_step + 1)\n",
        "        results = {'rec': [], 'pre': [], 'f1': []}\n",
        "        self.basic_f1(t_total_acts, t_right_acts, t_tagged_acts, results)\n",
        "        t_rec = results['rec'][-1]\n",
        "        t_pre = results['pre'][-1]\n",
        "        t_f1 = results['f1'][-1]\n",
        "\n",
        "        outfile.write('\\n\\nSummary:\\n')\n",
        "        outfile.write('total_act: %d\\t right_act: %d\\t tagged_act: %d\\n' % (t_total_acts, t_right_acts, t_tagged_acts))  \n",
        "        outfile.write('rec: %f\\t pre: %f\\t f1: %f\\n' % (t_rec, t_pre, t_f1))\n",
        "        outfile.write('\\ncumulative reward: %f\\t average reward: %f\\n' % (cumulative_reward, average_reward))\n",
        "        print('\\n\\nSummary:')\n",
        "        print('total_act: %d\\t right_act: %d\\t tagged_act: %d' % (t_total_acts, t_right_acts, t_tagged_acts))  \n",
        "        print('rec: %f\\t pre: %f\\t f1: %f' % (t_rec, t_pre, t_f1))\n",
        "        print('\\ncumulative reward: %f\\t average reward: %f\\n' % (cumulative_reward, average_reward))\n",
        "        return t_rec, t_pre, t_f1, average_reward\n",
        "\n",
        "\n",
        "    def compute_f1(self, display):\n",
        "        \"\"\"\n",
        "        Compute f1 score for the current text\n",
        "        \"\"\"\n",
        "        text_vec_tags = self.env.text_vec[:,-1]  # ground truth\n",
        "        state_tags = self.env.state[:,-1]\n",
        "        # print(\"stage_tags:{}\".format(state_tags))\n",
        "        # print(\"text_vec_tags:{}\".format(text_vec_tags))\n",
        "        \n",
        "        # action names are not action arguments, so mask them when extracting action arguments\n",
        "        if self.agent_mode == 'arg' and self.filter_act_ind: \n",
        "            state_tags[self.env.current_text['act_inds']] = 1\n",
        "        \n",
        "        total_words = self.num_words\n",
        "        temp_words = len(self.env.current_text['tokens'])\n",
        "        if temp_words > total_words:\n",
        "            temp_words = total_words\n",
        "\n",
        "        record_ecs_act_inds = []  # record indices of exclusive action names or arguments  \n",
        "        right_acts = tagged_acts = total_acts = 0\n",
        "        # go through all words of the current text\n",
        "        for i in range(temp_words):\n",
        "            if state_tags[i] == 2:      # tagged as an action (name/argument)\n",
        "                tagged_acts += 1\n",
        "            if text_vec_tags[i] == 2:   # essential action (name/argument)\n",
        "                total_acts += 1\n",
        "                if state_tags[i] == 2:  # extract\n",
        "                    right_acts += 1\n",
        "            elif text_vec_tags[i] == 3: # optional action (name/argument)\n",
        "                if state_tags[i] == 2:  # extract\n",
        "                    total_acts += 1\n",
        "                    right_acts += 1\n",
        "            elif text_vec_tags[i] == 4: # exclusive action (name/argument)\n",
        "                if i not in record_ecs_act_inds:\n",
        "                    total_acts += 1\n",
        "                    record_ecs_act_inds.append(i)\n",
        "                # for action argument extractor\n",
        "                if self.agent_mode == 'arg':\n",
        "                    right_flag = True\n",
        "                    if i in self.env.current_text['obj_inds'][0]:\n",
        "                        exc_objs = self.env.current_text['obj_inds'][1]\n",
        "                    else:\n",
        "                        exc_objs = self.env.current_text['obj_inds'][0]\n",
        "                    record_ecs_act_inds.extend(exc_objs)\n",
        "                    # if select an action argument which is exclusive with the current one\n",
        "                    # then the operation is wrong\n",
        "                    for oi in exc_objs:\n",
        "                        if state_tags[oi] == 2:\n",
        "                            right_flag = False\n",
        "                            break\n",
        "                    if state_tags[i] == 2 and right_flag:\n",
        "                        right_acts += 1\n",
        "                # for action name extractor\n",
        "                else:\n",
        "                    assert i in self.env.current_text['act2related']\n",
        "                    exclusive_act_inds = self.env.current_text['act2related'][i]\n",
        "                    record_ecs_act_inds.extend(exclusive_act_inds)\n",
        "                    # if select an action name which is exclusive with the current one\n",
        "                    # then exclusive_flag is True, the operation is wrong\n",
        "                    exclusive_flag = False\n",
        "                    for ind in exclusive_act_inds:\n",
        "                        if state_tags[ind] == 2: # extracted as an action\n",
        "                            exclusive_flag = True\n",
        "                            break\n",
        "                    if not exclusive_flag and state_tags[i] == 2: # extract\n",
        "                        right_acts += 1\n",
        "\n",
        "        results = {'rec': [], 'pre': [], 'f1': []}\n",
        "        self.basic_f1(total_acts, right_acts, tagged_acts, results)\n",
        "        rec = results['rec'][-1]\n",
        "        pre = results['pre'][-1]\n",
        "        f1 = results['f1'][-1]\n",
        "        if display:\n",
        "            print('rec: {:>13.6f}\\t pre: {:>13.6f}\\t f1: {:>14.6f}'.format(rec, pre, f1))\n",
        "        return total_acts, right_acts, tagged_acts, rec, pre, f1\n",
        "\n",
        "\n",
        "    def basic_f1(self, total, right, tagged, results):\n",
        "        \"\"\"\n",
        "        Compute f1 scores\n",
        "        \"\"\"\n",
        "        rec = pre = f1 = 0.0\n",
        "        if total > 0:\n",
        "            rec = right / float(total)\n",
        "        if tagged > 0:\n",
        "            pre = right / float(tagged)\n",
        "        if rec + pre > 0:\n",
        "            f1 = 2 * pre * rec / (pre + rec)\n",
        "        results['rec'].append(rec)\n",
        "        results['pre'].append(pre)\n",
        "        results['f1'].append(f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKi1p63vesLo"
      },
      "source": [
        "#构建环境"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WqEfR2l0NYB"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: UTF-8 -*-\n",
        "################################################################################\n",
        "# Project:  Extracting Action Sequences Based on Deep Reinforcement Learning\n",
        "# Module:   Environment\n",
        "# Author:   Wenfeng Feng \n",
        "# Time:     2017.12\n",
        "################################################################################\n",
        "\n",
        "import re\n",
        "import ipdb\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "\n",
        "\n",
        "\n",
        "class Environment:\n",
        "    \"\"\"\n",
        "    Environment\n",
        "    \"\"\"\n",
        "    def __init__(self, args, agent_mode):\n",
        "        print('Initializing the Environment...')  \n",
        "        self.agent_mode = agent_mode \n",
        "        self.domain = args.domain\n",
        "        self.dis_dim = args.dis_dim\n",
        "        self.tag_dim = args.tag_dim\n",
        "        self.word_dim = args.word_dim\n",
        "        self.num_words = args.num_words\n",
        "        self.context_len = args.context_len\n",
        "        self.action_rate = args.action_rate\n",
        "        self.use_act_rate = args.use_act_rate\n",
        "        self.use_act_att = args.use_act_att\n",
        "        self.reward_base = args.reward_base\n",
        "        self.ra = args.reward_assign\n",
        "        self.word2ids = args.word2ids\n",
        "       \n",
        "        self.fold_id = args.fold_id\n",
        "        self.k_fold = args.k_fold\n",
        "        self.k_fold_indices = args.k_fold_indices\n",
        "\n",
        "        self.terminal_flag = False\n",
        "        self.train_epoch_end_flag = False\n",
        "        self.valid_epoch_end_flag = False\n",
        "        self.train_text_idx = -1\n",
        "        self.valid_text_idx = -1\n",
        "        self.build_dict()\n",
        "        self.get_fold_data(args.fold_id)\n",
        "        \n",
        "        args.num_pos = len(self.pos_dict) + 1\n",
        "        temp_size = self.train_steps * args.epochs + self.valid_steps\n",
        "        if temp_size < args.replay_size:\n",
        "            args.replay_size = temp_size\n",
        "        if args.train_episodes == 0:\n",
        "            args.train_episodes = self.num_train\n",
        "        args.valid_episodes = self.num_valid\n",
        "        args.train_steps = self.train_steps\n",
        "        args.valid_steps = self.valid_steps\n",
        "        \n",
        "        \n",
        "    def build_dict(self):\n",
        "        \"\"\"\n",
        "        Buiild part-of-speech dictionary\n",
        "        \"\"\"\n",
        "        self.pos_dict = {'PAD': 0}\n",
        "        for domain in ['cooking', 'win2k', 'wikihow']:\n",
        "            sent_data = load_pkl('/content/drive/My Drive/EASDRL/data/%s_dependency.pkl' % domain)\n",
        "            for sents in sent_data:\n",
        "                for sent in sents:\n",
        "                    for word, pos in sent:\n",
        "                        if pos not in self.pos_dict:\n",
        "                            self.pos_dict[pos] = len(self.pos_dict)\n",
        "\n",
        "        print('len(pos_dict): %d' % len(self.pos_dict))\n",
        "\n",
        "\n",
        "    def get_fold_data(self, fold_id):\n",
        "        \"\"\"\n",
        "        Get a fold of data (train & validate)\n",
        "        \"\"\"\n",
        "        if self.agent_mode == 'act':\n",
        "            data = self.read_act_texts()\n",
        "        else:\n",
        "            data = self.read_arg_sents()\n",
        "        print('\\n\\nGet new fold data')\n",
        "        self.train_data = data['train'][fold_id]\n",
        "        self.valid_data = data['valid'][fold_id]\n",
        "        self.train_steps = len(self.train_data) * self.num_words\n",
        "        self.valid_steps = len(self.valid_data) * self.num_words\n",
        "\n",
        "        self.num_train = len(self.train_data)\n",
        "        self.num_valid = len(self.valid_data)\n",
        "        print('training texts: %d\\tvalidation texts: %d' % (len(self.train_data), len(self.valid_data)))\n",
        "        print('self.train_steps: %d\\tself.valid_steps: %d\\n\\n' % (self.train_steps, self.valid_steps))\n",
        "       \n",
        "\n",
        "\n",
        "    def read_act_texts(self):\n",
        "        \"\"\"\n",
        "        Read data for action name extractor\n",
        "        PS: pos_data is generated by the function ``pos_tagging\" in utils.py\n",
        "        \"\"\"\n",
        "        text_data = load_pkl('/content/drive/My Drive/EASDRL/data/%s_labeled_text_data.pkl' % self.domain)\n",
        "        pos_data = load_pkl('/content/drive/My Drive/EASDRL/data/%s_dependency.pkl' % self.domain)\n",
        "        act_texts = []\n",
        "      \n",
        "        #ipdb.set_trace()\n",
        "        for i in range(len(text_data)):\n",
        "            act_text = {}\n",
        "            act_text['tokens'] = text_data[i]['words']\n",
        "            act_text['sents'] = text_data[i]['sents']\n",
        "            act_text['acts'] = text_data[i]['acts']\n",
        "            act_text['sent_acts'] = text_data[i]['sent_acts']\n",
        "            act_text['word2sent'] = text_data[i]['word2sent']\n",
        "            act_text['tags'] = np.ones(shape=(len(text_data[i]['words'])), dtype=np.int32)\n",
        "            act_text['act2related'] = {}\n",
        "            for acts in text_data[i]['acts']:\n",
        "                act_text['act2related'][acts['act_idx']] = acts['related_acts']\n",
        "                act_text['tags'][acts['act_idx']] = acts['act_type'] + 1 # 2, 3, 4\n",
        "            act_text['pos']=[]\n",
        "            for sent in pos_data[i]:\n",
        "                for word, pos in sent:\n",
        "                    act_text['pos'].append(self.pos_dict[pos])\n",
        "                 \n",
        "            self.create_matrix(act_text)\n",
        "            act_texts.append(act_text)\n",
        "        act_indices = ten_fold_split_ind(len(act_texts), self.k_fold_indices, self.k_fold)\n",
        "        act_data = index2data(act_indices, act_texts)\n",
        "        return act_data\n",
        "\n",
        "\n",
        "    def read_arg_sents(self):\n",
        "        \"\"\"\n",
        "        Read data for action argument extractor\n",
        "        PS: pos_data is generated by the function ``pos_tagging\" in utils.py\n",
        "        \"\"\"\n",
        "        \n",
        "        indata = load_pkl('/content/drive/My Drive/EASDRL/data/refined_%s_data.pkl' % self.domain)[-1]\n",
        "        pos_data = load_pkl('/content/drive/My Drive/EASDRL/data/%s_arg_pos.pkl' % self.domain)\n",
        "        \n",
        "        arg_sents = []\n",
        "        \n",
        "        # ipdb.set_trace()\n",
        "        for i in range(len(indata)):\n",
        "            for j in range(len(indata[i])):\n",
        "                if len(indata[i][j]) == 0:\n",
        "                    continue\n",
        "                # -1 obj_ind refer to UNK\n",
        "                words = indata[i][j]['last_sent'] + indata[i][j]['this_sent']+['unknown'] \n",
        "                pos = [self.pos_dict[p] for w, p in pos_data[i][j][0] + pos_data[i][j][1]] + [0]\n",
        "                if len(words) != len(words):\n",
        "                    ipdb.set_trace()\n",
        "                    print('len(words) != len(words)')\n",
        "                sent_len = len(words)\n",
        "                act_inds = [a['act_idx'] for a in indata[i][j]['acts'] if a['act_idx'] < self.num_words]\n",
        "                for k in range(len(indata[i][j]['acts'])):\n",
        "                    act_ind = indata[i][j]['acts'][k]['act_idx']\n",
        "                    obj_inds = indata[i][j]['acts'][k]['obj_idxs']\n",
        "                    arg_sent = {}\n",
        "                    arg_tags = np.ones(sent_len, dtype=np.int32)\n",
        "                    if len(obj_inds[1]) == 0:\n",
        "                        arg_tags[obj_inds[0]] = 2 # essential objects\n",
        "                    else:\n",
        "                        arg_tags[obj_inds[0]] = 4 # exclusive objects\n",
        "                        arg_tags[obj_inds[1]] = 4 # exclusive objects\n",
        "                    # generate distance representation\n",
        "                    position = np.zeros(sent_len, dtype=np.int32)\n",
        "                    position.fill(act_ind)\n",
        "                    distance = np.abs(np.arange(sent_len) - position)\n",
        "                    \n",
        "                    arg_sent['tokens'] = words\n",
        "                    arg_sent['tags'] = arg_tags\n",
        "                    arg_sent['pos'] = deepcopy(pos)\n",
        "                    arg_sent['act_ind'] = act_ind\n",
        "                    arg_sent['distance'] = distance\n",
        "                    arg_sent['act_inds'] = act_inds\n",
        "                    arg_sent['obj_inds'] = obj_inds\n",
        "                    self.create_matrix(arg_sent)\n",
        "                    arg_sents.append(arg_sent)\n",
        "        # get k-fold split data\n",
        "        arg_indices = ten_fold_split_ind(len(arg_sents), self.k_fold_indices, self.k_fold)\n",
        "        arg_data = index2data(arg_indices, arg_sents)\n",
        "       \n",
        "        return arg_data\n",
        "        \n",
        "\n",
        "    def create_matrix(self, sentence):\n",
        "        \"\"\"\n",
        "        Create state representation\n",
        "        \"\"\"\n",
        "        # get word vectors from pre-trained word2vec model\n",
        "       \n",
        "        \n",
        "        sent_ids=[]\n",
        "        for word in sentence[\"tokens\"]:\n",
        "            if word in list(self.word2ids.keys()):\n",
        "                sent_ids.append(self.word2ids[word])\n",
        "            elif word.lower() in list(self.word2ids.keys()):\n",
        "                sent_ids.append(self.word2ids[word.lower()])\n",
        "            else:\n",
        "                sent_ids.append(0)\n",
        "        \n",
        "        # padding\n",
        "        \n",
        "        \n",
        "        pad_len = self.num_words - len(sentence[\"tokens\"])\n",
        "       \n",
        "        if self.agent_mode == 'act':\n",
        "            if pad_len > 0:\n",
        "                sent_ids.extend([0]*pad_len)\n",
        "                sentence['tags'] = np.concatenate((np.array(sentence['tags']), np.ones(pad_len ,dtype=np.int32)))\n",
        "                sentence['pos'].extend([0]*pad_len)\n",
        "               \n",
        "                \n",
        "                \n",
        "            else:\n",
        "                sent_ids = sent_ids[:self.num_words]\n",
        "                sentence['pos'] = sentence['pos'][: self.num_words]\n",
        "                sentence['tokens'] = sentence['tokens'][: self.num_words]\n",
        "                sentence['tags'] = np.array(sentence['tags'])[: self.num_words]\n",
        "           # for action names extraction\n",
        "            sentence['sent_vec'] = np.array(sent_ids)[:,np.newaxis]\n",
        "        else: # self.agent_mode == 'arg':\n",
        "            distance = np.zeros([self.num_words, 1])\n",
        "            act_vec = sent_ids[sentence['act_ind']]  # word vector of the input action \n",
        "            # compute dot attention between the input action and its context \n",
        "            # attention = np.sum(sent_vec * act_vec, axis=1)  \n",
        "            # attention = np.exp(attention)\n",
        "            # attention /= sum(attention)\n",
        "            if pad_len > 0:\n",
        "                sent_vec = np.concatenate((np.array(sent_ids)[:,np.newaxis], np.zeros([pad_len, 1])))\n",
        "                sentence['tags'] = np.concatenate((np.array(sentence['tags']), np.ones(pad_len,dtype=np.int32)))\n",
        "                sentence['pos'].extend([0]* pad_len)\n",
        "                # attention = np.concatenate((attention, np.zeros(pad_len)))\n",
        "                for d in range(len(sentence['distance'])):\n",
        "                    distance[d] = sentence['distance'][d]\n",
        "            else:\n",
        "                sent_vec = np.array(sent_ids[: self.num_words])[:,np.newaxis]\n",
        "                sentence['tokens'] = sentence['tokens'][: self.num_words]\n",
        "                sentence['tags'] = np.array(sentence['tags'])[: self.num_words]\n",
        "                sentence['pos'] = sentence['pos'][: self.num_words]\n",
        "                # attention = attention[: self.num_words]\n",
        "                for d in range(self.num_words):\n",
        "                    distance[d] = sentence['distance'][d]\n",
        "            # ipdb.set_trace()\n",
        "            # if self.use_act_att: # apply attention to word embedding\n",
        "            #     sent_vec = attention.reshape(-1, 1) * sent_vec\n",
        "            sentence['sent_vec'] = np.concatenate((sent_vec, distance), axis=1)\n",
        "            \n",
        "  \n",
        "\n",
        "       \n",
        "        sentence['pos'] = np.array(sentence['pos'])[:, np.newaxis]\n",
        "        sentence['tags'].shape = (self.num_words,1)\n",
        "        \n",
        "\n",
        "    def init_predict_act_text(self, raw_text, isKeras=True):\n",
        "        \"\"\"\n",
        "        Generate action name state representation of an online text input\n",
        "        \"\"\"\n",
        "        # raw_text = re.sub(r'\\n|\\r|\\(|\\)|,|;', ' ', raw_text)\n",
        "        # raw_text = re.split(r'\\. |\\? |\\! ', raw_text)\n",
        "        tag_dim=2\n",
        "        text = {'tokens': [], 'sents': [], 'word2sent': {}}\n",
        "        for s in raw_text:\n",
        "            words = s.split()\n",
        "            if len(words) > 0:\n",
        "                for i in range(len(words)):\n",
        "                    text['word2sent'][i + len(text['tokens'])] = [len(text['sents']), i]\n",
        "                text['tokens'].extend(words)\n",
        "                text['sents'].append(words)\n",
        "\n",
        "        sent_vec = np.zeros([self.num_words, self.word_dim + tag_dim])\n",
        "        for i, w in enumerate(text['tokens']):\n",
        "            if i >= self.num_words:\n",
        "                break\n",
        "            if w in self.word2vec.vocab:\n",
        "                sent_vec[i][: self.word_dim] = self.word2vec[w]\n",
        "\n",
        "        self.state = sent_vec\n",
        "        self.terminal_flag = False\n",
        "        self.current_text = text\n",
        "\n",
        "\n",
        "    def init_predict_arg_text(self, act_idx, text, isKeras=True):\n",
        "        \"\"\"\n",
        "        Generate action argument state representation of an online text input\n",
        "        \"\"\"\n",
        "        tag_dim = 2\n",
        "        self.terminal_flag = False\n",
        "        sents = text['sents']\n",
        "        word2sent = text['word2sent']\n",
        "        sent_idx = word2sent[act_idx][0]\n",
        "        word_ids = []\n",
        "        this_sent = sents[sent_idx]\n",
        "        if sent_idx > 0: # use the former sentence and current one\n",
        "            last_sent = sents[sent_idx - 1]\n",
        "            for k, v in word2sent.items():\n",
        "                if v[0] == sent_idx or v[0] == sent_idx - 1:\n",
        "                    word_ids.append(k)\n",
        "        else:\n",
        "            last_sent = []\n",
        "            for k, v in word2sent.items():\n",
        "                if v[0] == sent_idx:\n",
        "                    word_ids.append(k)\n",
        "        words = last_sent + this_sent + ['UNK']\n",
        "        end_idx = max(word_ids) # the last index of words of these two sents\n",
        "        start_idx = min(word_ids)\n",
        "        sent_len = len(words)\n",
        "\n",
        "        position = np.zeros(sent_len, dtype=np.int32)\n",
        "        position.fill(act_idx - start_idx)\n",
        "        distance = np.abs(np.arange(sent_len) - position)\n",
        "        sent_vec = np.zeros([self.context_len, self.word_dim + self.dis_dim + 2])\n",
        "        for i, w in enumerate(words):\n",
        "            if i >= self.context_len:\n",
        "                break\n",
        "            if w in self.word2vec.vocab:\n",
        "                sent_vec[i][: self.word_dim] = self.word2vec[w]\n",
        "            sent_vec[i][self.word_dim: self.word_dim + self.dis_dim] = distance[i]\n",
        "        self.state = sent_vec\n",
        "        self.current_text = {'tokens': words, 'word2sent': word2sent, 'distance': distance}\n",
        "        return last_sent, this_sent\n",
        "\n",
        "\n",
        "    def act_online(self, action, word_ind):\n",
        "        \"\"\"\n",
        "        Perform an action of the RL Agent, for online use\n",
        "        \"\"\"\n",
        "        #self.state[word_ind, -1] = action + 1\n",
        "        self.state[word_ind, -self.tag_dim:] = action + 1\n",
        "        if word_ind + 1 >= len(self.current_text['tokens']):\n",
        "            self.terminal_flag = True\n",
        "\n",
        "\n",
        "    def restart(self, train_flag, init=False):\n",
        "        \"\"\"\n",
        "        Start a new text and get its state representation\n",
        "        \"\"\"\n",
        "        if train_flag:\n",
        "            if init:\n",
        "                self.train_text_ind = -1\n",
        "                self.train_epoch_end_flag = False\n",
        "            self.train_text_ind += 1\n",
        "            if self.train_text_ind >= len(self.train_data):\n",
        "                self.train_epoch_end_flag = True\n",
        "                print('\\n\\n-----train_epoch_end_flag = True-----\\n\\n')\n",
        "                return\n",
        "            self.current_text = self.train_data[self.train_text_ind%self.num_train]\n",
        "            print('\\ntrain_text_ind: %d of %d' % (self.train_text_ind, len(self.train_data)))\n",
        "        else:\n",
        "            if init:\n",
        "                self.valid_text_ind = -1\n",
        "                self.valid_epoch_end_flag = False\n",
        "            self.valid_text_ind += 1\n",
        "            if self.valid_text_ind >= len(self.valid_data):\n",
        "                self.valid_epoch_end_flag = True\n",
        "                print('\\n\\n-----valid_epoch_end_flag = True-----\\n\\n')\n",
        "                return\n",
        "            self.current_text = self.valid_data[self.valid_text_ind]\n",
        "            print('\\nvalid_text_ind: %d of %d' % (self.valid_text_ind, len(self.valid_data)))\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.text_vec = np.concatenate((self.current_text['sent_vec'], \n",
        "                                        self.current_text['pos'],\n",
        "                                        self.current_text['tags']), \n",
        "                                        axis=1)\n",
        "        self.state = self.text_vec.copy() \n",
        "        self.state[:, -1] = 0   # initialize all operation to NULL\n",
        "        self.terminal_flag = False\n",
        "\n",
        "        \n",
        "    def act(self, action, word_ind):\n",
        "        '''\n",
        "        Performs action and returns reward\n",
        "        even num refers to tagging action, odd num refer to non-action\n",
        "\n",
        "        '''\n",
        "        \n",
        "        self.state[word_ind, -1] = action +1\n",
        "        # amount of tagged actions \n",
        "        t_a_count = sum(self.state[: word_ind + 1, -1]) - (word_ind + 1)\n",
        "        t_a_rate = float(t_a_count) / self.num_words\n",
        "\n",
        "        label = self.text_vec[word_ind,-1]\n",
        "        if self.agent_mode == 'arg':\n",
        "            # text_vec is labelled data\n",
        "            if label == 2:\n",
        "                if action == 1:\n",
        "                    reward = self.ra[1] * self.reward_base\n",
        "                else:\n",
        "                    reward = -self.ra[1] * self.reward_base\n",
        "            elif label == 4:\n",
        "                right_flag = True\n",
        "                if word_ind in self.current_text['obj_inds'][0]:\n",
        "                    exc_objs = self.current_text['obj_inds'][1]\n",
        "                else:\n",
        "                    exc_objs = self.current_text['obj_inds'][0]\n",
        "                for oi in exc_objs: # exclusive objs\n",
        "                    if self.state[oi, -1] == 2:\n",
        "                        right_flag = False\n",
        "                        break\n",
        "                if action == 1 and right_flag:\n",
        "                    reward = self.ra[2] * self.reward_base\n",
        "                elif action == 2 and not right_flag:\n",
        "                    reward = self.ra[2] * self.reward_base\n",
        "                elif action == 2 and word_ind != self.current_text['obj_inds'][1][-1]:\n",
        "                    reward = self.ra[2] * self.reward_base\n",
        "                else:\n",
        "                    reward = -self.ra[2] * self.reward_base\n",
        "            else: #if label == 1: # non_action \n",
        "                if action == 0:\n",
        "                    reward = self.ra[0] * self.reward_base\n",
        "                else:\n",
        "                    reward = -self.ra[0] * self.reward_base\n",
        "\n",
        "        else: # self.agent_mode == 'act'\n",
        "            if label == 2: #required action\n",
        "                if action == 1: # extracted as action\n",
        "                    reward = self.ra[1] * self.reward_base\n",
        "                else: # filtered out\n",
        "                    reward = -self.ra[1] * self.reward_base\n",
        "            elif label == 3: #optional action\n",
        "                if action == 1:\n",
        "                    reward = self.ra[0] * self.reward_base\n",
        "                else:\n",
        "                    reward = 0.0\n",
        "            elif label == 4: # exclusive action\n",
        "                #ipdb.set_trace()\n",
        "                assert word_ind in self.current_text['act2related']\n",
        "                exclusive_act_inds = self.current_text['act2related'][word_ind]\n",
        "                exclusive_flag = False\n",
        "                not_biggest_flag = False\n",
        "                for ind in exclusive_act_inds:\n",
        "                    if self.state[ind, -1] == 2: # extracted as action\n",
        "                        exclusive_flag = True\n",
        "                    if ind > word_ind:\n",
        "                        not_biggest_flag = True\n",
        "                if action == 1 and not exclusive_flag:\n",
        "                # extract current word and no former exclusive action was extracted\n",
        "                    reward = self.ra[2] * self.reward_base\n",
        "                elif action == 0 and exclusive_flag:\n",
        "                # filtered out current word because one former exclusive action was extracted\n",
        "                    reward = self.ra[2] * self.reward_base\n",
        "                elif action == 0 and not_biggest_flag:\n",
        "                # filtered out current word and at least one exclusive action left \n",
        "                    reward = self.ra[2] * self.reward_base\n",
        "                else:\n",
        "                    reward = -self.ra[2] * self.reward_base\n",
        "            else: #if label == 1: # non_action \n",
        "                if action == 0:\n",
        "                    reward = self.ra[0] * self.reward_base\n",
        "                else:\n",
        "                    reward = -self.ra[0] * self.reward_base\n",
        "        \n",
        "        if self.use_act_rate and reward != 0:\n",
        "            if t_a_rate <= self.action_rate and reward > 0:\n",
        "                reward += 5.0 * np.square(t_a_rate) * self.reward_base\n",
        "            else:\n",
        "                reward -= 5.0 * np.square(t_a_rate) * self.reward_base\n",
        "        # all words of current text are tagged, break\n",
        "        if word_ind + 1 >= len(self.current_text['tokens']):\n",
        "            self.terminal_flag = True\n",
        "        \n",
        "        return reward\n",
        "\n",
        "\n",
        "    def getState(self):\n",
        "        '''\n",
        "        Gets current text state\n",
        "        '''\n",
        "        return self.state\n",
        "\n",
        "\n",
        "    def isTerminal(self):\n",
        "        '''\n",
        "        Returns if tag_actions is done\n",
        "        if all the words of a text have been tagged, then terminate\n",
        "        '''\n",
        "        return self.terminal_flag\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGAik4-g3Z5A"
      },
      "source": [
        "#注意力机制\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIF9ybxMFpPh"
      },
      "source": [
        "##Feed Forward注意力机制"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3gRTB_JSUI2"
      },
      "source": [
        "import  tensorflow.keras.initializers\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "                 W_regularizer=None,\n",
        "                 b_regularizer=None,\n",
        "                 W_constraint=None,\n",
        "                 b_constraint=None,\n",
        "                 bias=True,\n",
        "                 **kwargs\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        Keras Layer that implements an Attention mechanism for temporal data.\n",
        "        Supports Masking.\n",
        "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
        "        # Input shape\n",
        "            3D tensor with shape: `(samples, steps, features)`.\n",
        "        # Output shape\n",
        "            2D tensor with shape: `(samples, features)`.\n",
        "        :param kwargs:\n",
        "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
        "        The dimensions are inferred based on the output shape of the RNN.\n",
        "        Example:\n",
        "            # 1\n",
        "            model.add(LSTM(64, return_sequences=True))\n",
        "            model.add(Attention())\n",
        "            # next add a Dense layer (for classification/regression) or whatever...\n",
        "            # 2\n",
        "            hidden = LSTM(64, return_sequences=True)(words)\n",
        "            sentence = Attention()(hidden)\n",
        "            # next add a Dense layer (for classification/regression) or whatever...\n",
        "        \"\"\"\n",
        "        super(Attention, self).__init__()\n",
        "        self.bias = bias\n",
        "        self.init = tf.keras.initializers.get('glorot_uniform')\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        '''\n",
        "        :param input_shape:\n",
        "        :return:\n",
        "        '''\n",
        "        self.output_dim = input_shape[-1]\n",
        "        self.W = self.add_weight(\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 shape=(input_shape[2], 1),\n",
        "                                 initializer=self.init,\n",
        "                                 trainable=True\n",
        "                                 )\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight(\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     shape=(input_shape[1], 1),\n",
        "                                     initializer='zero',\n",
        "                                     trainable=True\n",
        "                                     )\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return None\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        # (N, step, d), (d, 1)  ==>   (N, step, 1)\n",
        "        e = tf.matmul(inputs, self.W, )\n",
        "        if self.bias:\n",
        "            e += self.b\n",
        "        e = tf.tanh(e)\n",
        "        a = tf.nn.softmax(e, axis=1)\n",
        "        # (N, step, d) (N, step, 1) ====> (N, step, d)\n",
        "        c = inputs*a\n",
        "        # (N, d)\n",
        "        c = tf.reduce_sum(c, axis=1)\n",
        "        return c\n",
        "\n",
        "    def get_config(self):\n",
        "        return {'units': self.output_dim}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDpjbw89Ftfz"
      },
      "source": [
        "##自注意力机制"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKOYIZbmFzXN"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import math\n",
        "class self_attention(keras.layers.Layer):\n",
        "  def __init__(self,hidden_dim):\n",
        "    super(self_attention,self).__init__()\n",
        "    \n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.norm = keras.layers.BatchNormalization()\n",
        "  def build(self,input_shape):\n",
        "    #输入维度为(samples,steps,features)\n",
        "    self.Wq=self.add_weight(name=\"Wq\",shape=(input_shape[-1],self.hidden_dim),\n",
        "                            initializer=\"glorot_uniform\",\n",
        "                            trainable=True)\n",
        "    self.Wk=self.add_weight(name=\"Wk\",shape=(input_shape[-1],self.hidden_dim),\n",
        "                            initializer=\"glorot_uniform\",\n",
        "                            trainable=True)\n",
        "    self.Wv=self.add_weight(name=\"Wv\",shape=(input_shape[-1],self.hidden_dim),\n",
        "                            initializer=\"glorot_uniform\",\n",
        "                            trainable=True)\n",
        "    self.built=True\n",
        "  def call(self,inputs):\n",
        "    \n",
        "    Q=tf.matmul(inputs,self.Wq)\n",
        "    K=tf.matmul(inputs,self.Wk)\n",
        "    V=tf.matmul(inputs,self.Wv)\n",
        "\n",
        "    #转制进行矩阵乘法\n",
        "    e=tf.matmul(Q,tf.transpose(K,perm=[0,2,1]))\n",
        "    #归一化处理\n",
        "    e=tf.divide(e,math.sqrt(self.hidden_dim))\n",
        "    #在维度1上做softmax得到注意力权重\n",
        "    e=tf.nn.softmax(e,axis=1)\n",
        "    #输入向量乘以注意力权重\n",
        "    e=tf.matmul(e,V)\n",
        "    #在维度1上做加法得到最后的输出\n",
        "    out=tf.reduce_sum(e,axis=1)\n",
        "   \n",
        "    \n",
        "    return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F6eIUuAzG2v"
      },
      "source": [
        "#使用tf2.0创建DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5ZxZ_ywFpvT"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: UTF-8 -*-\n",
        "################################################################################\n",
        "# Project:  Extracting Action Sequences Based on Deep Reinforcement Learning\n",
        "# Module:   KerasEADQN\n",
        "# Author:   Wenfeng Feng \n",
        "# Time:     2017.12\n",
        "################################################################################\n",
        "\n",
        "import ipdb \n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Conv1D,LSTM,Activation,Flatten,Dense,Input,concatenate,BatchNormalization,GRU,SimpleRNN\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "pos_dict = {'PAD': 0}\n",
        "for domain in ['cooking', 'win2k', 'wikihow']:\n",
        "    sent_data = load_pkl('/content/drive/My Drive/EASDRL/data/%s_dependency.pkl' % domain)\n",
        "    for sents in sent_data:\n",
        "        for sent in sents:\n",
        "            for word, pos in sent:\n",
        "                if pos not in pos_dict:\n",
        "                    pos_dict[pos] = len(pos_dict)\n",
        "\n",
        "pos_value=[]\n",
        "for key,value in pos_dict.items():\n",
        "    pos_value.append(value)\n",
        "   \n",
        "tag_value=[]\n",
        "for i in range(3):\n",
        "    tag_value.append(i)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#自定义嵌入层\n",
        "class Embedding_layer(keras.layers.Layer):\n",
        "    def __init__(self,emb_matrix):\n",
        "            \n",
        "        super(Embedding_layer,self).__init__()\n",
        "        # tag_embedding\n",
        "        self.tag_emb_layer = tf.keras.layers.Embedding(trainable=True,input_dim=3,output_dim=50,dtype=tf.float32)\n",
        "        #pos_embedding\n",
        "        self.pos_emb_layer = tf.keras.layers.Embedding(trainable=True,input_dim=37,output_dim=50 ,dtype=tf.float32)\n",
        "        #distance embedding for action arguments extraction\n",
        "        self.dis_emb_layer = tf.keras.layers.Embedding(trainable=True,input_dim=200,output_dim=50,dtype=tf.float32)\n",
        "        # word_embedding\n",
        "        self.word_emb_layer = tf.keras.layers.Embedding(input_dim=len(emb_matrix),output_dim=50,mask_zero=True,\n",
        "                                 weights=[emb_matrix],trainable=True)\n",
        "    def call(self,state):\n",
        "        # for action names extraction\n",
        "        word_emb = int(state[:,:,0])\n",
        "        pos_ind = int(state[:,:,1])\n",
        "        tag_ind = int(state[:,:,2])\n",
        "\n",
        "\n",
        "        # for action arguments extraction\n",
        "        # word_emb = int(state[:,:,0])\n",
        "        # dis      = int(state[:,:,1]) \n",
        "        # pos_ind = int(state[:,:,2])\n",
        "        # tag_ind = int(state[:,:,3])\n",
        "        \n",
        "        \n",
        "        # dis_emb = self.dis_emb_layer(dis) \n",
        "        pos_emb = self.pos_emb_layer(pos_ind)\n",
        "        tag_emb = self.tag_emb_layer(tag_ind)\n",
        "        word_emb = self.word_emb_layer(word_emb)\n",
        "        # # for action arguments extraction\n",
        "        # out=tf.keras.layers.concatenate([word_emb,dis_emb,pos_emb,tag_emb],axis=2)\n",
        "        \n",
        "        #for action names extraction\n",
        "        out=tf.keras.layers.concatenate([word_emb,pos_emb,tag_emb],axis=2)\n",
        "        return out\n",
        "            \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DeepQLearner:\n",
        "    \"\"\"\n",
        "    Deep Q-Network, in keras\n",
        "    \"\"\"\n",
        "    def __init__(self, args, agent_mode):\n",
        "        print('Initializing the DQN...')\n",
        "        self.word_dim = args.word_dim\n",
        "        self.tag_dim = args.tag_dim\n",
        "        self.num_pos = args.num_pos\n",
        "        self.pos_dim = args.pos_dim\n",
        "        self.dropout = args.dropout\n",
        "        self.optimizer = args.optimizer\n",
        "        self.dense_dim = args.dense_dim\n",
        "        self.batch_size = args.batch_size\n",
        "        self.gamma = args.gamma\n",
        "        self.learning_rate = args.learning_rate\n",
        "        self.num_actions = args.num_actions\n",
        "        self.num_filters = args.num_filters\n",
        "        self.emb_matrix = args.emb_matrix\n",
        "        if agent_mode == 'act':\n",
        "            self.num_words = args.num_words\n",
        "            self.input_dim = 3\n",
        "            self.units = 256\n",
        "        elif agent_mode == 'arg':\n",
        "            self.num_words = args.context_len\n",
        "            self.input_dim = 4\n",
        "            self.emb_dim = args.word_dim + args.dis_dim + 100\n",
        "            self.units = 256\n",
        "        self.emb = Embedding_layer(self.emb_matrix)\n",
        "        self.attention=self_attention(256)\n",
        "            \n",
        "        self.target_model=self.build_dqn()\n",
        "        self.model=self.build_dqn()\n",
        "        self.compile_model()\n",
        "    \n",
        "   \n",
        "    \n",
        "   \n",
        "            \n",
        "    def build_dqn(self):\n",
        "        \"\"\"\n",
        "        Build Text-CNN\n",
        "        \"\"\"\n",
        "        \n",
        "        # ipdb.set_trace()\n",
        "        units = self.units #filter width\n",
        "        fn = self.num_filters  #filter num\n",
        "        # inputs = Input(shape=(self.num_words, self.emb_dim, 1))\n",
        "        x = Input(shape=(self.num_words, self.input_dim))\n",
        "        inputs=self.emb(x)\n",
        "        \n",
        "        out_forward = GRU(units, return_sequences=True, recurrent_initializer='glorot_uniform')(inputs)\n",
        "        # out_forward = keras.layers.GlobalAveragePooling1D()(out_forward)\n",
        "        out_forward = self.attention(out_forward)\n",
        "        \n",
        "        out_backward = GRU(units, return_sequences=True, recurrent_initializer='glorot_uniform', go_backwards=True)(inputs)\n",
        "        # out_backward = keras.layers.GlobalAveragePooling1D()(out_backward)\n",
        "        out_backward = self.attention(out_backward)\n",
        "        concate = concatenate([out_backward,out_forward],axis=-1)\n",
        "        concate = keras.layers.Reshape([1,512])(concate)\n",
        "        feature = Conv1D(16,3,activation=\"relu\",padding=\"valid\",kernel_initializer=\"he_uniform\",\n",
        "                         data_format=\"channels_first\")(concate)\n",
        "        feature = Conv1D(32,3,activation=\"relu\",padding=\"valid\",kernel_initializer=\"he_uniform\",\n",
        "                         data_format=\"channels_first\")(feature)\n",
        "        feature = keras.layers.GlobalAveragePooling1D()(feature)\n",
        "        flat = keras.layers.Flatten()(feature)\n",
        "        \n",
        "        \n",
        "        full_con = Dense(256,activation=\"relu\",kernel_initializer=\"he_uniform\")(flat)\n",
        "        out = Dense(self.num_actions,kernel_initializer='truncated_normal')(full_con)\n",
        "        \n",
        "        return Model(x,out)\n",
        "    \n",
        "  \n",
        "    \n",
        "   \n",
        "    \n",
        "    \n",
        "    def compile_model(self):\n",
        "        \"\"\"\n",
        "        Choose optimizer and compile model\n",
        "        \"\"\"\n",
        "        if self.optimizer == 'sgd':\n",
        "            opt = keras.optimizers.SGD(lr=self.learning_rate, momentum=0.9, decay=0.9, nesterov=True)\n",
        "        elif self.optimizer == 'adam':\n",
        "            opt = keras.optimizers.Adam(lr=self.learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "        elif self.optimizer == 'nadam':\n",
        "            opt = keras.optimizers.Nadam(lr=self.learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
        "        elif self.optimizer == 'adadelta':\n",
        "            opt = keras.optimizers.Adadelta(lr=self.learning_rate, rho=0.95, epsilon=1e-08, decay=0.0)\n",
        "        else:\n",
        "            opt = keras.optimizers.RMSprop(lr=self.learning_rate, rho=0.9, epsilon=1e-06)\n",
        "\n",
        "        self.model.compile(optimizer=\"nadam\", loss='mse')\n",
        "        self.target_model.compile(optimizer=\"nadam\", loss='mse')\n",
        "        print(self.model.summary())\n",
        "        \n",
        "        \n",
        "\n",
        "    def update_target_network(self):\n",
        "        \"\"\"\n",
        "        Update target DQN\n",
        "        \"\"\"\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "    \n",
        "  \n",
        "          \n",
        " \n",
        "    def train(self, minibatch):\n",
        "        \"\"\"\n",
        "        Train DQN with a mini-batch of samples\n",
        "        \"\"\"\n",
        "        # expand components of minibatch\n",
        "        prestates, actions, rewards, poststates, terminals = minibatch\n",
        "       \n",
        "        prestates=tf.convert_to_tensor(list(prestates))\n",
        "        poststates=tf.convert_to_tensor(list(poststates))\n",
        "        \n",
        "        postq = self.target_model.predict_on_batch(poststates)\n",
        "        targets = self.model.predict_on_batch(prestates)\n",
        "        \n",
        "        maxpostq = postq.max(axis=-1)\n",
        "\n",
        "        # set_trace()\n",
        "        # update Q-value targets for actions taken  \n",
        "        for i, action in enumerate(actions):\n",
        "            if terminals[i]:  \n",
        "                targets[i, action] = float(rewards[i])\n",
        "            else: \n",
        "                targets[i, action] = float(rewards[i]) + self.gamma * maxpostq[i]\n",
        "        \n",
        "        \n",
        "        loss=self.model.train_on_batch(prestates,targets)\n",
        "        return loss\n",
        "        # set_trace()\n",
        "    def predict(self, current_state):\n",
        "        \"\"\"\n",
        "        Predict Q-values\n",
        "        \"\"\"\n",
        "        \n",
        "        current_state=tf.expand_dims(tf.convert_to_tensor(current_state),axis=0)\n",
        "       \n",
        "        qvalues = self.model.predict_on_batch(current_state)\n",
        "        return qvalues\n",
        "\n",
        "\n",
        "    def save_weights(self, weight_dir):\n",
        "        \"\"\"\n",
        "        Save weights\n",
        "        \"\"\"\n",
        "        self.model.save_weights(weight_dir)\n",
        "        print('Saved weights to %s ...' % weight_dir)\n",
        "\n",
        "\n",
        "    def load_weights(self, weight_dir):\n",
        "        \"\"\"\n",
        "        Load weights\n",
        "        \"\"\"\n",
        "        self.model.load_weights(weight_dir)\n",
        "        print('Loaded weights from %s ...' % weight_dir)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "179hmf1oe9oJ"
      },
      "source": [
        "#主函数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqaSsAUFxfOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86d49527-66e2-4ce1-beac-99978e0c80a7"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: UTF-8 -*-\n",
        "################################################################################\n",
        "# Project:  Extracting Action Sequences Based on Deep Reinforcement Learning\n",
        "# Module:   main\n",
        "# Author:   Wenfeng Feng \n",
        "# Time:     2017.12\n",
        "################################################################################\n",
        "\n",
        "import time\n",
        "# import ipdb\n",
        "import pickle\n",
        "import argparse\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "# from EASDRL.utils import get_time, plot_results, str2bool, print_args\n",
        "# from EASDRL.Agent import Agent\n",
        "# # from EASDRL.EADQN import DeepQLearner\n",
        "# from EASDRL.KerasEADQN import DeepQLearner\n",
        "# from EASDRL.Environment import Environment\n",
        "# from EASDRL.ReplayMemory import ReplayMemory\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "\n",
        "\n",
        "def preset_args():\n",
        "    \"\"\"\n",
        "    Preset args\n",
        "    \"\"\"\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    envarg = parser.add_argument_group('Environment')\n",
        "    envarg.add_argument(\"--domain\",         type=str,       default='wikihow',  help=\"One of 'win2k', 'cooking' and 'wikihow'\")\n",
        "    envarg.add_argument(\"--model_dim\",      type=str,       default=50,         help=\"Dimension of the pretrained word vectors (word2vec model)\")\n",
        "    envarg.add_argument(\"--num_words\",      type=int,       default=400,        help=\"Maximum number of words in a text\")\n",
        "    envarg.add_argument(\"--word_dim\",       type=int,       default=50,         help=\"Dimension of word embedding\")\n",
        "    envarg.add_argument(\"--tag_dim\",        type=int,       default=50,         help=\"Dimension of tag embedding\")\n",
        "    envarg.add_argument(\"--dis_dim\",        type=int,       default=50,         help=\"Dimension of distance embedding\")\n",
        "    envarg.add_argument(\"--pos_dim\",        type=int,       default=50,         help=\"Dimension of part-of-speech embedding\")\n",
        "    envarg.add_argument(\"--context_len\",    type=int,       default=100,        help=\"Maximum number of words in the context (of an action name)\")\n",
        "    envarg.add_argument(\"--reward_assign\",  type=list,      default=[1, 2, 3],  help=\"Reward for essential, optional and exclusive items\")\n",
        "    envarg.add_argument(\"--reward_base\",    type=float,     default=50.0,       help=\"For reward scaling\")\n",
        "    envarg.add_argument(\"--object_rate\",    type=float,     default=0.07,       help=\"Percentage of action arguments, a priori\")\n",
        "    envarg.add_argument(\"--action_rate\",    type=float,     default=0.10,       help=\"Percentage of action names, a priori\")\n",
        "    envarg.add_argument(\"--use_act_rate\",   type=str2bool,  default=True,       help=\"Whether or not use action_rate, for name extractor\")\n",
        "    envarg.add_argument(\"--use_act_att\",    type=str2bool,  default=False,      help=\"Whether or not use the given action name to compute attention, for arguments extractor\")\n",
        "    envarg.add_argument(\"--use_pos\",        type=str2bool,  default=True,       help=\"Whether or not use the part-of-speech of words as input\")\n",
        "    \n",
        "    memarg = parser.add_argument_group('Replay memory')\n",
        "    memarg.add_argument(\"--positive_rate\",      type=float,     default=0.9,    help=\"Percentage of positive samples in a minibatch\")\n",
        "    memarg.add_argument(\"--priority\",           type=str2bool,  default=True,   help=\"Whether or not apply prioritized sampling\")\n",
        "    memarg.add_argument(\"--save_replay\",        type=str2bool,  default=False,  help=\"Whether or not save the replay memory\")\n",
        "    memarg.add_argument(\"--load_replay\",        type=str2bool,  default=False,  help=\"Whether or not load last replay memory\")\n",
        "    memarg.add_argument(\"--replay_size\",        type=int,       default=50000,  help=\"Size of the replay memory\")\n",
        "    memarg.add_argument(\"--save_replay_size\",   type=int,       default=1000,   help=\"Size of the replay memory to be saved\")\n",
        "    memarg.add_argument(\"--save_replay_name\",   type=str,       default='data/saved_replay_memory.pkl', help=\"Directory of saved replay memory\")\n",
        "\n",
        "    netarg = parser.add_argument_group('Deep Q-learning network')\n",
        "    netarg.add_argument(\"--batch_size\",     type=int,   default=32,     help=\"Size of minibatch\")\n",
        "    netarg.add_argument(\"--num_filters\",    type=int,   default=32,     help=\"Number of convolutional filters\")\n",
        "    netarg.add_argument(\"--dense_dim\",      type=int,   default=256,    help=\"Dimension of the last but two fully-connected layer\")\n",
        "    netarg.add_argument(\"--num_actions\",    type=int,   default=2,      help=\"Number of actions\")\n",
        "    netarg.add_argument(\"--optimizer\",      type=str,   default='nadam', help=\"Optimizer\")\n",
        "    netarg.add_argument(\"--learning_rate\",  type=float, default=0.001,  help=\"Learning rate\")\n",
        "    netarg.add_argument(\"--dropout\",        type=float, default=0.25,   help=\"Dropout rate\")\n",
        "    netarg.add_argument(\"--gamma\",          type=float, default=0.9,    help=\"Discount factor\")\n",
        "\n",
        "    antarg = parser.add_argument_group('Agent')\n",
        "    antarg.add_argument(\"--exploration_rate_start\",     type=float,     default=1,      help=\"Initial exploration probability for training\")\n",
        "    antarg.add_argument(\"--exploration_rate_end\",       type=float,     default=0.1,    help=\"Final exploration probability for training\")\n",
        "    antarg.add_argument(\"--exploration_rate_test\",      type=float,     default=0.0,    help=\"Exploration probability for testing\")\n",
        "    antarg.add_argument(\"--exploration_decay_steps\",    type=int,       default=1000,   help=\"How many time steps to anneal exploration probability\")\n",
        "    antarg.add_argument(\"--train_frequency\",            type=int,       default=1,      help=\"Train model after 'train_frequency' time steps\")\n",
        "    antarg.add_argument(\"--train_repeat\",               type=int,       default=1,      help=\"How many bathes to train at a time step\")\n",
        "    antarg.add_argument(\"--target_steps\",               type=int,       default=5,      help=\"Copy main network to target network after 'target_steps' time steps\")\n",
        "    antarg.add_argument(\"--random_play\",                type=str2bool,  default=False,  help=\"Play randomly or not\")\n",
        "    antarg.add_argument(\"--display_training_result\",    type=str2bool,  default=True,   help=\"Whether or not display the F1 scores after each episode\")\n",
        "    antarg.add_argument(\"--filter_act_ind\",             type=str2bool,  default=True,   help=\"Whether or not neglact the words of action names when extracting action arguments\")\n",
        "\n",
        "    mainarg = parser.add_argument_group('Main loop')\n",
        "    mainarg.add_argument(\"--gui_mode\",              type=str2bool,  default=False,  help=\"Whether or not in the human-agent interaction environment\")\n",
        "    mainarg.add_argument(\"--gpu_fraction\",          type=float,     default=0.2,    help=\"Fraction of GPU memory to use\")\n",
        "    mainarg.add_argument(\"--epochs\",                type=int,       default=20,     help=\"Training epochs\")\n",
        "    mainarg.add_argument(\"--start_epoch\",           type=int,       default=0,      help=\"Training from this epoch\")\n",
        "    mainarg.add_argument(\"--stop_epoch_gap\",        type=int,       default=5,      help=\"Early stop if no improvement after 'stop_epoch_gap' epochs\")\n",
        "    mainarg.add_argument(\"--train_episodes\",        type=int,       default=50,     help=\"Test once after training 'train_episodes' texts\")\n",
        "    mainarg.add_argument(\"--load_weights\",          type=str2bool,  default=False,  help=\"Load weights or not\")\n",
        "    mainarg.add_argument(\"--save_weights\",          type=str2bool,  default=True,   help=\"Save weights or not\")\n",
        "    mainarg.add_argument(\"--fold_id\",               type=int,       default=0,      help=\"Which fold of data to be trained\")\n",
        "    mainarg.add_argument(\"--start_fold\",            type=int,       default=0,      help=\"Start training from this fold of data\")\n",
        "    mainarg.add_argument(\"--end_fold\",              type=int,       default=5,      help=\"End training after this fold of data\")\n",
        "    mainarg.add_argument(\"--k_fold\",                type=int,       default=5,      help=\"Number of folds, usually 10 or 5\")\n",
        "    mainarg.add_argument(\"--result_dir\",            type=str,       default='test', help=\"File name for saving results\")\n",
        "    mainarg.add_argument(\"--agent_mode\",            type=str,       default='act',  help=\"One of 'act' (name extractor) and 'arg' (arguments extractor)\")\n",
        "    \n",
        "    return parser.parse_known_args()[0]\n",
        "\n",
        "\n",
        "def args_init(args):\n",
        "    \"\"\"\n",
        "    args initialization\n",
        "    \"\"\"\n",
        "    with open(\"/content/drive/My Drive/EASDRL/Glove_emb/{}_word2ids.pkl\".format(args.domain),\"rb\") as f:\n",
        "        data = pickle.load(f)\n",
        "    args.word2ids = data\n",
        "    \n",
        "    with open(\"/content/drive/My Drive/EASDRL/data/{}_wordvec_dim50_matrix.pkl\".format(args.domain),\"rb\") as f:\n",
        "        embedding = pickle.load(f)\n",
        "    args.emb_matrix = np.array(embedding)\n",
        "    if args.load_weights:\n",
        "        args.exploration_rate_start = args.exploration_rate_end\n",
        "    if args.agent_mode == 'arg':\n",
        "        args.num_words = 100\n",
        "        args.train_episodes = 1000\n",
        "        args.display_training_result = 0\n",
        "    args.k_fold_indices = '/content/drive/My Drive/EASDRL/data/indices/%s_%s_%d_fold_indices.pkl' % (args.domain, args.agent_mode, args.k_fold)\n",
        "    args.result_dir = 'results/%s_%s_%s' % (args.domain, args.agent_mode, args.result_dir)\n",
        "    if args.end_fold > args.k_fold or args.end_fold <= 0:\n",
        "        args.end_fold = args.k_fold\n",
        "    return args\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    \"\"\"\n",
        "    main function, build, train, validate, save and load model\n",
        "    \"\"\"\n",
        "    start = time.time()\n",
        "    print('Current time is: %s' % get_time())\n",
        "    print('Starting at main...')\n",
        "    # store k-fold cross-validation results, including recall, precision, f1 and average reward\n",
        "    fold_result = {'rec': [], 'pre': [], 'f1': [], 'rw': []}\n",
        "\n",
        "    # one can continue to train model from the start_fold rather than fold 0 \n",
        "    for fi in range(args.start_fold, args.end_fold):\n",
        "        fold_start = time.time()\n",
        "        args.fold_id = 3\n",
        "        if args.fold_id == args.start_fold:\n",
        "            # Initialize environment and replay memory\n",
        "            env_act = Environment(args, args.agent_mode)\n",
        "            mem_act = ReplayMemory(args, args.agent_mode)\n",
        "        else:\n",
        "            env_act = Environment(args, args.agent_mode)\n",
        "            mem_act = ReplayMemory(args, args.agent_mode)\n",
        "            env_act.get_fold_data(3)\n",
        "            mem_act.reset()\n",
        "\n",
        "\n",
        "            # ipdb.set_trace()\n",
        "            # Initialize deep_q_net and agent\n",
        "        net_act = DeepQLearner(args, args.agent_mode)\n",
        "        agent = Agent(env_act, mem_act, net_act, args)\n",
        "\n",
        "            # loop over epochs\n",
        "        epoch_result = {'rec': [0.0], 'pre': [0.0], 'f1': [0.0], 'rw': [0.0]}\n",
        "        training_result = {'rec': [], 'pre': [], 'f1': [], 'loss': [], 'rw': []}\n",
        "        log_epoch = 0\n",
        "        with open(\"/content/drive/My Drive/EASDRL/%s_fold%d.txt\" % (args.result_dir, args.fold_id), 'w') as outfile:\n",
        "                # print all args to the screen and outfile\n",
        "            print_args(args, outfile)\n",
        "\n",
        "            if args.load_weights:\n",
        "                print('Loading weights ...')\n",
        "                filename = '/content/drive/My Drive/EASDRL/model/%s_%s_%d_fold%d.h5' % (args.domain, args.agent_mode, args.k_fold, args.fold_id)\n",
        "                net_act.load_weights(filename)\n",
        "\n",
        "            for epoch in range(args.start_epoch, args.start_epoch + args.epochs):\n",
        "                    # test the model every args.train_episodes or at the end of an epoch\n",
        "                num_test = -1\n",
        "                env_act.train_epoch_end_flag = False\n",
        "                while not env_act.train_epoch_end_flag:\n",
        "                        # training\n",
        "                    num_test += 1\n",
        "                    restart_init = False if num_test > 0 else True\n",
        "                    tmp_result = agent.train(args.train_steps, args.train_episodes, restart_init)\n",
        "                    for k in training_result:\n",
        "                        training_result[k].extend(tmp_result[k])\n",
        "                        # testing\n",
        "                    rec, pre, f1, rw = agent.test(args.valid_steps, outfile)\n",
        "\n",
        "                    if f1 > max(epoch_result['f1']):\n",
        "                        if args.save_weights:\n",
        "                            filename = '/content/drive/My Drive/EASDRL/model/%s_%s_%d_fold%d.h5' % (args.domain, args.agent_mode, args.k_fold, args.fold_id)\n",
        "                            net_act.save_weights(filename)\n",
        "\n",
        "                        epoch_result['f1'].append(f1)\n",
        "                        epoch_result['rec'].append(rec)\n",
        "                        epoch_result['pre'].append(pre)\n",
        "                        epoch_result['rw'].append(rw)\n",
        "                        log_epoch = epoch\n",
        "                        outfile.write('\\n\\n Best f1 score: {}  best epoch: {}\\n'.format(epoch_result, log_epoch))\n",
        "                        print('\\n\\n Best f1 score: {}  best epoch: {}\\n'.format(epoch_result, log_epoch))\n",
        "                    else:\n",
        "                        print(\"当前最高F1为{}\".format(max(epoch_result['f1'])))\n",
        "                    # if no improvement after args.stop_epoch_gap, break\n",
        "                if epoch - log_epoch >= args.stop_epoch_gap:\n",
        "                    outfile.write('\\n\\nBest f1 score: {}  best epoch: {}\\n'.format(epoch_result, log_epoch))\n",
        "                    print('\\nepoch: %d  result_dir: %s' % (epoch, args.result_dir))\n",
        "                    print('-----Early stopping, no improvement after %d epochs-----\\n' % args.stop_epoch_gap)\n",
        "                    break\n",
        "            if args.save_replay:\n",
        "                mem_act.save(args.save_replay_name, args.save_replay_size)\n",
        "                \n",
        "                # plot the training process results if you want\n",
        "                # filename = '%s_fold%d_training_process.pdf'%(args.result_dir, args.fold_id)\n",
        "                # plot_results(epoch_result, args.domain, filename)\n",
        "                # outfile.write('\\n\\n training process:\\n{}\\n\\n'.format(epoch_result))\n",
        "\n",
        "                # find out the best f1 score in the current fold, add it to fold_result\n",
        "            best_ind = epoch_result['f1'].index(max(epoch_result['f1']))\n",
        "            for k in epoch_result:\n",
        "                fold_result[k].append(epoch_result[k][best_ind])\n",
        "                outfile.write('{}: {}\\n'.format(k, fold_result[k]))\n",
        "                print(('{}: {}\\n'.format(k, fold_result[k])))\n",
        "                # compute the average f1 and average reward of all fold results up to now\n",
        "            avg_f1 = sum(fold_result['f1']) / len(fold_result['f1'])\n",
        "            avg_rw = sum(fold_result['rw']) / len(fold_result['rw'])\n",
        "            outfile.write('\\nAvg f1: {}  Avg reward: {}\\n'.format(avg_f1, avg_rw))\n",
        "            print('\\nAvg f1: {}  Avg reward: {}\\n'.format(avg_f1, avg_rw))\n",
        "                \n",
        "            fold_end = time.time()\n",
        "            print('Total time cost of fold %d is: %ds' % (args.fold_id, fold_end - fold_start))\n",
        "            outfile.write('\\nTotal time cost of fold %d is: %ds\\n' % (args.fold_id, fold_end - fold_start))\n",
        "        \n",
        "        tf.compat.v1.reset_default_graph()\n",
        "    end = time.time()\n",
        "    print('Total time cost: %ds' % (end - start))\n",
        "    print('Current time is: %s\\n' % get_time())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    args = args_init(preset_args())\n",
        "    main(args)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current time is: 2020-11-20_12:30:23\n",
            "Starting at main...\n",
            "Initializing the Environment...\n",
            "len(pos_dict): 36\n",
            "Getting tenfold indices ...\n",
            "Loading tenfold indices from /content/drive/My Drive/EASDRL/data/indices/wikihow_act_5_fold_indices.pkl\n",
            "\n",
            "Spliting data according to indices ...\n",
            "\n",
            "\n",
            "Get new fold data\n",
            "training texts: 120\tvalidation texts: 30\n",
            "self.train_steps: 48000\tself.valid_steps: 12000\n",
            "\n",
            "\n",
            "Initializing ReplayMemory...\n",
            "Getting tenfold indices ...\n",
            "Loading tenfold indices from /content/drive/My Drive/EASDRL/data/indices/wikihow_act_5_fold_indices.pkl\n",
            "\n",
            "Spliting data according to indices ...\n",
            "\n",
            "\n",
            "Get new fold data\n",
            "training texts: 120\tvalidation texts: 30\n",
            "self.train_steps: 48000\tself.valid_steps: 12000\n",
            "\n",
            "\n",
            "Reset the replay memory\n",
            "Initializing the DQN...\n",
            "Model: \"functional_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 400, 3)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_layer_1 (Embedding_la (None, 400, 150)     370350      input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "gru_7 (GRU)                     (None, 400, 256)     313344      embedding_layer_1[1][0]          \n",
            "__________________________________________________________________________________________________\n",
            "gru_6 (GRU)                     (None, 400, 256)     313344      embedding_layer_1[1][0]          \n",
            "__________________________________________________________________________________________________\n",
            "self_attention_1 (self_attentio (None, 256)          196608      gru_6[0][0]                      \n",
            "                                                                 gru_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 512)          0           self_attention_1[3][0]           \n",
            "                                                                 self_attention_1[2][0]           \n",
            "__________________________________________________________________________________________________\n",
            "reshape_3 (Reshape)             (None, 1, 512)       0           concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 16, 510)      64          reshape_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_7 (Conv1D)               (None, 32, 508)      1568        conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_3 (Glo (None, 508)          0           conv1d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 508)          0           global_average_pooling1d_3[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 256)          130304      flatten_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 2)            514         dense_6[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,326,096\n",
            "Trainable params: 1,326,096\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Initializing the Agent...\n",
            "\n",
            " Arguments:\n",
            "action_rate: 0.1\n",
            "agent_mode: act\n",
            "batch_size: 32\n",
            "context_len: 100\n",
            "dense_dim: 256\n",
            "dis_dim: 50\n",
            "display_training_result: True\n",
            "domain: wikihow\n",
            "dropout: 0.25\n",
            "emb_matrix: [[ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [-1.31705451 -1.62075329 -3.55342197 ...  0.39982    -2.43118143\n",
            "   1.83672523]\n",
            " [-0.00642309  2.33945322 -1.01872885 ...  0.26340801  3.23175573\n",
            "   2.52312183]\n",
            " ...\n",
            " [-0.37692416 -0.72535241  0.17852852 ...  0.74083066  0.7840535\n",
            "   1.23777938]\n",
            " [ 0.15963718 -1.40158629 -0.99150044 ... -1.85479236  0.72357142\n",
            "   2.7478621 ]\n",
            " [ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]]\n",
            "end_fold: 5\n",
            "epochs: 20\n",
            "exploration_decay_steps: 1000\n",
            "exploration_rate_end: 0.1\n",
            "exploration_rate_start: 1\n",
            "exploration_rate_test: 0.0\n",
            "filter_act_ind: True\n",
            "fold_id: 3\n",
            "gamma: 0.9\n",
            "gpu_fraction: 0.2\n",
            "gui_mode: False\n",
            "k_fold: 5\n",
            "k_fold_indices: /content/drive/My Drive/EASDRL/data/indices/wikihow_act_5_fold_indices.pkl\n",
            "learning_rate: 0.001\n",
            "load_replay: False\n",
            "load_weights: False\n",
            "model_dim: 50\n",
            "num_actions: 2\n",
            "num_filters: 32\n",
            "num_pos: 37\n",
            "num_words: 400\n",
            "object_rate: 0.07\n",
            "optimizer: nadam\n",
            "pos_dim: 50\n",
            "positive_rate: 0.9\n",
            "priority: True\n",
            "random_play: False\n",
            "replay_size: 50000\n",
            "result_dir: results/wikihow_act_test\n",
            "reward_assign: [1, 2, 3]\n",
            "reward_base: 50.0\n",
            "save_replay: False\n",
            "save_replay_name: data/saved_replay_memory.pkl\n",
            "save_replay_size: 1000\n",
            "save_weights: True\n",
            "start_epoch: 0\n",
            "start_fold: 0\n",
            "stop_epoch_gap: 5\n",
            "tag_dim: 50\n",
            "target_steps: 5\n",
            "train_episodes: 50\n",
            "train_frequency: 1\n",
            "train_repeat: 1\n",
            "train_steps: 48000\n",
            "use_act_att: False\n",
            "use_act_rate: True\n",
            "use_pos: True\n",
            "valid_episodes: 30\n",
            "valid_steps: 12000\n",
            "word2ids: {'PAD': 0, 'Throw': 1, 'away': 2, 'or': 3, 'recycle': 4, 'alkaline': 5, 'batteries': 6, 'Alkaline': 7, 'are': 8, 'the': 9, 'type': 10, 'that': 11, 'power': 12, 'most': 13, 'simple': 14, 'battery-operated': 15, 'devices': 16, 'such': 17, 'as': 18, 'flashlights': 19, 'toys': 20, 'remote': 21, 'controls': 22, 'smoke': 23, 'alarms': 24, 'They': 25, 'come': 26, 'in': 27, 'a': 28, 'variety': 29, 'of': 30, 'sizes': 31, 'ranging': 32, 'from': 33, 'AAA': 34, 'to': 35, '9': 36, 'volt': 37, 'Proper': 38, 'disposal': 39, 'methods': 40, 'for': 41, 'may': 42, 'depend': 43, 'on': 44, 'your': 45, 'local': 46, 'waste': 47, 'regulations': 48, 'Most': 49, 'manufactured': 50, 'since': 51, '1996': 52, 'made': 53, 'relatively': 54, 'non-hazardous': 55, 'materials': 56, 'and': 57, 'can': 58, 'be': 59, 'tossed': 60, 'directly': 61, 'into': 62, 'trash': 63, 'However': 64, 'some': 65, 'states': 66, 'municipalities': 67, 'still': 68, 'require': 69, 'treated': 70, 'hazardous': 71, 'In': 72, 'these': 73, 'cases': 74, 'must': 75, 'recycled': 76, 'dropped': 77, 'off': 78, 'at': 79, 'designated': 80, 'facility': 81, 'You': 82, 'able': 83, 'drop': 84, 'recycling': 85, 'electronics': 86, 'retailer': 87, 'center': 88, 'community': 89, 'Check': 90, 'Earth911': 91, 'website': 92, 'drop-off': 93, 'locations': 94, 'area': 95, 'Dispose': 96, 'car': 97, 'an': 98, 'auto': 99, 'parts': 100, 'collection': 101, 'site': 102, 'Since': 103, 'contain': 104, 'lead': 105, 'acid': 106, 'they': 107, 'can’t': 108, 'disposed': 109, 'with': 110, 'Many': 111, 'retailers': 112, 'like': 113, 'Home': 114, 'Depot': 115, 'Auto': 116, 'Zone': 117, 'will': 118, 'accept': 119, 'dead': 120, 'used': 121, 'also': 122, 'them': 123, 'facilities': 124, 'specialize': 125, 'Take': 126, 'rechargeable': 127, 'Rechargeable': 128, 'nickel': 129, 'cadmium': 130, 'which': 131, 'create': 132, 'environmental': 133, 'hazard': 134, 'if': 135, 'thrown': 136, 'landfill': 137, 'incinerator': 138, 'These': 139, 'recycles': 140, 'Radio': 141, 'Shack': 142, 'Staples': 143, 'Earth911com': 144, 'participating': 145, 'Donate': 146, 'lithium-ion': 147, 'kinds': 148, 'typically': 149, 'portable': 150, 'mobile': 151, 'phone': 152, 'digital': 153, 'camera': 154, 'tablet': 155, 'laptop': 156, 'Lithium-ion': 157, 'donated': 158, 'refurbishers': 159, 'recyclers': 160, 'USA': 161, 'participate': 162, 'program': 163, 'called': 164, 'Sustainable': 165, 'Materials': 166, 'Management': 167, 'Electronics': 168, 'Challenge': 169, 'The': 170, 'SSM': 171, 'encourages': 172, 'companies': 173, 'refurbish': 174, 'lithium': 175, 'other': 176, 'electronic': 177, 'components': 178, 'EPA': 179, 'list': 180, 'businesses': 181, 'here': 182, 'httpswwwepagovrecycleelectronics-donation-and-recycling': 183, 'find': 184, 'out': 185, 'donations': 186, 'button': 187, 'This': 188, 'kind': 189, 'battery': 190, 'is': 191, 'hearing': 192, 'aids': 193, 'watches': 194, 'contains': 195, 'mercuric': 196, 'oxide': 197, 'silver': 198, 'zinc-air': 199, 'considered': 200, 'brought': 201, 'household': 202, 'proper': 203, 'handling': 204, 'Button': 205, 'extremely': 206, 'toxic': 207, 'should': 208, 'never': 209, 'dispose': 210, 'Stack': 211, 'books': 212, 'vertically': 213, 'horizontally': 214, 'Books': 215, 'don’t': 216, 'all': 217, 'have': 218, 'lined': 219, 'up': 220, 'along': 221, 'shelf': 222, 'Create': 223, 'vertical': 224, 'stack': 225, 'two': 226, 'three': 227, 'place': 228, 'it': 229, 'beside': 230, 'variation': 231, 'stacks': 232, 'serve': 233, 'bookends': 234, 'hold': 235, 'others': 236, 'upright': 237, 'Use': 238, 'risers': 239, 'tables': 240, 'Place': 241, 'small': 242, 'side': 243, 'table': 244, 'coffee': 245, 'Then': 246, 'put': 247, 'lamp': 248, 'clock': 249, 'candlestick': 250, 'top': 251, 'Change': 252, 'how': 253, 'many': 254, 'until': 255, 'you': 256, 'level': 257, 'best': 258, 'Find': 259, 'large': 260, 'use': 261, 'Look': 262, 'old': 263, 'atlases': 264, 'almanacs': 265, 'textbooks': 266, 'Put': 267, 'tray': 268, 'vase': 269, 'flowers': 270, 'interesting': 271, 'object': 272, 'vintage': 273, 'magnifying': 274, 'glass': 275, 'compass': 276, 'Pair': 277, 'same': 278, 'color': 279, 'scheme': 280, 'line': 281, 'cohesive': 282, 'For': 283, 'example': 284, 'gather': 285, 'varying': 286, 'shades': 287, 'blue': 288, 'choose': 289, 'different': 290, 'browns': 291, 'neutral': 292, 'look': 293, 'Or': 294, 'remove': 295, 'covers': 296, 'get': 297, 'white': 298, 'cream-colored': 299, 'spine': 300, 'paint': 301, 'burlap': 302, 'If': 303, 'want': 304, 'match': 305, 'each': 306, 'decor': 307, 'brush': 308, 'lightly': 309, 'then': 310, 'hang': 311, 'dry': 312, 'wrap': 313, 'wallpaper': 314, 'glue': 315, 'material': 316, 'inside': 317, 'cover': 318, 'Match': 319, 'theme': 320, 'room': 321, \"children's\": 322, 'decorate': 323, 'nursery': 324, 'gardening': 325, 'sun': 326, 'lots': 327, 'plants': 328, 'nautical': 329, 'search': 330, 'seafaring': 331, 'copies': 332, 'Moby': 333, 'Dick': 334, 'Tempest': 335, 'Bundle': 336, 'pages': 337, 'twine': 338, 'Whether': 339, 'book': 340, 'that’s': 341, 'falling': 342, 'apart': 343, 'just': 344, 'one': 345, 'book’s': 346, 'separate': 347, 'completely': 348, 'leave': 349, 'intact': 350, 'Wrap': 351, 'around': 352, 'display': 353, 'this': 354, 'little': 355, 'package': 356, 'bookshelf': 357, 'windowsill': 358, 'Tie': 359, 'bow': 360, 'add': 361, 'dried': 362, 'flower': 363, 'more': 364, 'feminine': 365, 'style': 366, 'Pour': 367, 'dye': 368, 'containers': 369, 'food': 370, 'coloring': 371, 'liquid': 372, 'water': 373, 'colors': 374, 'bowl': 375, 'compartment': 376, 'ice': 377, 'cube': 378, 'Spread': 379, 'towels': 380, 'vinyl': 381, 'tablecloth': 382, 'protect': 383, 'work': 384, 'surface': 385, 'Dip': 386, 'paper': 387, 'brushes': 388, 'eyedroppers': 389, 'apply': 390, 'vivid': 391, '(and': 392, 'aren’t': 393, 'worried': 394, 'about': 395, 'cleanup)': 396, 'fold': 397, 'towel': 398, '(at': 399, 'least': 400, 'twice)': 401, 'square': 402, 'dip': 403, 'corner': 404, 'Allow': 405, 'any': 406, 'excess': 407, 'back': 408, 'container': 409, 'Don’t': 410, 'soak': 411, 'too': 412, 'deeply': 413, 'very': 414, 'long': 415, 'overly': 416, 'saturated': 417, 'colored': 418, '–': 419, 'areas': 420, 'great': 421, 'good': 422, 'time': 423, 'explain': 424, 'mixing': 425, 'while': 426, 'children': 427, 'observe': 428, 'experiment': 429, 'Lay': 430, 'Open': 431, 'had': 432, 'folded': 433, 'sunny': 434, 'sidewalk': 435, 'driveway': 436, 'nearby': 437, 'fireplace': 438, '(not': 439, 'close!)': 440, 'Let': 441, 'sit': 442, 'totally': 443, 'touch': 444, 'flip': 445, 'over': 446, 'once': 447, 'seem': 448, 'drying': 449, 'hairdryer': 450, 'help': 451, 'it’s': 452, 'windy': 453, 'day': 454, 'either': 455, 'indoors': 456, 'make': 457, 'sure': 458, 'weigh': 459, 'corners': 460, 'down': 461, 'Secure': 462, 'middle': 463, 'twist': 464, 'tie': 465, 'pipe': 466, 'cleaner': 467, 'Bunch': 468, 'together': 469, 'couple': 470, 'times': 471, 'Fluff': 472, 'wings': 473, 'gently': 474, 'hands': 475, 'definition': 476, 'accordion-style': 477, 'before': 478, 'bunching': 479, 'Fold': 480, 'outer': 481, 'edge': 482, 'form': 483, 'thin': 484, 'rectangle': 485, 'onto': 486, 'itself': 487, 'Flip': 488, 'again': 489, 'repeat': 490, 'pleated': 491, 'Make': 492, 'antennae': 493, 'straws': 494, 'wrapped': 495, 'simply': 496, 'bend': 497, 'curl': 498, 'ends': 499, 'Otherwise': 500, 'bendy': 501, 'Attach': 502, 'butterfly': 503, 'secure': 504, 'tape': 505, 'hot': 506, 'Decorate': 507, 'further': 508, 'desired': 509, 'Add': 510, 'stickers': 511, 'eyes': 512, 'googly': 513, 'eye': 514, 'imagination': 515, 'whatever': 516, 'decorations': 517, 'buttons': 518, 'puff': 519, 'balls': 520, 'colorful': 521, 'bit': 522, 'double-sided': 523, 'follow': 524, 'instructions': 525, 'age': 526, 'group': 527, 'recommendations': 528, 'safety': 529, 'precautions': 530, 'when': 531, 'using': 532, 'butterflies': 533, 'ceiling': 534, 'string': 535, 'pin': 536, 'bulletin': 537, 'board': 538, 'Paint': 539, 'canisters': 540, 'latex': 541, 'Pick': 542, 'purchase': 543, 'choice': 544, 'pick': 545, 'matches': 546, 'kitchen': 547, 'you’re': 548, 'going': 549, 'canister': 550, 'regular': 551, 'coat': 552, 'Afterward': 553, 'another': 554, 'By': 555, 'adding': 556, 'you’ll': 557, 'better': 558, 'seal': 559, 'job': 560, 'seamless': 561, 'save': 562, 'yourself': 563, 'consider': 564, 'spray': 565, 'painting': 566, 'industrial': 567, 'Consider': 568, 'picking': 569, 'blends': 570, 'chalk': 571, 'distressed': 572, 'A': 573, 'complement': 574, 'shabby': 575, 'chic': 576, 'might': 577, 'trying': 578, 'given': 579, 'To': 580, 'do': 581, 'light': 582, 'take': 583, 'damp': 584, 'wipe': 585, 'When': 586, 'doing': 587, 'allowing': 588, 'bottom': 589, 'bleed': 590, 'through': 591, 'Some': 592, 'popular': 593, 'include': 594, 'green': 595, 'pastel': 596, 'Feel': 597, 'free': 598, 'paints': 599, \"There's\": 600, 'no': 601, 'reason': 602, 'why': 603, \"shouldn't\": 604, 'fun!': 605, 'Stencil': 606, 'Purchase': 607, 'own': 608, 'stencils': 609, 'fix': 610, 'stencil': 611, 'fill': 612, 'let': 613, 'stenciled': 614, 'Ultimately': 615, 'ways': 616, 'personalize': 617, 'spelling': 618, 'words': 619, 'letters': 620, 'by': 621, 'taking': 622, 'thick': 623, 'cardboard': 624, 'cutting': 625, 'shape': 626, 'letter': 627, 'design': 628, 'fabric': 629, 'Depending': 630, 'taste': 631, 'entire': 632, 'specific': 633, 'wrapping': 634, 'ribbon': 635, 'part': 636, 'Cut': 637, 'attach': 638, 'borders': 639, 'labels': 640, 'addition': 641, 'decorating': 642, 'Do': 643, 'stenciling': 644, 'attaching': 645, 'tags': 646, 'Your': 647, 'purely': 648, 'decorative': 649, 'reflect': 650, 'new': 651, 'lives': 652, 'refurbished': 653, 'label': 654, 'cans': 655, '“sugar”': 656, '“oatmeal”': 657, 'store': 658, 'glitter': 659, 'dries': 660, 'Sprinkle': 661, 'hour': 662, 'after': 663, \"you've\": 664, 'painted': 665, 'applying': 666, 'stick': 667, 'personal': 668, 'you’ve': 669, 'done': 670, 'pets': 671, 'carriers': 672, 'As': 673, 'soon': 674, 'tornado': 675, 'warning': 676, 'announced': 677, 'spot': 678, 'potential': 679, 'twister': 680, 'spring': 681, 'action': 682, 'immediately': 683, 'Command': 684, 'coax': 685, 'cat': 686, 'its': 687, 'carrier': 688, 'necessary': 689, 'pillowcase': 690, 'scoop': 691, '(with': 692, 'opening': 693, 'facing': 694, 'up)': 695, 'Lead': 696, 'command': 697, 'dog': 698, 'storm': 699, 'shelter': 700, 'safe': 701, 'zone': 702, 'familiar': 703, 'comforts': 704, 'blankets': 705, 'sturdy': 706, 'pet': 707, 'crate': 708, 'provides': 709, 'added': 710, 'protection': 711, 'keeps': 712, 'running': 713, 'hiding': 714, 'fear': 715, 'Provide': 716, 'debris': 717, 'No': 718, 'matter': 719, 'happens': 720, 'padding': 721, 'reduce': 722, 'risk': 723, 'injury': 724, 'due': 725, 'flying': 726, 'heavy': 727, 'even': 728, 'mattress': 729, 'kept': 730, 'huddle': 731, 'beneath': 732, 'run': 733, 'drills': 734, 'element': 735, 'well': 736, 'It': 737, 'likely': 738, 'disorienting': 739, 'perhaps': 740, 'frightening': 741, 'Try': 742, 'remain': 743, '—': 744, 'appear': 745, 'calm': 746, 'often': 747, 'train': 748, 'experiencing': 749, 'real': 750, 'terrifying': 751, 'experience': 752, 'involved': 753, 'loud': 754, 'powerful': 755, 'barrels': 756, 'present': 757, 'keel': 758, 'sake': 759, 'humans': 760, 'Trust': 761, 'planning': 762, 'preparation': 763, 'see': 764, 'outcome': 765, 'point': 766, 'largely': 767, 'now': 768, 'control': 769, 'behind': 770, 'possible': 771, 'After': 772, 'passes': 773, 'but': 774, 'temporarily': 775, 'evacuate': 776, 'damage': 777, 'contamination': 778, 'Bring': 779, 'their': 780, 'supplies': 781, 'unless': 782, 'absolutely': 783, 'cannot': 784, 'so': 785, 'assume': 786, 'right': 787, 'within': 788, 'few': 789, 'hours': 790, 'care': 791, 'left': 792, 'home': 793, 'Of': 794, 'course': 795, 'only': 796, 'decide': 797, 'much': 798, 'willing': 799, 'during': 800, 'risks': 801, 'regardless': 802, 'Keep': 803, 'post-storm': 804, 'dangers': 805, 'emerge': 806, 'difficult': 807, 'recognize': 808, 'surrounding': 809, 'Tornadoes': 810, 'cause': 811, 'catastrophic': 812, 'leaves': 813, 'both': 814, 'stunned': 815, 'disoriented': 816, 'Such': 817, 'disorientation': 818, 'additional': 819, 'anxiety': 820, 'turn': 821, 'unusual': 822, 'reckless': 823, 'behavior': 824, 'leashed': 825, 'aftermath': 826, 'frightened': 827, 'try': 828, 'hide': 829, 'Watch': 830, 'broken': 831, 'protruding': 832, 'nails': 833, 'sharp': 834, 'unstable': 835, 'walls': 836, 'floors': 837, 'structures': 838, 'gas': 839, 'leaks': 840, 'downed': 841, 'lines': 842, 'note': 843, 'puddles': 844, 'pools': 845, 'contaminated': 846, 'Revive': 847, 'tree': 848, 'log': 849, 'sitting': 850, 'yard': 851, 'haven’t': 852, 'chance': 853, 'live': 854, 'near': 855, 'beach': 856, 'beautiful': 857, 'pieces': 858, 'wood': 859, 'washed': 860, 'ashore': 861, 'Filling': 862, 'logs': 863, 'succulents': 864, 'stunning': 865, 'way': 866, 'revive': 867, 'moist': 868, 'creating': 869, 'Hollow': 870, 'Replace': 871, 'hollowed': 872, 'cactus': 873, 'soil': 874, 'plant': 875, 'Build': 876, 'succulent': 877, 'bed': 878, 'Gardens': 879, 'terraces': 880, 'spaces': 881, 'incorporate': 882, 'existing': 883, 'garden': 884, 'potters': 885, 'Decide': 886, 'smaller': 887, 'pots': 888, 'larger': 889, 'potter': 890, 'several': 891, 'types': 892, 'visual': 893, 'effect': 894, 'arrange': 895, 'fountains': 896, 'wheelbarrows': 897, 'implements': 898, 'Arrange': 899, 'ground': 900, 'flower-': 901, 'beds': 902, 'specifies': 903, 'features': 904, 'humidity': 905, 'calming': 906, 'Shutter': 907, 'Much': 908, 'indoor': 909, 'wall': 910, 'displays': 911, 'outdoor': 912, 'versions': 913, 'shutters': 914, 'not': 915, 'hanging': 916, 'house': 917, 'space': 918, 'garage': 919, 'repurposing': 920, 'provide': 921, 'easy': 922, 'frame': 923, 'chicken': 924, 'wire': 925, 'Spanish': 926, 'moss': 927, 'nailing': 928, 'shutter': 929, 'between': 930, 'slats': 931, 'shutter(s)': 932, 'Fill': 933, 'Connect': 934, 'windows': 935, 'case': 936, 'chances': 937, 'storage': 938, 'Wiring': 939, 'frames': 940, 'creative': 941, 'terrace': 942, 'Construct': 943, 'four': 944, 'nailed': 945, 'wired': 946, 'stable': 947, 'structure': 948, 'Nail': 949, 'piece': 950, 'move': 951, 'window': 952, 'selection': 953, 'textures': 954, 'visually': 955, 'stimulating': 956, 'arrangement': 957, 'Consult': 958, 'contractor': 959, 'person': 960, 'charge': 961, 'renovation': 962, 'project': 963, 'exact': 964, 'timeline': 965, 'regarding': 966, 'dates': 967, 'information': 968, 'plan': 969, 'ahead': 970, 'preparing': 971, 'strangers': 972, 'arriving': 973, 'Reserve': 974, 'freight': 975, 'elevator': 976, 'high-rise': 977, 'building': 978, 'contractors': 979, 'equipment': 980, 'Remove': 981, 'pictures': 982, 'mirrors': 983, 'curtains': 984, 'hangings': 985, 'furnishings': 986, 'shared': 987, 'prevent': 988, 'breaking': 989, 'becoming': 990, 'damaged': 991, 'Prepare': 992, 'cook': 993, 'meals': 994, 'advance': 995, 'access': 996, 'stove-top': 997, 'oven': 998, 'Freeze': 999, 'refrigerate': 1000, 'foods': 1001, 'prolong': 1002, 'life': 1003, 'freshness': 1004, 'period': 1005, 'Pack': 1006, 'gadgets': 1007, 'appliances': 1008, 'belongings': 1009, \"won't\": 1010, 'need': 1011, 'throughout': 1012, 'labeled': 1013, 'boxes': 1014, 'bubble': 1015, 'packing': 1016, 'fragile': 1017, 'items': 1018, 'china': 1019, 'drinking': 1020, 'glasses': 1021, 'dinnerware': 1022, 'break': 1023, 'easily': 1024, 'Store': 1025, 'shed': 1026, 'rent': 1027, 'construction': 1028, 'phase': 1029, 'Set': 1030, 'aside': 1031, 'essential': 1032, 'daily': 1033, 'basis': 1034, 'makers': 1035, 'openers': 1036, 'utensils': 1037, 'kitchenware': 1038, 'disposable': 1039, 'cups': 1040, 'plates': 1041, 'eating': 1042, 'process': 1043, 'wash': 1044, 'dishes': 1045, 'valuable': 1046, 'Move': 1047, 'refrigerator': 1048, 'location': 1049, 'mini-refrigerator': 1050, 'plate': 1051, 'microwave': 1052, 'heat': 1053, 'advantage': 1054, 'grill': 1055, 'occurring': 1056, 'nice': 1057, 'weather': 1058, 'conditions': 1059, 'furniture': 1060, 'protective': 1061, 'plastic': 1062, 'sheet': 1063, 'resides': 1064, 'damages': 1065, 'Cover': 1066, 'walkways': 1067, 'sheets': 1068, 'damaging': 1069, 'carpets': 1070, 'Relocate': 1071, \"pet's\": 1072, 'bowls': 1073, 'section': 1074, '10': 1075, 'days': 1076, 'prior': 1077, 'stressed': 1078, 'confused': 1079, 'Determine': 1080, 'working': 1081, 'utilizing': 1082, 'bathroom': 1083, 'private': 1084, 'rugs': 1085, 'seen': 1086, 'photographs': 1087, 'paintings': 1088, 'family-related': 1089, 'something': 1090, 'created': 1091, 'Pictures': 1092, 'animals': 1093, 'landscapes': 1094, 'Just': 1095, 'fits': 1096, 'mantle': 1097, 'ornate': 1098, 'covered': 1099, 'gold': 1100, 'leaf': 1101, \"wouldn't\": 1102, 'rustic': 1103, 'wooden': 1104, 'Hang': 1105, 'mirror': 1106, 'above': 1107, 'Again': 1108, 'compliments': 1109, 'thicker': 1110, 'trinkets': 1111, 'knickknacks': 1112, 'souvenirs': 1113, 'pretty': 1114, 'rock': 1115, 'shell': 1116, 'found': 1117, 'complex': 1118, 'antique': 1119, 'doll': 1120, 'spiritual': 1121, 'statue': 1122, 'angel': 1123, 'Virgin': 1124, 'Mary': 1125, 'Buddha': 1126, 'related': 1127, 'interests': 1128, 'sailing': 1129, 'putting': 1130, 'model': 1131, 'sailboat': 1132, 'nature': 1133, 'driftwood': 1134, 'fake': 1135, 'ones': 1136, '(unless': 1137, \"they're\": 1138, 'high-quality)': 1139, 'cheap': 1140, 'potted': 1141, 'vases': 1142, 'Here': 1143, 'ideas': 1144, 'started': 1145, 'season': 1146, 'poinsettias': 1147, 'winter': 1148, 'tulips': 1149, 'garlands': 1150, 'hooks': 1151, 'Ivy': 1152, 'works': 1153, 'year-round': 1154, 'evergreen': 1155, 'pinecones': 1156, \"don't\": 1157, '(or': 1158, \"can't\": 1159, 'have)': 1160, 'natural': 1161, 'candles': 1162, 'tall': 1163, 'short': 1164, 'pillar': 1165, 'keep': 1166, 'shade': 1167, 'ivory': 1168, 'mixture': 1169, 'pink': 1170, 'red': 1171, 'setting': 1172, '\"hurricane\"': 1173, 'chargers': 1174, 'clean': 1175, 'cards': 1176, 'postcards': 1177, 'received': 1178, 'special': 1179, 'occasions': 1180, 'birthdays': 1181, 'weddings': 1182, 'baby': 1183, 'showers': 1184, 'current': 1185, 'holiday': 1186, 'Christmas': 1187, 'Easter': 1188, 'every': 1189, 'month': 1190, 'looking': 1191, 'year': 1192, 'across': 1193, 'holidays': 1194, 'offer': 1195, 'excuse': 1196, 'change': 1197, 'mantel': 1198, 'Valentines': 1199, 'filled': 1200, 'roses': 1201, 'Halloween': 1202, 'pumpkins': 1203, 'fall': 1204, 'drape': 1205, 'garland': 1206, 'berries': 1207, 'holly': 1208, 'Refurbished': 1209, 'lot': 1210, 'character': 1211, 'Decorating': 1212, 'unfinished': 1213, 'lets': 1214, 'statement': 1215, 'following': 1216, 'tips': 1217, 'uses': 1218, 'unique': 1219, 'Turn': 1220, 'combination': 1221, 'radiophonograph': 1222, 'beautifully': 1223, 'veneered': 1224, 'cabinet': 1225, 'elegant': 1226, 'foyer': 1227, 'console': 1228, 'drums': 1229, 'bunch': 1230, 'trio': 1231, 'tribal': 1232, 'individual': 1233, 'Discreet': 1234, 'reveals': 1235, 'source': 1236, '(garage': 1237, 'sale': 1238, 'heirloom?)': 1239, 'adds': 1240, 'room’s': 1241, 'o': 1242, 'overall': 1243, 'beauty': 1244, 'comfort': 1245, 'instant': 1246, 'slipcover': 1247, 'throwing': 1248, 'king-size': 1249, 'quilt': 1250, 'bedspread': 1251, 'unsightly': 1252, 'sofa': 1253, 'rope': 1254, 'grosgrain': 1255, 'chest': 1256, 'teenage': 1257, 'boy’s': 1258, 'discarded': 1259, 'license': 1260, 'Garage': 1261, 'sales': 1262, 'junk': 1263, 'stores': 1264, 'sources': 1265, 'where': 1266, 'Decoupage': 1267, 'dresser': 1268, 'dressing': 1269, 'motifs': 1270, 'cut': 1271, 'Safeguard': 1272, 'finishing': 1273, 'glaze': 1274, 'polyurethane': 1275, 'Dress': 1276, 'plain-Jane': 1277, 'upholstered': 1278, 'chair': 1279, 'Get': 1280, 'trusty': 1281, 'hot-glue': 1282, 'gun': 1283, 'gimp': 1284, '(an': 1285, 'ornamental': 1286, 'braid': 1287, 'cord)': 1288, 'fringe': 1289, 'trim': 1290, 'seat': 1291, 'Faux': 1292, 'rows': 1293, 'booklets': 1294, 'describing': 1295, 'various': 1296, 'techniques': 1297, 'step-by-step': 1298, 'carry': 1299, 'kits': 1300, 'Everything': 1301, 'there': 1302, 'Individualize': 1303, 'dining': 1304, 'chairs': 1305, 'mismatched': 1306, 'bold': 1307, '(all': 1308, 'coordinating': 1309, 'colors)': 1310, 'children’s': 1311, 'names': 1312, 'backs': 1313, 'motif': 1314, 'Craft': 1315, 'shops': 1316, 'glue-on': 1317, 'numbers': 1318, 'initials': 1319, 'patterns': 1320, 'Victorian': 1321, 'already': 1322, 'historic': 1323, 'combinations': 1324, 'check': 1325, 'inspiration': 1326, 'stripes': 1327, 'RTA': 1328, 'nightstand': 1329, 'All': 1330, '(you': 1331, 'buy': 1332, 'masking': 1333, 'store)': 1334, 'brilliant': 1335, 'steady': 1336, 'hand': 1337, 'wielding': 1338, 'paintbrush': 1339, 'Measure': 1340, 'ruler': 1341, 'pencil': 1342, 'marks': 1343, 'tip': 1344, 'Colonial': 1345, 'homemakers': 1346, 'flat-weave': 1347, 'Dhurries': 1348, 'needlepoint': 1349, 'chain-stitch': 1350, 'transform': 1351, 'desks': 1352, 'framed': 1353, 'family': 1354, 'photos': 1355, 'deep': 1356, 'glass-fronted': 1357, 'TV': 1358, 'minibar': 1359, 'living': 1360, 'Punch': 1361, 'hole': 1362, 'slip': 1363, 'wires': 1364, 'Display': 1365, 'buckets': 1366, 'pillows': 1367, 'convert': 1368, 'lumber': 1369, 'sorts': 1370, 'outside': 1371, 'flatter': 1372, 'seats': 1373, 'fluffier': 1374, 'decoration': 1375, 'Handle': 1376, 'hygienically': 1377, 'Handling': 1378, 'ensure': 1379, 'minimal': 1380, 'wear': 1381, 'tear': 1382, 'accidental': 1383, 'spills': 1384, 'permanent': 1385, 'stains': 1386, 'Wash': 1387, 'avoid': 1388, 'having': 1389, 'liquids': 1390, 'reading': 1391, 'ideal': 1392, 'condition': 1393, 'Avoid': 1394, 'saliva': 1395, 'instead': 1396, 'sponge': 1397, 'cotton': 1398, 'gloves': 1399, 'rare': 1400, 'bulky': 1401, 'bookmarks': 1402, 'Large': 1403, 'stress': 1404, 'binding': 1405, 'impressions': 1406, 'indentations': 1407, 'page': 1408, 'being': 1409, 'accidentally': 1410, 'ripped': 1411, 'torn': 1412, 'thread': 1413, 'silk': 1414, 'bookmark': 1415, 'unwanted': 1416, 'wish': 1417, 'envelope': 1418, 'next': 1419, 'insert': 1420, 'folding': 1421, '“dog-earing”': 1422, 'lasting': 1423, 'widely': 1424, 'Both': 1425, 'paperback': 1426, 'hardback': 1427, 'bound': 1428, 'adhesive': 1429, 'stitching': 1430, 'wider': 1431, 'open': 1432, 'under': 1433, 'caution': 1434, 'hardbacks': 1435, 'stiff': 1436, 'prone': 1437, 'cracking': 1438, 'carefully': 1439, 'Pages': 1440, 'deterioration': 1441, 'become': 1442, 'quite': 1443, 'brittle': 1444, 'turning': 1445, 'order': 1446, 'tears': 1447, 'wrinkles': 1448, 'dimples': 1449, 'rips': 1450, 'bindings': 1451, 'Using': 1452, \"book's\": 1453, 'unnecessarily': 1454, 'holding': 1455, 'uncomfortable': 1456, 'fragility': 1457, 'size': 1458, 'weight': 1459, 'lay': 1460, 'cradle': 1461, 'lap': 1462, 'resume': 1463, 'original': 1464, 'causes': 1465, 'Invest': 1466, 'preserve': 1467, 'Send': 1468, 'specialist': 1469, 'conservation': 1470, 'first': 1471, 'edition': 1472, 'sentimental': 1473, 'value': 1474, 'sending': 1475, 'repair': 1476, 'Book': 1477, 'conservationists': 1478, 'specializations': 1479, 'preservation': 1480, 'national': 1481, 'organizations': 1482, 'Regional': 1483, 'Alliance': 1484, 'Preservation': 1485, '(RAP)': 1486, 'American': 1487, 'Institute': 1488, 'Conservation': 1489, '(AIC)': 1490, 'consultation': 1491, 'dirt': 1492, 'moisture': 1493, 'standing': 1494, 'ponds': 1495, \"you're\": 1496, 'spending': 1497, 'outdoors': 1498, 'higher-up': 1499, 'hill': 1500, 'physical': 1501, 'barrier': 1502, 'insects': 1503, 'spend': 1504, 'camping': 1505, 'backyard': 1506, 'deck': 1507, 'install': 1508, 'mesh': 1509, 'screen': 1510, 'bugs': 1511, 'walk': 1512, 'head': 1513, 'net': 1514, 'paired': 1515, 'long-sleeved': 1516, 'shirt': 1517, 'pants': 1518, 'tent': 1519, 'cabin': 1520, 'openings': 1521, 'allow': 1522, 'airflow': 1523, 'keeping': 1524, 'specialty': 1525, 'sell': 1526, 'screens': 1527, 'nets': 1528, 'Wear': 1529, 'bug': 1530, 'repellant': 1531, 'wristband': 1532, 'bites': 1533, 'go': 1534, 'clothing': 1535, 'brands': 1536, 'lightweight': 1537, 'shirts': 1538, 'wristbands': 1539, 'resting': 1540, 'biting': 1541, 'skin': 1542, 'insect-repellent': 1543, 'layer': 1544, 'clothes': 1545, 'sleeves': 1546, 'trap': 1547, 'divert': 1548, 'jelly': 1549, 'sugary': 1550, '10-20': 1551, 'feet': 1552, 'though': 1553, 'distracted': 1554, 'Usually': 1555, '1-2': 1556, 'tablespoons': 1557, '(05-10': 1558, 'oz)': 1559, 'enough': 1560, 'distract': 1561, 'desserts': 1562, 'granulatedpowdered': 1563, 'sugar': 1564, 'sweet': 1565, 'snack': 1566, 'Hanging': 1567, 'bags': 1568, 'perimeter': 1569, 'flies': 1570, 'garlic': 1571, 'lights': 1572, 'Steep': 1573, '2': 1574, '3': 1575, 'cloves': 1576, '15': 1577, 'minutes': 1578, 'bottle': 1579, 'Spray': 1580, 'bulbs': 1581, 'method': 1582, 'because': 1583, 'emits': 1584, 'smell': 1585, 'repellent': 1586, 'sage': 1587, 'burning': 1588, 'fire': 1589, 'deter': 1590, 'campfire': 1591, 'bonfire': 1592, 'sticks': 1593, 'Sage': 1594, 'pleasant-smelling': 1595, 'than': 1596, 'insect': 1597, 'repellents': 1598, 'alternative': 1599, 'chemical': 1600, 'sprays': 1601, 'especially': 1602, 'effective': 1603, 'repelling': 1604, 'mosquitoes': 1605, 'Light': 1606, 'citronella': 1607, 'mosquitos': 1608, 'bay': 1609, 'Citronella': 1610, 'sweet-smelling': 1611, 'fragrance': 1612, 'repels': 1613, 'mosquito': 1614, 'persistent': 1615, 'problem': 1616, 'rag': 1617, 'cloth': 1618, 'zinc': 1619, 'objects': 1620, 'currently': 1621, 'could': 1622, 'ruined': 1623, 't-shirt': 1624, 'Choose': 1625, 'lemon': 1626, 'vinegar': 1627, \"aren't\": 1628, 'removing': 1629, 'patina': 1630, 'juice': 1631, 'lemons': 1632, 'half': 1633, 'plain': 1634, 'toothpaste': 1635, 'baking': 1636, 'soda': 1637, 'paste': 1638, 'solutions': 1639, 'less': 1640, 'acidic': 1641, 'ie': 1642, 'Arm': 1643, '&': 1644, 'Hammer': 1645, 'Colgate': 1646, 'Aquafresh': 1647, 'depending': 1648, 'teaspoon': 1649, '(for': 1650, 'jewelry)': 1651, 'tablespoon': 1652, 'tabletops': 1653, 'pots)': 1654, 'gradually': 1655, 'mix': 1656, 'forms': 1657, 'Gradually': 1658, 'alternating': 1659, 'drops': 1660, 'toothbrush': 1661, 'cleaning': 1662, 'hard': 1663, 'reach': 1664, 'discard': 1665, 'replace': 1666, 'reuse': 1667, 'Gather': 1668, 'prepare': 1669, 'toilet': 1670, 'bidet': 1671, 'bleach': 1672, 'measuring': 1673, 'cup': 1674, 'pour': 1675, '14': 1676, 'Before': 1677, 'flush': 1678, 'has': 1679, 'rust': 1680, 'Bleach': 1681, 'set': 1682, 'manufacturer': 1683, 'determine': 1684, 'product': 1685, 'bidets': 1686, 'scrub': 1687, 'lifting': 1688, 'lid': 1689, 'Immediately': 1690, 'splashes': 1691, 'rim': 1692, 'floor': 1693, 'Scrub': 1694, 'interior': 1695, 'spiral': 1696, 'towards': 1697, 'waterline': 1698, 'scrubbing': 1699, 'Once': 1700, 'chute': 1701, 'irritation': 1702, 'lungs': 1703, 'corrosive': 1704, 'substance—wear': 1705, 'eyewear': 1706, 'ventilated': 1707, 'body': 1708, 'contact': 1709, 'Flush': 1710, 'fresh': 1711, 'rinse': 1712, 'Gently': 1713, 'tap': 1714, 'return': 1715, 'holder': 1716, 'Ask': 1717, 'seeds': 1718, 'greenhouse': 1719, 'sells': 1720, 'houseplants': 1721, 'ask': 1722, 'someone': 1723, 'identify': 1724, 'county': 1725, 'Extension': 1726, 'office': 1727, 'horticulturist': 1728, 'him': 1729, 'her': 1730, 'identification': 1731, 'clear': 1732, 'photo': 1733, 'close': 1734, 'ups': 1735, 'post': 1736, 'online': 1737, 'forums': 1738, 'communities': 1739, 'diagnostic': 1740, 'clinics': 1741, 'college': 1742, 'horticulture': 1743, 'departments': 1744, 'experts': 1745, 'Identify': 1746, 'houseplant': 1747, 'Collect': 1748, 'reference': 1749, 'making': 1750, 'native': 1751, 'helpful': 1752, 'references': 1753, 'listed': 1754, 'below': 1755, 'library': 1756, 'nurseries': 1757, 'greenhouses': 1758, 'County': 1759, 'offices': 1760, 'US': 1761, 'nominal': 1762, 'fee': 1763, 'Examine': 1764, 'complete': 1765, 'specimen': 1766, 'stems': 1767, 'roots': 1768, 'fruits': 1769, 'Answer': 1770, 'questions': 1771, 'record': 1772, 'notebook': 1773, 'magnification': 1774, 'lens': 1775, 'needed': 1776, 'What': 1777, 'seed': 1778, 'fruit': 1779, 'Does': 1780, 'male': 1781, 'female': 1782, 'sex': 1783, 'organs': 1784, 'See': 1785, 'Tips': 1786, 'How': 1787, 'petals': 1788, 'does': 1789, 'count': 1790, 'term': 1791, 'multiple': 1792, 'sepals--the': 1793, 'leaf-like': 1794, 'sometimes': 1795, 'Split': 1796, 'pods': 1797, 'sections': 1798, 'what': 1799, 'Are': 1800, 'divided': 1801, 'lobes': 1802, 'leaflets': 1803, 'leaf’s': 1804, 'Is': 1805, 'smooth': 1806, '“teeth”': 1807, 'big': 1808, 'jagged': 1809, 'stem': 1810, 'branches': 1811, 'trunk': 1812, '(if': 1813, 'any)': 1814, 'arranged': 1815, 'opposite': 1816, 'alternately': 1817, 'visible': 1818, 'hairs': 1819, 'front': 1820, 'root': 1821, 'system': 1822, 'fibrous': 1823, 'rooted': 1824, 'tubers': 1825, 'Search': 1826, 'seems': 1827, 'Read': 1828, 'description': 1829, 'compare': 1830, 'observations': 1831, 'Compare': 1832, 'stop': 1833, 'Record': 1834, 'name': 1835, 'including': 1836, 'Latin': 1837, 'watering': 1838, 'Catnip': 1839, 'drier': 1840, 'rot': 1841, 'thoroughly': 1842, 'saturate': 1843, 'test': 1844, 'touching': 1845, 'finger': 1846, 'feels': 1847, 'wet': 1848, 'later': 1849, 'fairly': 1850, 'hardy': 1851, 'drought-resistant': 1852, 'concerned': 1853, 'over-watering': 1854, 'Shear': 1855, 'deadhead': 1856, 'promote': 1857, 'growth': 1858, \"plant's\": 1859, 'blooms': 1860, 'finish': 1861, 'spent': 1862, 'one-third': 1863, 'bloom': 1864, 'regularly': 1865, 'Shearing': 1866, 'deadheading': 1867, 'result': 1868, 'bushier': 1869, 'consistently': 1870, 'Divide': 1871, 'systems': 1872, 'propagate': 1873, 'dividing': 1874, 'plant’s': 1875, 'Dig': 1876, 'cluster': 1877, 'stalks': 1878, 'pot': 1879, 'Soak': 1880, 'ball': 1881, 'saturation': 1882, 'trowel': 1883, 'knife': 1884, 'divide': 1885, 'replant': 1886, 'Continue': 1887, 'frequently': 1888, 'would': 1889, 'normal': 1890, 'catnip': 1891, 'Dividing': 1892, 'overgrowth': 1893, 'renew': 1894, 'fading': 1895, 'share': 1896, 'friend': 1897, 'kitty': 1898, 'Cats': 1899, 'attracted': 1900, 'love': 1901, 'nip': 1902, 'delicate': 1903, 'wouldn’t': 1904, 'positioning': 1905, 'places': 1906, 'knocked': 1907, 'fencing': 1908, 'bracing': 1909, 'bamboo': 1910, 'support': 1911, 'laying': 1912, 'Harvest': 1913, 'air': 1914, 'harvest': 1915, 'stalk': 1916, 'base': 1917, 'joint': 1918, 'whole': 1919, 'Cutting': 1920, 'rapid': 1921, 'Air': 1922, 'preserving': 1923, 'sunlit': 1924, 'upside': 1925, 'cool': 1926, 'weeks': 1927, 'closed': 1928, 'door': 1929, 'jumping': 1930, 'getting': 1931, 'they’ve': 1932, 'airtight': 1933, 'focal': 1934, 'shorter': 1935, 'narrow': 1936, \"it's\": 1937, 'narrower': 1938, 'bigger': 1939, 'entertainment': 1940, 'Basically': 1941, 'attention': 1942, 'focused': 1943, 'Break': 1944, 'tactic': 1945, 'particularly': 1946, 'oddly': 1947, 'shaped': 1948, 'Designate': 1949, '\"areas\"': 1950, 'treat': 1951, 'groupings': 1952, 'textiles': 1953, 'helps': 1954, 'coordinate': 1955, 'things': 1956, 'hallway': 1957, 'common': 1958, 'horizontal': 1959, \"that's\": 1960, \"doesn't\": 1961, 'bet': 1962, 'One': 1963, 'well-divided': 1964, 'free-form': 1965, 'organic': 1966, 'wave-like': 1967, 'length': 1968, 'illusion': 1969, 'anything': 1970, 'instance': 1971, 'mural': 1972, 'funky': 1973, 'cross-stitch': 1974, 'pattern': 1975, 'based': 1976, 'sewing': 1977, 'fun': 1978, 'color-blocking': 1979, 'marked': 1980, 'starburst': 1981, 'outline': 1982, 'thing': 1983, 'black': 1984, 'chalkboard': 1985, 'Chalkboard': 1986, 'comes': 1987, 'improvement': 1988, 'draw': 1989, 'pens': 1990, 'Alternatively': 1991, 'kids': 1992, 'town': 1993, 'inspirational': 1994, 'quotes': 1995, 'changing': 1996, 'landscape': 1997, 'option': 1998, 'floral': 1999, 'plaid': 2000, 'textured': 2001, 'Victorian-type': 2002, \"You'll\": 2003, 'wide': 2004, 'wallpapers': 2005, 'everyone': 2006, 'afraid': 2007, 'dark': 2008, 'stain': 2009, 'Go': 2010, 'bedroom': 2011, 'Select': 2012, 'naturally': 2013, 'mahogany': 2014, 'walnut': 2015, 'rosewood': 2016, 'lighter': 2017, 're-stain': 2018, 'sanding': 2019, 'appropriate': 2020, 'high-gloss': 2021, 'lacquer': 2022, 'polished': 2023, 'rougher': 2024, 'matte': 2025, 'aged': 2026, 'quality': 2027, 'casual': 2028, '“shabby-chic”': 2029, 'carving': 2030, 'details': 2031, 'carved': 2032, 'designs': 2033, 'inlays': 2034, 'qualities': 2035, 'older': 2036, 'arched': 2037, 'scalloped': 2038, 'Moroccan': 2039, 'architecture': 2040, 'affix': 2041, 'headboard': 2042, 'drawers': 2043, 'update': 2044, 'exotic': 2045, 'oasis': 2046, 'feeling': 2047, 'sleep': 2048, 'four-poster': 2049, 'canopy': 2050, 'dreamy': 2051, 'hangs': 2052, 'carvings': 2053, 'inlay': 2054, 'simulate': 2055, 'rich': 2056, 'tapestry': 2057, 'rug': 2058, 'matching': 2059, 'nightstands': 2060, 'similar': 2061, 'low': 2062, 'Easily': 2063, 'staining': 2064, 'Update': 2065, 'doesn’t': 2066, 'repainting': 2067, 'bench': 2068, 'thoughtfully': 2069, 'soft': 2070, 'comfortable': 2071, 'low-to-the-ground': 2072, 'seating': 2073, 'fit': 2074, 'accent': 2075, 'typical': 2076, '16”-18”': 2077, 'x': 2078, 'standard': 2079, 'sized': 2080, 'neatly': 2081, 'Standard': 2082, '18': 2083, 'inches': 2084, 'Having': 2085, 'average': 2086, 'comfy': 2087, 'overwhelmed': 2088, 'oversized': 2089, '24': 2090, 'Oversized': 2091, 'feel': 2092, 'inviting': 2093, 'slim': 2094, '16': 2095, 'overwhelm': 2096, 'contrasting': 2097, 'shapes': 2098, 'Accent': 2099, 'main': 2100, 'bolster': 2101, 'round': 2102, 'dimension': 2103, 'sparsely': 2104, 'decorated': 2105, 'Rectangle': 2106, 'supporting': 2107, 'neck': 2108, 'couch': 2109, 'leather': 2110, 'lumbar': 2111, 'won’t': 2112, 'propped': 2113, 'severe': 2114, 'soften': 2115, 'squishy': 2116, 'sharpen': 2117, 'texture': 2118, 'Contrasting': 2119, 'sense': 2120, 'contrast': 2121, 'Contrast': 2122, 'linen': 2123, 'against': 2124, 'fuzzy': 2125, 'stay': 2126, 'traction': 2127, 'shift': 2128, 'high': 2129, 'filling': 2130, 'last': 2131, 'lumpy': 2132, 'saggy': 2133, 'Cheaper': 2134, 'feather': 2135, 'foam': 2136, 'synthetic': 2137, 'lean': 2138, 'cheaper': 2139, 'Instead': 2140, 'goose': 2141, 'softer': 2142, 'higher': 2143, 'vegan': 2144, 'cruelty-free': 2145, 'Pillows': 2146, 'stuffed': 2147, 'wheat': 2148, 'husks': 2149, 'options': 2150, 'rubbing': 2151, 'alcohol': 2152, 'Rubbing': 2153, 'solution': 2154, 'isopropyl': 2155, 'distilled': 2156, 'commonly': 2157, 'sold': 2158, 'box': 2159, 'grocery': 2160, 'hardware': 2161, '60%': 2162, '90%': 2163, 'lower': 2164, 'percentage': 2165, 'Sometimes': 2166, 'ethanol': 2167, 'purposes': 2168, 'interchangeable': 2169, 'disinfectant': 2170, 'properties': 2171, 'kill': 2172, 'bacteria': 2173, 'viruses': 2174, 'Buy': 2175, 'products': 2176, 'aid': 2177, 'surfaces': 2178, 'scratch': 2179, 'microfiber': 2180, 'rags': 2181, '(like': 2182, 'keyboards': 2183, 'mice)': 2184, 'computer': 2185, 'accessories': 2186, 'Q-tips': 2187, 'groves': 2188, 'metal': 2189, 'somewhat': 2190, 'abrasive': 2191, 'Whenever': 2192, 'lessen': 2193, 'transfer': 2194, 'Confirm': 2195, 'begin': 2196, 'important': 2197, 'certain': 2198, 'tag': 2199, 'directions': 2200, 'read': 2201, 'consumer': 2202, 'LCD': 2203, 'LED': 2204, 'smartphone': 2205, 'fine': 2206, 'slick': 2207, 'coating': 2208, 'enables': 2209, 'touchscreen': 2210, 'smoothly': 2211, 'Be': 2212, 'careful': 2213, 'fabrics': 2214, 'finished': 2215, 'Clean': 2216, 'wiping': 2217, 'Wiping': 2218, 'grime': 2219, 'residual': 2220, 'hinder': 2221, 'andor': 2222, 'whether': 2223, \"you'll\": 2224, 'Dampen': 2225, 'Wipe': 2226, 'slowly': 2227, 'blowing': 2228, 'blow': 2229, 'dryer': 2230, 'dust': 2231, 'fibers': 2232, 'cooking': 2233, 'vessels': 2234, 'handle': 2235, 'raw': 2236, 'poultry': 2237, 'meat': 2238, 'fish': 2239, 'Refrigerate': 2240, 'freeze': 2241, 'perishable': 2242, 'ready': 2243, 'Thaw': 2244, 'frozen': 2245, 'cold': 2246, 'Never': 2247, 'thaw': 2248, 'appearance': 2249, 'indicate': 2250, 'spoilage': 2251, 'longest': 2252, 'expiration': 2253, 'date': 2254, \"Don't\": 2255, 'eggs': 2256, 'cracked': 2257, 'temperature': 2258, '40': 2259, 'degrees': 2260, '(44': 2261, 'C)': 2262, 'thermometer': 2263, 'adjust': 2264, 'thermostat': 2265, 'cutlery': 2266, 'boards': 2267, 'soap': 2268, 'Utensils': 2269, 'harmful': 2270, 'vegetables': 2271, 'breads': 2272, 'ready-to-eat': 2273, 'Disinfect': 2274, 'countertops': 2275, 'sink': 2276, 'Boil': 2277, 'marinade': 2278, 'marinating': 2279, 'basting': 2280, 'drawer': 2281, 'contaminating': 2282, 'blood': 2283, 'tight': 2284, 'seepage': 2285, 'Seal': 2286, 'refrigerated': 2287, 'germs': 2288, 'odors': 2289, 'Cook': 2290, 'seafood': 2291, 'clams': 2292, 'oysters': 2293, 'mussels': 2294, 'internal': 2295, 'temperatures': 2296, 'fowl': 2297, 'Experts': 2298, 'advise': 2299, '145': 2300, '(6278': 2301, 'seconds': 2302, 'whites': 2303, 'firm': 2304, 'yellow': 2305, 'yolk': 2306, 'begins': 2307, 'harden': 2308, 'consume': 2309, 'cookie': 2310, 'dough': 2311, 'eggnog': 2312, 'homemade': 2313, 'mayonnaise': 2314, 'dressings': 2315, 'egg': 2316, 'Stores': 2317, 'pasteurized': 2318, 'safer': 2319, 'temperatures--either': 2320, 'cold--until': 2321, 'serving': 2322, 'sponges': 2323, 'collect': 2324, 'particles': 2325, 'harbor': 2326, 'Reduce': 2327, 'dish': 2328, 'cloths': 2329, 'Empty': 2330, 'garbage': 2331, 'disease-carrying': 2332, 'vermin': 2333, 'flour': 2334, 'cereal': 2335, 'crackers': 2336, 'cookies': 2337, 'discourage': 2338, 'pests': 2339, 'parasites': 2340, 'leaking': 2341, 'Fix': 2342, 'Wait': 2343, 'end': 2344, 'shopping': 2345, 'trip': 2346, 'requires': 2347, 'refrigeration': 2348, 'leak': 2349, 'shop': 2350, 'Separate': 2351, 'cart': 2352, 'meats': 2353, 'Prevent': 2354, 'refrigerating': 2355, 'Scrape': 2356, 'extra': 2357, 'grease': 2358, 'oil': 2359, 'acting': 2360, 'spread': 2361, 'easier': 2362, 'fight': 2363, 'razor': 2364, 'blade': 2365, 'slide': 2366, 'substitute': 2367, 'scrape': 2368, 'Blot': 2369, 'blot': 2370, 'blotting': 2371, 'didn’t': 2372, 'scraped': 2373, 'dab': 2374, 'mush': 2375, 'smearing': 2376, 'powder': 2377, 'cornstarch': 2378, 'While': 2379, 'probably': 2380, 'removed': 2381, 'substantial': 2382, 'amount': 2383, 'item': 2384, 'question': 2385, 'sprinkling': 2386, 'corn': 2387, 'starch': 2388, 'absorbent': 2389, 'sprinkle': 2390, '‘': 2391, 'concrete': 2392, 'problems': 2393, 'launder': 2394, 'Carefully': 2395, 'scraper': 2396, 'vacuum': 2397, 'foliage': 2398, 'spots': 2399, 'Start': 2400, 'examining': 2401, 'underneath': 2402, 'brown': 2403, 'Note': 2404, 'healthy': 2405, 'bright': 2406, 'wilted': 2407, 'shiny': 2408, 'been': 2409, 'chemicals': 2410, 'fertilizer': 2411, 'buds': 2412, 'grower': 2413, 'opt': 2414, 'young': 2415, 'healthier': 2416, 'vibrant': 2417, 'signs': 2418, 'disease': 2419, 'examine': 2420, 'pest': 2421, 'crawling': 2422, 'bite': 2423, 'holes': 2424, 'bring': 2425, 'infect': 2426, 'Discuss': 2427, 'maintenance': 2428, 'buying': 2429, 'talk': 2430, 'maintaining': 2431, 'advice': 2432, 'giving': 2433, 'declining': 2434, 'health': 2435, 'nurse': 2436, '“How': 2437, 'I': 2438, 'plants?”': 2439, '“What': 2440, 'my': 2441, 'decline': 2442, 'Opt': 2443, 'anthurium': 2444, 'peace': 2445, 'lily': 2446, 'blooming': 2447, 'poisonous': 2448, 'chewed': 2449, 'swallowed': 2450, 'Naturally': 2451, 'word': 2452, '\"cave\"': 2453, 'loosely': 2454, 'developing': 2455, 'man': 2456, 'cave': 2457, 'attic': 2458, 'unused': 2459, 'basement': 2460, 'anywhere': 2461, 'else': 2462, 'spare': 2463, 'adequate': 2464, 'elbow': 2465, 'abandoned': 2466, 'dig': 2467, 'prepared': 2468, 'negotiate': 2469, 'negotiations': 2470, 'compromises': 2471, 'agree': 2472, 'exchange': 2473, 'allowed': 2474, 'relaxation': 2475, \"one's\": 2476, 'happy': 2477, 'offering': 2478, 'partner': 2479, 'housemate': 2480, 'interested': 2481, 'Setting': 2482, 'exercise': 2483, 'sauna': 2484, 'spa': 2485, 'playroom': 2486, 'astronomy': 2487, 'observatory': 2488, 'Promising': 2489, 'number': 2490, 'repairs': 2491, 'odd': 2492, 'jobs': 2493, 'chores': 2494, 'Arranging': 2495, 'Insulate': 2496, 'sound-proof': 2497, 'Man': 2498, 'caves': 2499, 'start': 2500, 'without': 2501, 'insulation': 2502, 'warmer': 2503, 'installing': 2504, 'fiberglass': 2505, 'studs': 2506, 'Unless': 2507, 'stand-alone': 2508, 'alone': 2509, 'priority': 2510, 'sound': 2511, 'proofing': 2512, 'choosing': 2513, 'midnight': 2514, 'sports': 2515, 'poker': 2516, 'game': 2517, 'interrupted': 2518, 'tired': 2519, 'spouse': 2520, 'neighbor': 2521, '(optional)': 2522, 'relegated': 2523, 'dingiest': 2524, 'smallest': 2525, 'give': 2526, 'coming': 2527, 'On': 2528, 'theater': 2529, 'setups': 2530, 'relaxed': 2531, 'bar-like': 2532, 'atmosphere': 2533, \"couldn't\": 2534, 'think': 2535, 'congratulations': 2536, 'Skipping': 2537, 'step': 2538, 'benefit': 2539, 'decisions': 2540, 'lighting': 2541, 'Install': 2542, 'recessed': 2543, 'dimmer': 2544, 'switch': 2545, 'watch': 2546, 'movies': 2547, 'blackout': 2548, 'pull': 2549, 'afternoon': 2550, 'world': 2551, 'butting': 2552, 'terracotta': 2553, 'statues': 2554, 'growing': 2555, 'myriad': 2556, 'greenery': 2557, 'interest': 2558, 'medium': 2559, 'lavender': 2560, 'geraniums': 2561, 'Not': 2562, 'burnt-orange': 2563, 'pop': 2564, 'purple': 2565, 'swimming': 2566, 'gnome': 2567, 'warrior': 2568, 'stand': 2569, 'guard': 2570, 'tomatoes': 2571, 'spiraling': 2572, 'cucumbers': 2573, 'Line': 2574, 'porches': 2575, 'planters': 2576, 'patio': 2577, 'trade': 2578, 'stone': 2579, 'porcelain': 2580, 'breath': 2581, 'porosity': 2582, 'clay': 2583, 'allows': 2584, 'penetrate': 2585, 'much-needed': 2586, 'nutrients': 2587, 'cement': 2588, 'traditional': 2589, 'orange': 2590, 'stairs': 2591, 'darker': 2592, 'cream': 2593, 'blues': 2594, 'purples': 2595, 'yellows': 2596, 'planter': 2597, 'mimic': 2598, 'Grecian': 2599, 'urns': 2600, 'intricate': 2601, 'fence': 2602, 'An': 2603, 'trick': 2604, 'collages': 2605, 'murals': 2606, 'miniature': 2607, 'geometric': 2608, 'squares': 2609, 'circles': 2610, 'stars': 2611, 'fences': 2612, 'yards': 2613, 'Leave': 2614, 'backdrop': 2615, 'picket': 2616, 'reddish': 2617, 'hues': 2618, 'striking': 2619, 'far': 2620, 'herbs!': 2621, 'wreath': 2622, 'stray': 2623, 'seasonal': 2624, 'spruce': 2625, 'urban': 2626, 'artificial': 2627, 'trees': 2628, 'snip': 2629, 'five': 2630, 'ribbons': 2631, 'push': 2632, 'pins': 2633, 'bouquet': 2634, 'lettering': 2635, 'you’d': 2636, 'spell': 2637, 'craft-supply': 2638, 'wintery': 2639, 'holiday-related': 2640, '“JOY”': 2641, '“THANKS”': 2642, 'longer': 2643, 'word—eg': 2644, '“CHRISTMAS”': 2645, '“NEW': 2646, 'YEAR”': 2647, '“GOD': 2648, 'BLESS”': 2649, 'ornaments': 2650, 'angling': 2651, 'Christmas-specific': 2652, 'durable': 2653, 'candy': 2654, 'canes': 2655, 'Santa': 2656, 'over-the-door': 2657, 'hanger': 2658, 'simpler': 2659, 'stocking': 2660, 'sleigh-bell': 2661, 'doorknob': 2662, 'themed': 2663, 'doorknob-specific': 2664, 'prefer': 2665, 'stockings': 2666, 'cluttering': 2667, 'fewer': 2668, 'cohesion': 2669, 'theming': 2670, 'snowflakes': 2671, 'Incorporate': 2672, 'necessarily': 2673, 'shy': 2674, 'connote': 2675, 'holiday—and': 2676, 'focus': 2677, 'Options': 2678, '(including': 2679, 'those': 2680, 'notebook-sized': 2681, 'paper)': 2682, 'mistletoe': 2683, 'visit': 2684, 'craft': 2685, 'single': 2686, 'cornucopia-like': 2687, 'spraying': 2688, 'spackle': 2689, 'faux-snow': 2690, 'months': 2691, 'Natural': 2692, 'celebrate': 2693, 'November': 2694, 'March': 2695, 'picture': 2696, 'eyeshot': 2697, 'photograph': 2698, 'sight': 2699, 'subject': 2700, 'makes': 2701, 'abstract': 2702, 'print': 2703, 'pleasing': 2704, 'candle': 2705, 'Candles': 2706, 'intense': 2707, 'relax': 2708, 'romantic': 2709, 'evening': 2710, 'Scented': 2711, 'pleasant': 2712, 'aroma': 2713, 'composition': 2714, 'hardcover': 2715, 'elements': 2716, 'exhibition': 2717, 'catalogues': 2718, 'largest': 2719, 'bases': 2720, 'lamps': 2721, 'points': 2722, 'balancing': 2723, 'really': 2724, 'sculpture': 2725, 'small-': 2726, 'medium-sized': 2727, 'complementary': 2728, 'rest': 2729, 'décor': 2730, 'dominated': 2731, 'antiques': 2732, 'subdued': 2733, 'clash': 2734, 'organic-shaped': 2735, 'rectangular': 2736, 'alarm': 2737, 'jewelry': 2738, 'artistic': 2739, 'prevents': 2740, 'clutter': 2741, 'caused': 2742, 'waking': 2743, 'oval': 2744, 'compliment': 2745, 'straight': 2746, 'balance': 2747, 'basket': 2748, 'shells': 2749, 'offset': 2750, 'bedding': 2751, 'tackstrips': 2752, 'reinstall': 2753, 'carpet': 2754, 'wearing': 2755, 'pry-bar': 2756, 'carpeting': 2757, 'inspect': 2758, 'reusable': 2759, 'dull': 2760, 'loose': 2761, 'otherwise': 2762, 'worn': 2763, 'pulling': 2764, 'replacing': 2765, 'anyway': 2766, \"It's\": 2767, 'idea': 2768, 'screws': 2769, 'tacks': 2770, 'Sweep': 2771, 'throw': 2772, \"there'll\": 2773, 'staples': 2774, 'pain': 2775, 'pliers': 2776, 'subfloor': 2777, 'prybar': 2778, 'Carpeting': 2779, 'adhesives': 2780, 'scraping': 2781, 'thorough': 2782, 'sub-floor': 2783, 'removes': 2784, 'got': 2785, \"It'd\": 2786, 'shame': 2787, '$800': 2788, 'worth': 2789, 'squeaky': 2790, 'showing': 2791, 'mold': 2792, 'Walk': 2793, 'bounce': 2794, 'Subfloor': 2795, 'panels': 2796, 'attached': 2797, 'joists': 2798, 'squeak': 2799, 'joist': 2800, 'ring': 2801, 'shank': 2802, 'grooved': 2803, 'gripping': 2804, 'decreasing': 2805, 'ever': 2806, 'About': 2807, 'previous': 2808, 'nail': 2809, 'screw': 2810, 'hammer': 2811, 'was': 2812, 'impacted': 2813, 'serious': 2814, 'replaced': 2815, 'flooring': 2816, 'Vacuum': 2817, 'sweep': 2818, 'shop-vac': 2819, 'remaining': 2820, 'glue-scrapings': 2821, 'moving': 2822, 'forward': 2823, 'installation': 2824, 'laminate': 2825, 'degreaser': 2826, 'cleaned': 2827, 'weaken': 2828, 'Quick': 2829, 'Degreaser': 2830, 'detergent': 2831, 'laundry': 2832, 'Dawn': 2833, 'hardwood': 2834, 'Apply': 2835, 'quickly': 2836, 'soaking': 2837, 'bonding': 2838, 'spilled': 2839, 'warm': 2840, 'liberally': 2841, 'Rub': 2842, 'applied': 2843, 'dabbing': 2844, 'pad': 2845, 'instruct': 2846, 'Rinse': 2847, 'rinsed': 2848, 'chosen': 2849, 'soaks': 2850, 'scrubbed': 2851, 'faucets': 2852, 'spigots': 2853, 'flow': 2854, 'rate': 2855, 'inaccurate': 2856, 'water-fed': 2857, 'position': 2858, 'measure': 2859, 'Water-fed': 2860, 'dishwasher': 2861, 'machine': 2862, 'spigot': 2863, 'faucet': 2864, 'accurate': 2865, 'closest': 2866, 'supply': 2867, 'Water': 2868, 'loses': 2869, 'pressure': 2870, 'travels': 2871, 'pipes': 2872, 'usually': 2873, 'located': 2874, '1': 2875, 'gallon': 2876, '(38': 2877, 'L)': 2878, 'bucket': 2879, 'exactly': 2880, 'finding': 2881, 'gallons': 2882, 'liters': 2883, 'per': 2884, 'minute': 2885, 'needs': 2886, 'timer': 2887, 'takes': 2888, 'mark': 2889, 'crawlspace': 2890, 'heater': 2891, 'connected': 2892, '60': 2893, 'took': 2894, 'house’s': 2895, '(liters)': 2896, 'GPM': 2897, '(LPM)': 2898, 'residential': 2899, 'houses': 2900, 'maintain': 2901, '6': 2902, '(23': 2903, 'washer': 2904, 'shower': 2905, 'operate': 2906, '30': 2907, \"you'd\": 2908, 'calculate': 2909, '6030': 2910, '=': 2911, 'regulator': 2912, 'mean': 2913, 'Hire': 2914, 'plumber': 2915, 'handyman': 2916, 'regulate': 2917, 'shoot': 2918, 'sides': 2919, 'crevices': 2920, 'Call': 2921, 'landlord': 2922, 'company': 2923, 'means': 2924, 'Low': 2925, 'blocked': 2926, 'malfunction': 2927, 'booster': 2928, 'installed': 2929, 'increase': 2930, 'quietly': 2931, 'aloud': 2932, 'friends': 2933, 'art': 2934, 'story': 2935, 'Very': 2936, 'Hungry': 2937, 'Caterpillar': 2938, '\"next\"': 2939, 'gluing': 2940, 'printer': 2941, 'Rainbow': 2942, 'Fish': 2943, 'fret': 2944, 'cozy': 2945, 'blanket': 2946, 'favorite': 2947, 'writing': 2948, 'pass': 2949, 'pen': 2950, 'Write': 2951, 'relative': 2952, 'lists': 2953, 'Play': 2954, 'word-games': 2955, 'Hangman': 2956, 'Mad': 2957, 'Libs': 2958, 'write': 2959, 'DVD': 2960, 'player': 2961, 'fully': 2962, 'charged': 2963, 'dim': 2964, 'apps': 2965, 'Listen': 2966, 'play': 2967, 'music': 2968, 'listening': 2969, 'songs': 2970, 'know': 2971, 'instrument': 2972, 'practice': 2973, 'song': 2974, 'learning': 2975, 'teach': 2976, 'restless': 2977, 'singing': 2978, 'dancing': 2979, 'crafty': 2980, 'creativity': 2981, 'soar': 2982, 'show': 2983, 'gets': 2984, 'Even': 2985, 'crafter': 2986, 'hobby': 2987, 'discover': 2988, 'hidden': 2989, 'talent': 2990, 'Listed': 2991, 'drawing': 2992, 'canvas': 2993, 'yarn': 2994, 'dolls': 2995, 'duct': 2996, 'wallets': 2997, 'knitting': 2998, 'crochet': 2999, 'embroidery': 3000, 'scarf': 3001, 'potholder': 3002, 'play-doh': 3003, 'days)': 3004, 'harsh': 3005, 'built-up': 3006, 'asleep': 3007, 'roommates': 3008, 'members': 3009, 'leaving': 3010, 'Locate': 3011, '0.25': 3012, 'lift': 3013, 'stained': 3014, 'substance': 3015, 'overnight': 3016, 'shut': 3017, 'busy': 3018, 'night’s': 3019, 'Lift': 3020, 'reveal': 3021, 'stain-free': 3022, 'wicker': 3023, 'retain': 3024, 'shine': 3025, 'mild': 3026, 'stubborn': 3027, 'afterwards': 3028, 'salt': 3029, 'non-abrasive': 3030, 'eliminate': 3031, 'mildew': 3032, 'built': 3033, 'cushions': 3034, 'separately': 3035, 'cushion': 3036, 'removable': 3037, 'washing': 3038, 'doubt': 3039, 'gentle': 3040, 'remover': 3041, 'Hose': 3042, 'hose': 3043, 'fiber': 3044, 'destroyed': 3045, 'Washing': 3046, 'mainly': 3047, 'reed': 3048, 'tilt': 3049, 'tighter': 3050, 'weave': 3051, 'looser': 3052, 'wiped': 3053, 'trapped': 3054, 'Steam-clean': 3055, 'steam-cleaner': 3056, 'dirty': 3057, 'hair': 3058, 'full': 3059, 'wind': 3060, 'warping': 3061, 'bending': 3062, 'Ensure': 3063, 'fan': 3064, 'distance': 3065, 'accelerate': 3066, 'placing': 3067, 'Expect': 3068, 'wait': 3069, 'entirely': 3070, 'Protect': 3071, 'transporting': 3072, 'raining': 3073, 'bag': 3074, 'librarian': 3075, 'stuffing': 3076, 'pockets': 3077, 'bent': 3078, 'pencils': 3079, 'highlighters': 3080, 'markers': 3081, 'Animals': 3082, 'chew': 3083, 'Small': 3084, 'bookcase': 3085, 'Someone': 3086, 'bathtub': 3087, \"isn't\": 3088, 'heating': 3089, 'vents': 3090, 'radiators': 3091, 'Heat': 3092, 'Any': 3093, 'flat': 3094, 'paperboard': 3095, 'dog-ear': 3096, 'sticky': 3097, 'notes': 3098, 'school': 3099, 'bus': 3100, \"friend's\": 3101, 'securely': 3102, 'lending': 3103, 'lose': 3104, 'destroy': 3105, 'pay': 3106, 'libraries': 3107, 'notify': 3108, 'patrons': 3109, 'returned': 3110, 'wants': 3111, 'borrow': 3112, 'tell': 3113, 'service': 3114, 'accompany': 3115, 'edges': 3116, 'firmly': 3117, 'fore-edge': 3118, 'tail': 3119, 'extreme': 3120, 'address': 3121, 'Brush': 3122, 'direction': 3123, 'mentally': 3124, 'halves': 3125, 'rather': 3126, 'raised': 3127, 'bands': 3128, 'bumps': 3129, 'snagging': 3130, 'attachment': 3131, 'lowest': 3132, 'finally': 3133, 'cheesecloth': 3134, 'nylon': 3135, 'hovering': 3136, 'jacket': 3137, 'jackets': 3138, 'coverings': 3139, 'generally': 3140, 'glossy': 3141, 'they’re': 3142, 'attractive': 3143, 'dusty': 3144, 'wedges': 3145, 'outwards': 3146, 'Address': 3147, 'mustiness': 3148, 'musty': 3149, 'isolate': 3150, 'resealable': 3151, 'unscented': 3152, 'litter': 3153, 'minimum': 3154, '12': 3155, 'peanuts': 3156, 'mess': 3157, 'storing': 3158, 'pair': 3159, 'pantyhose': 3160, 'contained': 3161, 'drain': 3162, 'properly': 3163, 'view': 3164, 'Because': 3165, 'keys': 3166, 'sinking': 3167, 'polystyrene': 3168, 'key': 3169, 'chain': 3170, 'afloat': 3171, 'Stuff': 3172, 'zippered': 3173, 'fillers': 3174, 'bean': 3175, 'Sew': 3176, 'costumes': 3177, 'fat': 3178, 'belly': 3179, 'rippling': 3180, 'muscles': 3181, 'stuff': 3182, 'scarecrow': 3183, 'popcorn': 3184, 'festive': 3185, 'snowman': 3186, 'snake': 3187, 'creation': 3188, 'Simply': 3189, 'coolers': 3190, 'lunchboxes': 3191, 'zipper-locked': 3192, 'cooler': 3193, 'contents': 3194, 'zip-closure': 3195, 'lunch': 3196, 'flame': 3197, 'retardant': 3198, 'Pin': 3199, 'Glue': 3200, 'magnet': 3201, 'peanut': 3202, 'stamps': 3203, 'Stamp': 3204, 'scrapbooks': 3205, 'dipped': 3206, 'resemble': 3207, '“S”': 3208, '“C”': 3209, 'half-moon': 3210, 'stamper': 3211, 'triangles': 3212, 'sharp-pointed': 3213, 'tools': 3214, 'needle-nose': 3215, 'screwdrivers': 3216, 'jabbed': 3217, 'poked': 3218, 'rummaging': 3219, 'toolbox': 3220, 'legal': 3221, 'exterminate': 3222, 'relocate': 3223, 'voles': 3224, 'prohibited': 3225, 'traps': 3226, 'peak': 3227, 'reproduction': 3228, 'lethal': 3229, 'budget': 3230, 'perpendicular': 3231, 'runways': 3232, 'vole': 3233, 'entrances': 3234, 'tunnels': 3235, 'captured': 3236, 'Mouse': 3237, 'snap': 3238, 'cheapest': 3239, 'employ': 3240, 'late': 3241, 'early': 3242, 'Employ': 3243, 'one-time': 3244, 'dealing': 3245, 'mole': 3246, 'carcass': 3247, 'feed': 3248, 'attract': 3249, 'dangerous': 3250, 'covering': 3251, 'disguise': 3252, 'activity': 3253, 'apple': 3254, 'bits': 3255, 'oatmeal': 3256, 'butter': 3257, 'bait': 3258, 'lure': 3259, 'capture': 3260, 'illegal': 3261, 'humane': 3262, 'Havahart': 3263, 'SNG': 3264, 'Sherman': 3265, 'alive': 3266, 'Remember': 3267, 'caught': 3268, 'homes': 3269, 'mile': 3270, 'Humane': 3271, 'population': 3272, 'cost': 3273, 'labour': 3274, 'rise': 3275, '-': 3276, '50': 3277, 'poison': 3278, 'Poison': 3279, 'harm': 3280, 'finicky': 3281, 'diet': 3282, 'scarcity': 3283, 'Always': 3284, 'obey': 3285, 'safest': 3286, 'baits': 3287, 'Warfarin-based': 3288, 'Rodex': 3289, 'Kaput': 3290, 'Blocks': 3291, 'D-Con': 3292, 'purchased': 3293, 'Warfarin': 3294, 'acts': 3295, 'clotting': 3296, '5': 3297, 'rotate': 3298, 'week': 3299, 'Follow': 3300, 'Utilize': 3301, 'tunnel': 3302, 'open-ended': 3303, '--': 3304, 'organisms': 3305, 'associated': 3306, 'plague': 3307, 'tularemia': 3308, 'bat': 3309, 'land': 3310, 'Catching': 3311, 'injure': 3312, 'scratching': 3313, 'patient': 3314, 'noise': 3315, 'scares': 3316, 'encourage': 3317, 'mid-flight': 3318, 'panic': 3319, 'attempt': 3320, 'Waiting': 3321, 'opportunity': 3322, 'catch': 3323, 'lands': 3324, 'stationary': 3325, 'diameter': 3326, 'injuring': 3327, 'crushing': 3328, 'wing': 3329, 'ear': 3330, 'Approach': 3331, 'escape': 3332, 'Slide': 3333, 'enclose': 3334, 'Carry': 3335, 'release': 3336, 'preferable': 3337, 'nightfall': 3338, 'Catch': 3339, 'Another': 3340, 'viable': 3341, 'decent': 3342, 'landed': 3343, 'thickness': 3344, 'involve': 3345, 'direct': 3346, 'rapidly': 3347, 'fly': 3348, 'Pull': 3349, 'driving': 3350, 'safely': 3351, 'pulled': 3352, 'road': 3353, 'parking': 3354, 'final': 3355, 'destination': 3356, 'treating': 3357, 'spill': 3358, 'occurred': 3359, 'deal': 3360, 'easiest': 3361, 'till': 3362, 'napkin': 3363, 'absorb': 3364, 'rub': 3365, 'press': 3366, 'Repeat': 3367, 'napkins': 3368, 'becomes': 3369, 'absorbed': 3370, 'solid': 3371, 'grounds': 3372, 'It’s': 3373, 'stuck': 3374, 'blotted': 3375, 'solids': 3376, 'creases': 3377, 'suck': 3378, 'initially': 3379, 'dishwashing': 3380, 'lather': 3381, 'motions': 3382, 'event': 3383, 'birthday': 3384, 'limit': 3385, '2-3': 3386, 'correspond': 3387, 'buffet': 3388, 'centerpiece': 3389, 'incorporates': 3390, 'props': 3391, 'hint': 3392, 'edible': 3393, 'garnish': 3394, 'cinnamon': 3395, 'seashells': 3396, 'overdoing': 3397, 'classy': 3398, 'gaudy': 3399, 'overwhelming': 3400, 'Ideally': 3401, 'enhance': 3402, 'Also': 3403, 'non-edible': 3404, \"people's\": 3405, 'mouths': 3406, 'Coordinate': 3407, 'runner': 3408, 'placemats': 3409, 'Napkins': 3410, 'Placemats': 3411, 'optional': 3412, 'plenty': 3413, 'Table': 3414, 'runners': 3415, '(15': 3416, 'cm)': 3417, 'cardstock': 3418, 'written': 3419, 'font': 3420, 'guest': 3421, 'vegetarian': 3422, 'gluten': 3423, 'menu': 3424, 'served': 3425, 'easel': 3426, 'beginning': 3427, 'That': 3428, 'guests': 3429, 'informed': 3430, 'choices': 3431, 'parents': 3432, 'steps': 3433, 'lighters': 3434, 'hurt': 3435, 'stove': 3436, 'boiling': 3437, 'falls': 3438, 'face': 3439, 'badly': 3440, 'burnt': 3441, 'curious': 3442, 'handles': 3443, 'centre': 3444, 'adult': 3445, 'overheat': 3446, 'fallen': 3447, 'coals': 3448, 'flames': 3449, 'burn': 3450, 'brothers': 3451, 'sisters': 3452, 'Teach': 3453, 'close-fitting': 3454, 'nightwear': 3455, 'Loose': 3456, 'trail': 3457, 'Close-fitting': 3458, 'pajamas': 3459, 'electricity': 3460, 'wise': 3461, 'fingers': 3462, 'electrical': 3463, 'socket': 3464, 'carries': 3465, 'cords': 3466, 'Practice': 3467, 'drill': 3468, 'happened': 3469, 'automatically': 3470, 'Stop': 3471, 'screaming': 3472, 'crying': 3473, 'panicking': 3474, 'Drop': 3475, 'Roll': 3476, 'roll': 3477, 'Know': 3478, 'starts': 3479, 'crawl': 3480, 'There': 3481, 'rain': 3482, 'snow': 3483, 'precipitation': 3484, 'hail': 3485, 'dew': 3486, 'obstruct': 3487, 'funnel': 3488, 'stream': 3489, 'river': 3490, 'lake': 3491, 'clear-running': 3492, 'lakes': 3493, 'rivers': 3494, 'polluted': 3495, 'drink': 3496, 'looks': 3497, 'disinfected': 3498, 'Filter': 3499, 'Pass': 3500, 'collected': 3501, 'sort': 3502, 'filter': 3503, 'gunk': 3504, 'sock': 3505, 'porous': 3506, 'disinfect': 3507, 'boiled': 3508, 'aluminum': 3509, 'ceramic': 3510, 'pinch': 3511, 'boil': 3512, 'bark': 3513, 'vessel': 3514, 'solar': 3515, 'slightly': 3516, 'taller': 3517, 'vegetation': 3518, 'Weigh': 3519, 'rocks': 3520, 'angled': 3521, 'Condensation': 3522, 'formed': 3523, 'condensation': 3524, 'sizable': 3525, 'Lessen': 3526, '(field': 3527, 'Weed': 3528, 'Plant': 3529, 'dense': 3530, 'coverage': 3531, 'coyote': 3532, 'fox': 3533, 'urine': 3534, 'scent': 3535, 'act': 3536, 'deterrent': 3537, 'mouse': 3538, 'rat': 3539, 'Free': 3540, 'hogs': 3541, 'Epsom': 3542, 'salts': 3543, 'ammonia': 3544, 'soaked': 3545, 'flamethrower': 3546, 'underground': 3547, 'Other': 3548, 'smoking': 3549, 'jump': 3550, 'rabbits': 3551, 'gardens': 3552, 'chicken-wire': 3553, 'buried': 3554, \"1'\": 3555, 'foxglove': 3556, 'monkshood': 3557, 'detested': 3558, 'bunnies': 3559, 'moles': 3560, 'daffodils': 3561, 'caper': 3562, 'spurge': 3563, 'castor': 3564, 'oil-soaked': 3565, 'heavily': 3566, 'trafficked': 3567, 'hunting': 3568, 'rid': 3569, 'raccoons': 3570, 'acoustic': 3571, 'jalapeño': 3572, 'cayenne': 3573, 'pepper': 3574, 'beavers': 3575, 'Surround': 3576, 'trunks': 3577, 'heavy-gauge': 3578, 'deer': 3579, 'affected': 3580, 'sauce': 3581, 'bees': 3582, 'unobtrusive': 3583, 'nest': 3584, 'wasp': 3585, 'killer': 3586, 'eradicate': 3587, 'birdbaths': 3588, 'etc': 3589, 'providing': 3590, 'breeding': 3591, 'rosemary': 3592, 'bushes': 3593, 'grilling': 3594, 'toss': 3595, 'Feed': 3596, 'slugs': 3597, 'beer': 3598, 'wine': 3599, 'drunk': 3600, 'drown': 3601, 'ward': 3602, 'mice': 3603, 'Dab': 3604, 'peppermint': 3605, 'animal': 3606, 'spotted': 3607, 'bats': 3608, 'Illuminate': 3609, 'strong': 3610, 'drafts': 3611, 'fans': 3612, 'mothballs': 3613, 'repel': 3614, 'mammals': 3615, 'squirrels': 3616, 'Scare': 3617, 'lair': 3618, 'forage': 3619, 'entrance': 3620, 'Zen': 3621, 'Assess': 3622, 'available': 3623, 'fills': 3624, 'desk': 3625, 'scale': 3626, 'sand': 3627, 'gravel': 3628, 'Sand': 3629, 'matrix': 3630, '2\"': 3631, '4\"': 3632, 'railroad': 3633, 'ties': 3634, 'desktop': 3635, 'completed': 3636, 'varnishing': 3637, 'weed': 3638, 'retainer': 3639, 'receive': 3640, 'appeal': 3641, 'cleanliness': 3642, 'Keeping': 3643, 'weeds': 3644, 'evenly': 3645, 'aquarium': 3646, 'call': 3647, 'quarry': 3648, 'landscaping': 3649, 'selected': 3650, 'mossy': 3651, 'off-center': 3652, 'partially': 3653, 'submerged': 3654, 'additions': 3655, 'peaceful': 3656, 'feng': 3657, 'shui': 3658, 'karma': 3659, 'dharma': 3660, 'enjoying': 3661, 'zen': 3662, 'balanced': 3663, 'major': 3664, 'possibly': 3665, 'send': 3666, 'yin': 3667, 'yang': 3668, 'whirlwind': 3669, '(which': 3670, 'NOT': 3671, 'good!)': 3672, '*': 3673, 'Rake': 3674, 'curving': 3675, 'strokes': 3676, 'represent': 3677, 'ripples': 3678, 'accentuate': 3679, 'weathered': 3680, 'sign': 3681, 'distinctive': 3682, 'meaningful': 3683, 'message': 3684, 'restaurant': 3685, 'neighborhood': 3686, 'nostalgic': 3687, 'painter': 3688, 'Brighten': 3689, 'Fresh': 3690, 'staple': 3691, 'Farmhouse': 3692, 'background': 3693, 'energy': 3694, 'freshly': 3695, 'Whatever': 3696, 'liven': 3697, 'mason': 3698, 'jars': 3699, 'Industrial': 3700, 'authentic': 3701, 'Shop': 3702, 'pillowcases': 3703, 'throws': 3704, 'sofas': 3705, 'prioritize': 3706, 'Linen': 3707, 'wool': 3708, 'hallmarks': 3709, 'excessively': 3710, 'energetic': 3711, 'neon': 3712, 'baskets': 3713, 'mud': 3714, 'magazines': 3715, 'utilitarian': 3716, 'practical': 3717, 'hats': 3718, 'mitts': 3719, 'accessory': 3720, 'apartment': 3721, '\"sub-rooms\"': 3722, 'studio': 3723, 'functions': 3724, 'rooms': 3725, 'Couches': 3726, 'armchairs': 3727, 'loveseats': 3728, '\"living': 3729, 'room\"': 3730, 'similarly': 3731, 'sleeping': 3732, 'Rugs': 3733, 'shelves': 3734, 'anchor': 3735, 'sub-rooms': 3736, 'organized': 3737, 'placed': 3738, 'parallel': 3739, 'cramped': 3740, 'cluttered': 3741, 'investing': 3742, 'pull-out': 3743, 'low-rise': 3744, 'obstructing': 3745, 'blocking': 3746, 'moveable': 3747, 'dividers': 3748, 'Incorporating': 3749, 'privacy': 3750, 'Folding': 3751, 'ceiling-mounted': 3752, 'drapes': 3753, 'aesthetically-pleasing': 3754, 'ratios': 3755, 'Room': 3756, '13': 3757, '23': 3758, 'proportions': 3759, 'Think': 3760, 'utilize': 3761, 'massive': 3762, 'wardrobe': 3763, 'separator': 3764, 'Placing': 3765, 'doorways': 3766, 'upward': 3767, '(again': 3768, 'impression': 3769, 'space)': 3770, 'legs': 3771, '(as': 3772, 'opposed': 3773, 'draped': 3774, 'cloth)': 3775, 'airiness': 3776, 'Clear': 3777, 'pathway': 3778, 'block': 3779, 'Pushing': 3780, 'maximize': 3781, 'claustrophobic': 3782, 'walking': 3783, 'hassle': 3784, 'arranging': 3785, 'apartments': 3786, 'quantity': 3787, 'couches': 3788, 'always': 3789, 'latter': 3790, 'upon': 3791, 'Columns': 3792, 'parties': 3793, 'events': 3794, 'palette': 3795, 'Request': 3796, 'chips': 3797, 'columns': 3798, 'temporary': 3799, 'finishes': 3800, 'tutorials': 3801, 'barn': 3802, 'Tuscan': 3803, 'mind': 3804, 'atop': 3805, 'Half': 3806, 'permanently': 3807, 'hallways': 3808, 'traffic': 3809, 'bumped': 3810, 'Garlands': 3811, 'versatile': 3812, 'strands': 3813, 'adjusted': 3814, 'almost': 3815, 'occasion': 3816, 'wreath-like': 3817, 'faux-pine': 3818, 'sprigs': 3819, \"baby's\": 3820, 'felt': 3821, 'pendants': 3822, 'banner': 3823, 'tops': 3824, \"child's\": 3825, 'party': 3826, \"event's\": 3827, 'leafy': 3828, 'twisted': 3829, 'column': 3830, 'light-weight': 3831, 'First': 3832, 'chiffon': 3833, 'cross': 3834, 'stage': 3835, 'ceremony': 3836, 'strings': 3837, 'afterward': 3838, 'Plugging': 3839, 'strand': 3840, 'glow': 3841, 'hung': 3842, 'garden-like': 3843, 'crystal': 3844, 'mineral': 3845, 'appeals': 3846, 'costly': 3847, 'overwhelmingly': 3848, 'gemstones': 3849, 'noticeable': 3850, 'arts': 3851, 'crafts': 3852, 'gemstone': 3853, 'citrines': 3854, 'aquamarines': 3855, 'emeralds': 3856, 'rubies': 3857, 'Succulents': 3858, 'Amethyst': 3859, 'turquoise': 3860, 'unexpected': 3861, 'graceful': 3862, 'marble': 3863, 'den': 3864, 'coziness': 3865, 'Cream': 3866, 'gray': 3867, 'unobjectionable': 3868, 'light-colored': 3869, 'Bright': 3870, 'fluorescent': 3871, 'reds': 3872, 'oranges': 3873, 'high-energy': 3874, 'deciding': 3875, 'surround': 3876, 'Thick': 3877, 'plush': 3878, 'luxurious': 3879, 'Each': 3880, 'television': 3881, 'faces': 3882, 'semi-circle': 3883, 'conversation': 3884, 'L-shaped': 3885, 'Assuming': 3886, \"there's\": 3887, 'people': 3888, 'who': 3889, 'solitary': 3890, 'activities': 3891, 'gaming': 3892, 'movie': 3893, 'projector': 3894, 'ten': 3895, '(3': 3896, 'meters)': 3897, 'overhead': 3898, 'Harsh': 3899, 'kills': 3900, 'Finish': 3901, 'Decorative': 3902, 'austere': 3903, 'Wall': 3904, 'shadow': 3905, 'shallow': 3906, 'framing': 3907, 'border': 3908, 'jersey': 3909, 'displayed': 3910, 'box’s': 3911, 'backing': 3912, 'Store-bought': 3913, 'cork': 3914, 'spice': 3915, 'shares': 3916, 'contrasts': 3917, 'jersey’s': 3918, 'displaying': 3919, 'basic': 3920, 'strips': 3921, 'Press': 3922, 'dressmaker': 3923, 'shoulders': 3924, 'exposed': 3925, 'heavier': 3926, 'velcro': 3927, 'straps': 3928, 'rested': 3929, 'jerseys': 3930, 'playing': 3931, 'pucks': 3932, 'team': 3933, 'memorabilia': 3934, 'flags': 3935, 'collectible': 3936, 'hook': 3937, 'reverse': 3938, 'mount': 3939, 'generic': 3940, 'alter': 3941, 'strip': 3942, 'frame’s': 3943, 'reattach': 3944, 'bump': 3945, 'enjoy': 3946, 'chimes': 3947, 'tubes': 3948, 'clapper': 3949, 'refinish': 3950, 'light-grade': 3951, 'sandpaper': 3952, 'intended': 3953, 'strikes': 3954, 'fancy': 3955, 'two-tone': 3956, 'three-tone': 3957, 'tube’s': 3958, 'Give': 3959, 'tube': 3960, 'Stain': 3961, 'wipes': 3962, 'brushing': 3963, 'station': 3964, 'newspaper': 3965, 'complicated': 3966, 'faster': 3967, 'poke': 3968, 'toothpicks': 3969, 'clogging': 3970, 'weathering': 3971, 'spray-on': 3972, 'quick': 3973, 'brush-on': 3974, 'Adorn': 3975, 'staircase': 3976, 'viewer': 3977, 'image': 3978, 'equally': 3979, 'eras': 3980, 'black-and-white': 3981, 'sepia': 3982, '20': 3983, 'years': 3984, 'ago': 3985, 'city': 3986, 'map': 3987, 'aerial': 3988, 'streets': 3989, 'parks': 3990, 'engraving': 3991, 'lithograph': 3992, 'unframed': 3993, 'greet': 3994, 'monochromatic': 3995, 'engravings': 3996, 'Black-and-white': 3997, 'depictions': 3998, 'everyday': 3999, 'desire': 4000, 'country': 4001, 'scene': 4002, 'acrylic': 4003, 'reproductions': 4004, 'afford': 4005, 'Impressionist': 4006, 'masterpiece': 4007, 'latest': 4008, 'auction': 4009, 'catalogue': 4010, 'websites': 4011, 'reasonable': 4012, 'prices': 4013, 'posters': 4014, 'modern': 4015, 'Examples': 4016, 'propaganda': 4017, 'World': 4018, 'War': 4019, 'psychedelic': 4020, 'imagery': 4021, '1960s': 4022, '70s': 4023, 'dinging': 4024, 'classic': 4025, 'Spice': 4026, 'beige': 4027, 'brightly': 4028, 'bolder': 4029, 'empty': 4030, 'sundial': 4031, 'circular': 4032, 'dial': 4033, 'pedestal': 4034, 'armillary': 4035, 'sphere': 4036, 'definitely': 4037, \"everyone's\": 4038, 'Embellish': 4039, 'ordinary': 4040, 'lampshades': 4041, 'lampshade': 4042, 'delightful': 4043, 'macrame': 4044, 'knotted': 4045, 'meaning': 4046, 'custom': 4047, 'DIY': 4048, 'knotting': 4049, 'cord': 4050, 'beads': 4051, 'tassels': 4052, 'complexity': 4053, 'Weave': 4054, 'clever': 4055, 'unhappy': 4056, 'loveseat': 4057, 'recliner': 4058, 'accents': 4059, 'situation': 4060, 'Drape': 4061, 'enchanting': 4062, 'resurface': 4063, 'foot': 4064, 'stool': 4065, 'handmade': 4066, 'Top': 4067, 'assortment': 4068, 'designed': 4069, 'handy': 4070, 'optimum': 4071, 'projects': 4072, 't-shirts': 4073, 'sets': 4074, 'spacious': 4075, 'awfully': 4076, 'knit': 4077, 'negative': 4078, 'settings': 4079, 'cooked': 4080, 'springtime': 4081, 'whenever': 4082, 'inject': 4083, 'brightness': 4084, 'ornamentation': 4085, 'Outline': 4086, 'portrait': 4087, 'Hide': 4088, 'uninspired': 4089, 'Textile': 4090, 'perfectly': 4091, 'decked': 4092, 'Towel': 4093, 'measured': 4094, 'grams': 4095, 'meter': 4096, '(GSM)': 4097, '300-400': 4098, 'GSM': 4099, 'acceptable': 4100, 'gym': 4101, 'bath': 4102, 'low-weight': 4103, '400-600': 4104, 'Heavier': 4105, '600-900': 4106, 'Heavy': 4107, 'thinner': 4108, 'Unfortunately': 4109, 'Appropriately': 4110, 'high-quality': 4111, 'varieties': 4112, 'Pima': 4113, 'Egypt': 4114, 'Terrycloth': 4115, 'constructed': 4116, 'loops': 4117, 'absorbency': 4118, 'Towels': 4119, 'chafe': 4120, 'irritate': 4121, 'People': 4122, 'sensitive': 4123, 'tend': 4124, 'softness': 4125, 'rayon': 4126, 'Turkish': 4127, 'exceptionally': 4128, 'softener': 4129, 'flammable': 4130, 'softest': 4131, 'long-staple': 4132, 'extra-long-staple': 4133, '(ELS)': 4134, 'Egyptian': 4135, 'cottons': 4136, 'ELS': 4137, 'Supima': 4138, 'MicroCotton': 4139, 'shred': 4140, 'two-ply': 4141, 'one-ply': 4142, 'Cotton': 4143, 'cottonpolyester': 4144, 'hybrid': 4145, 'pure': 4146, 'stronger': 4147, 'combed': 4148, 'composed': 4149, 'threads': 4150, 'undesirable': 4151, 'ringspun': 4152, 'Ringspun': 4153, 'combines': 4154, 'finer': 4155, 'terrycloth': 4156, 'woven': 4157, 'knitted': 4158, 'Their': 4159, 'durability': 4160, 'overboard': 4161, 'tiered': 4162, 'calendar': 4163, 'functional': 4164, 'mugs': 4165, 'tea': 4166, 'yours': 4167, 'makeup': 4168, 'polish': 4169, 'phrases': 4170, 'scrapbooking': 4171, 'rhinestones': 4172, 'mini': 4173, 'holiday-themed': 4174, 'treats': 4175, 'jar': 4176, 'tiers': 4177, 'nothing': 4178, 'Best': 4179, 'mints': 4180, 'chocolate': 4181, 'trick-or-treat': 4182, 'St': 4183, \"Patrick's\": 4184, 'Day': 4185, 'coins': 4186, 'skittles': 4187, 'M&Ms': 4188, \"Valentine's\": 4189, 'hearts': 4190, '(such': 4191, 'Bunny)': 4192, 'depict': 4193, 'graduation': 4194, 'ceremonies': 4195, 'holertree': 4196, 'Tiered': 4197, 'trays': 4198, 'collections': 4199, 'cows': 4200, 'cow-related': 4201, 'figurines': 4202, 'creamers': 4203, 'drip': 4204, 'seep': 4205, 'Either': 4206, 'dunk': 4207, 'Wring': 4208, 'Too': 4209, 'Although': 4210, 'wringing': 4211, 'wetting': 4212, 'buildup': 4213, 'dampened': 4214, 'cultured': 4215, 'With': 4216, 'swipe': 4217, 'afflicted': 4218, 'douse': 4219, 'Hot': 4220, 'stubbornly': 4221, 'happening': 4222, 'Treat': 4223, 'agent': 4224, 'abrasives': 4225, 'compromise': 4226, 'results': 4227, 'usage': 4228, 'vary': 4229, 'combine': 4230, 'Barkeeper’s': 4231, 'Friend': 4232, 'Generally': 4233, 'cleaners': 4234, 'portion': 4235, 'dirtiness': 4236, 'counter': 4237, 'Polish': 4238, 'suitable': 4239, 'polishing': 4240, 'restore': 4241, 'luster': 4242, 'Mold': 4243, 'grow': 4244, 'ceilings': 4245, 'Pay': 4246, 'showertubs': 4247, 'bathrooms': 4248, 'notice': 4249, 'earthy': 4250, 'scents': 4251, 'forest': 4252, 'dehumidifier': 4253, 'smells': 4254, 'garages': 4255, 'attics': 4256, 'basements': 4257, 'Reflect': 4258, 'suffer': 4259, 'allergies': 4260, 'sneeze': 4261, 'cough': 4262, 'wheeze': 4263, 'runny': 4264, 'nose': 4265, 'bacteria-related': 4266, 'starve': 4267, 'Test': 4268, 'home’s': 4269, 'kit': 4270, 'hire': 4271, 'testing': 4272, 'contaminants': 4273, 'tests': 4274, 'concerns': 4275, 'asbestos': 4276, 'radon': 4277, 're-purpose': 4278, 'tins': 4279, 'Convert': 4280, 'tool': 4281, 'utility': 4282, 'needles': 4283, 'crayon': 4284, 'Tins': 4285, 'organize': 4286, 'purse': 4287, 'credit': 4288, 'gifts': 4289, 'gift': 4290, 'card': 4291, 'money': 4292, 'mixed': 4293, 'media': 4294, 'artists': 4295, 'shadowboxes': 4296, 'layered': 4297, 'images': 4298, 'goodies': 4299, 'grouping': 4300, 'advanced': 4301, 'etching': 4302, 'device': 4303, 'creates': 4304, 'involves': 4305, 'tin': 4306, 'saltwater': 4307, 'etch': 4308, 'altered': 4309, 'robots': 4310, 'simplest': 4311, 'scraps': 4312, 'confetti': 4313, 'polymer': 4314, 'residue': 4315, 'depends': 4316, 'adhere': 4317, 'lining': 4318, 'mouth': 4319, 'N-95': 4320, 'respirator': 4321, 'breathing': 4322, 'spores': 4323, 'elbow-length': 4324, 'rubber': 4325, 'goggles': 4326, 'airborne': 4327, 'Mix': 4328, 'equal': 4329, 'Dry': 4330, 'noticed': 4331, 'inspection': 4332, 'professional': 4333, 'sweating': 4334, 'caulk': 4335, 'Icynene': 4336, 'cracks': 4337, 'doors': 4338, 'critical': 4339, 'junctures': 4340, 'meet': 4341, 'stripping': 4342, 'panes': 4343, 'Have': 4344, 'ducts': 4345, 'trained': 4346, 'crucial': 4347, 'recurring': 4348, 'despite': 4349, 'efforts': 4350, 'professionals': 4351, 'department': 4352, 'recommendation': 4353, 'tiles': 4354, 'unfit': 4355, 'management': 4356, 'Seek': 4357, 'assistance': 4358, 'heirlooms': 4359, 'consult': 4360, 'senior': 4361, 'museum': 4362, 'conservator': 4363, 'refer': 4364, 'trusted': 4365, 'expert': 4366, 'searches': 4367, 'expert’s': 4368, 'accredited': 4369, 'organization': 4370, 'Library': 4371, 'Association': 4372, '(ALA)': 4373, 'pan': 4374, 'third': 4375, 'non-stick': 4376, 'cast': 4377, 'iron': 4378, 'pans': 4379, 'Putting': 4380, 'sud': 4381, 'copper': 4382, 'stainless': 4383, 'steel': 4384, 'affect': 4385, 'Concentrate': 4386, 'stuck-on': 4387, 'Hold': 4388, 'suds': 4389, 'rack': 4390, 'worry': 4391, 'absorbents': 4392, '“soak': 4393, 'up”': 4394, 'inexpensive': 4395, 'non-toxic': 4396, 'Neither': 4397, 'nor': 4398, 'environment': 4399, 'softly': 4400, 'harshly': 4401, 'force': 4402, 'carpet’s': 4403, 'Now': 4404, 'Squirt': 4405, 'alarmed': 4406, 'Inspect': 4407, 'fibres': 4408, 'traces': 4409, 'detritus': 4410, 'shipping': 4411, 'We': 4412, 'suggest': 4413, 'bristled': 4414, 'broom': 4415, 'we': 4416, 'recommend': 4417, 'mask': 4418, 'expedite': 4419, 'greatly': 4420, 'You’re': 4421, 'confined': 4422, 'seams': 4423, 'excessive': 4424, 'repairing': 4425, 'potentially': 4426, 'weak': 4427, 'sprayed': 4428, 'expose': 4429, 'corrosion': 4430, 'cider': 4431, 'isn’t': 4432, 'fast': 4433, 'cargo': 4434, 'foil': 4435, 'grit': 4436, 'orbital': 4437, 'sander': 4438, 'belt': 4439, 'speed': 4440, 'attacking': 4441, 'peeling': 4442, 'flaking': 4443, 'primer': 4444, 'future': 4445, 'Finally': 4446, 'exterior': 4447, 'however': 4448, 'there’s': 4449, 'associate': 4450, 'filters': 4451, 'cleanable': 4452, 'advertised': 4453, '“washable”': 4454, '“permanent”': 4455, '“reusable”': 4456, 'vacuuming': 4457, 'actually': 4458, 'clog': 4459, 'Disposable': 4460, 'compressed': 4461, 'At': 4462, 'long-term': 4463, '12000': 4464, '15000': 4465, 'miles': 4466, 'travel': 4467, 'roads': 4468, 'whereas': 4469, 'vacuumed': 4470, 'decrease': 4471, 'mileage': 4472, 'ignition': 4473, 'fouled': 4474, 'spark': 4475, 'plugs': 4476, 'routinely': 4477, 'in-season': 4478, 'furnace': 4479, 'monthly': 4480, 'More': 4481, 'frequent': 4482, 'Failing': 4483, 'HVAC': 4484, 'failure': 4485, 'Spot': 4486, 'grout': 4487, 'dive': 4488, 'proceed': 4489, 'pouring': 4490, 'furthest': 4491, 'thickly': 4492, 'dispersed': 4493, 'disturb': 4494, 'Often': 4495, 'appears': 4496, 'grime-free': 4497, 'tile': 4498, 'mop': 4499, 'lime': 4500, 'deposits': 4501, 'vapors': 4502, 'arms': 4503, 'acidity': 4504, 'oils': 4505, 'lingering': 4506, 'curry': 4507, 'usual': 4508, 'cuts': 4509, 'wounds': 4510, 'painful': 4511, 'stinging': 4512, 'articles': 4513, 'were': 4514, 'sweater': 4515, 'freshening': 4516, 'diffusers': 4517, 'deodorizing': 4518, 'plug-ins': 4519, 'scented': 4520, 'fresheners': 4521, 'vicinity': 4522, 'upholstery': 4523, 'packaging': 4524, 'emit': 4525, 'fragrant': 4526, 'Kewra': 4527, '(Screwpine)': 4528, 'vanilla': 4529, 'beansextract': 4530, 'simmer': 4531, 'emitted': 4532, 'odor': 4533, 'Screwpine': 4534, 'known': 4535, 'pandan': 4536, 'Asian': 4537, 'grocer': 4538, 'specifications': 4539, 'screwpine': 4540, 'permeate': 4541, 'residence': 4542, 'cooling': 4543, 'circulate': 4544, 'odorous': 4545, 'fade': 4546, 'water-moistened': 4547, 'garment': 4548, 'seam': 4549, 'discoloration': 4550, 'occurs': 4551, 'professionally': 4552, 'spot-treating': 4553, 'chlorine': 4554, 'lukewarm': 4555, 'inconspicuous': 4556, 'Mild': 4557, 'perspiration': 4558, 'deodorant': 4559, 'undiluted': 4560, 'diluted': 4561, 'tough': 4562, 'tougher': 4563, 'increasing': 4564, 'ratio': 4565, 'Diluted': 4566, 'ink': 4567, 'cosmetic': 4568, 'foundation': 4569, 'possums': 4570, 'sneak': 4571, 'vent': 4572, 'Block': 4573, 'sneaking': 4574, 'slotted': 4575, 'screwed': 4576, 'steam': 4577, 'Trim': 4578, 'Possums': 4579, 'roof': 4580, 'positioned': 4581, 'Tree': 4582, 'trimmed': 4583, 'ft': 4584, '(305': 4585, 'm)': 4586, 'climbing': 4587, 'eaves': 4588, 'decks': 4589, 'protected': 4590, '14-inch': 4591, '(635-mm)': 4592, 'grid': 4593, 'screening': 4594, 'flashing': 4595, 'trench': 4596, 'extend': 4597, 'underside': 4598, 'jolted': 4599, 'wiggled': 4600, 'Nails': 4601, 'one-way': 4602, 'exit': 4603, 'suspect': 4604, 'sealing': 4605, 'possum': 4606, 'roughly': 4607, '4': 4608, '(10': 4609, 'squeeze': 4610, 'passage': 4611, 'meal': 4612, 'Attic': 4613, 'chimneys': 4614, 'chimney': 4615, 'cap': 4616, 'Repair': 4617, 'lattices': 4618, 'extends': 4619, 'trailer': 4620, 'Cement': 4621, 'lattice': 4622, 'Lock': 4623, 'night': 4624, 'poultice': 4625, 'microbial': 4626, 'Lowes': 4627, '1)': 4628, '(baking': 4629, 'detergent)': 4630, '2)': 4631, '3)': 4632, 'bristles': 4633, 'acetone': 4634, 'xylene': 4635, '4)': 4636, '(buy': 4637, 'online)': 4638, 'anticipate': 4639, 'gear': 4640, 'degreasers': 4641, 'shield': 4642, 'resistant': 4643, 'superstore': 4644, 'Additionally': 4645, 'overalls': 4646, 'accidents': 4647, 'recorded': 4648, '7': 4649, 'week-': 4650, '(800)': 4651, '222-1222': 4652, 'Prior': 4653, 'pavement': 4654, 'pushing': 4655, 'deeper': 4656, 'recommended': 4657, 'caustic': 4658, 'formulated': 4659, 'aware': 4660, 'detergents': 4661, 'commercial': 4662, 'success': 4663, 'laundering': 4664, 'attack': 4665, 'letting': 4666, 'Shot': 4667, 'approaches': 4668, 'scrubber': 4669, 'denim': 4670, 'coarse': 4671, 'massage': 4672, 'dress': 4673, 'Tar': 4674, 'Scrubbing': 4675, 'tar': 4676, 'washcloth': 4677, 'cleanse': 4678, 'pumice': 4679, 'motion': 4680, 'pat': 4681, 'effectively': 4682, 'exfoliating': 4683, 'exfoliator': 4684, 'gone': 4685, 'Baking': 4686, 'Sugar': 4687, 'olive': 4688, 'coconut': 4689, 'Salt': 4690, 'almond': 4691, 'Finely': 4692, 'honey': 4693, 'doctor': 4694, 'removal': 4695, 'appointment': 4696, 'diagnose': 4697, 'administer': 4698, 'treatment': 4699, 'medical': 4700, 'discomfort': 4701, 'Figure': 4702, 'Choosing': 4703, 'sample': 4704, 'themes': 4705, 'boys': 4706, 'cowboys': 4707, 'sailors': 4708, 'dragons': 4709, 'pirates': 4710, 'mustaches': 4711, 'Themes': 4712, 'girls': 4713, 'princess': 4714, 'flowersand': 4715, 'Non-gendered': 4716, 'teddy': 4717, 'bears': 4718, 'duckies': 4719, 'owls': 4720, 'birds': 4721, 'peas': 4722, 'puzzles': 4723, 'Research': 4724, 'possibilities': 4725, 'abilities': 4726, 'centerpieces': 4727, '“cake”': 4728, 'diapers': 4729, 'balloons': 4730, 'Floating': 4731, 'Building': 4732, 'blocks': 4733, 'input': 4734, 'useful': 4735, 'parents-to-be': 4736, 'hipster': 4737, 'mustache': 4738, 'bird': 4739, 'cages': 4740, 'nesting': 4741, 'financial': 4742, 'diaper': 4743, 'cake': 4744, 'arrive': 4745, 'gave': 4746, 'chose': 4747, 'adorn': 4748, '\"cake\"': 4749, 'toy': 4750, 'stacking': 4751, 'bottles': 4752, 'washcloths': 4753, 'icing': 4754, 'layers': 4755, 'reaching': 4756, 'Obtain': 4757, 'vegetable': 4758, 'Contact': 4759, 'restaurants': 4760, 'cafeterias': 4761, 'hotels': 4762, 'establishments': 4763, 'compete': 4764, 'rendering': 4765, 'fried': 4766, 'French': 4767, 'fries': 4768, 'quantities': 4769, 'canola': 4770, 'biodiesel': 4771, 'hydrogenated': 4772, 'Fatty': 4773, 'Acids': 4774, 'production': 4775, 'expensive': 4776, 'sewer': 4777, 'obtain': 4778, 'frying': 4779, 'milky': 4780, 'cloudy': 4781, 'content': 4782, 'fats': 4783, 'interfere': 4784, 'procedures': 4785, 'obtained': 4786, 'transparent': 4787, 'jug': 4788, 'filtration': 4789, 'Label': 4790, 'clearly': 4791, '“used': 4792, 'oil”': 4793, '“unfiltered': 4794, 'confusion': 4795, 'barrel': 4796, 'downspouts': 4797, 'diverter': 4798, 'overflow': 4799, 'pooling': 4800, \"home's\": 4801, 'downspout': 4802, 'piping': 4803, 'saturating': 4804, '400': 4805, 'pounds': 4806, 'stability': 4807, 'ratchet': 4808, 'strap(s)': 4809, 'anchors': 4810, \"'X'\": 4811, 'Tightly': 4812, 'downward': 4813, 'Posts': 4814, 'backstop': 4815, 'property': 4816, '55': 4817, '(2082': 4818, 'Metal': 4819, 'sealed': 4820, 'rusting': 4821, 'rotting': 4822, 'decaying': 4823, 'grade': 4824, 'distribution': 4825, 'centers': 4826, 'processing': 4827, 'plastics': 4828, 'avoided': 4829, '#3': 4830, 'Polyvinyl': 4831, 'Chloride': 4832, '(PVC)': 4833, '#6': 4834, 'Polystyrene': 4835, '(PS)': 4836, '#7': 4837, 'Polycarbonate': 4838, 'Dump': 4839, 'soapy': 4840, 'sanitize': 4841, '(5': 4842, 'ml)': 4843, 'quart': 4844, '(94': 4845, 'liter)': 4846, 'teaspoons': 4847, '(20': 4848, 'expect': 4849, 'enter': 4850, 'valve': 4851, 'connect': 4852, 'drainage': 4853, \"house's\": 4854, \"building's\": 4855, 'lost': 4856, \"moment's\": 4857, 'fuel': 4858, 'handheld': 4859, 'pack': 4860, 'flimsy': 4861, 'odds': 4862, 'survival': 4863, 'stormy': 4864, 'wintry': 4865, 'Refer': 4866, 'linked': 4867, 'article': 4868, 'checklist': 4869, 'patch': 4870, 'rule': 4871, 'thumb': 4872, '091': 4873, 'metres': 4874, '0': 4875, 'in)': 4876, 'autumn': 4877, '030–091': 4878, '(1': 4879, 'in–3': 4880, '061': 4881, '(2': 4882, 'rainy': 4883, 'lean-to': 4884, 'boulder': 4885, 'tarp': 4886, 'available)': 4887, 'Pine': 4888, 'increases': 4889, 'resistance': 4890, 'bare': 4891, 'field': 4892, \"hours'\": 4893, 'packed': 4894, 'Pile': 4895, 'hillock': 4896, 'compact': 4897, 'shovel': 4898, 'feasible': 4899, 'emergency': 4900, 'situations': 4901, 'powdery': 4902, 'collapse)': 4903, 'shelters': 4904, 'Whichever': 4905, 'centimetersinches': 4906, 'stovetop': 4907, 'Cooking': 4908, 'leads': 4909, 'build-up': 4910, 'substances': 4911, 'leftover': 4912, 'disasters': 4913, 'ruin': 4914, 'Different': 4915, 'stovetops': 4916, 'deep-cleaning': 4917, 'loosen': 4918, 'coil': 4919, 'burners': 4920, 'coils': 4921, 'liners': 4922, 'burner': 4923, 'cooktops': 4924, 'grate': 4925, 'cooktop': 4926, 'counters': 4927, 'toasters': 4928, 'toaster': 4929, 'ovens': 4930, 'electric': 4931, 'griddles': 4932, 'fryers': 4933, 'develop': 4934, 'oil-based': 4935, 'concentrated': 4936, 'Regularly': 4937, 'Regular': 4938, 'inspections': 4939, 'servicing': 4940, 'Completing': 4941, 'lengthy': 4942, 'miss': 4943, 'occasionally': 4944, 'frayed': 4945, 'compromised': 4946, 'detectors': 4947, 'detector': 4948, 'harmless': 4949, 'incidents': 4950, 'leading': 4951, 're-attach': 4952, 'Unplug': 4953, 'turned': 4954, 'faulty': 4955, 'defective': 4956, 'plugged': 4957, 'Develop': 4958, 'habit': 4959, 'unplugging': 4960, 'trips': 4961, 'plug': 4962, 'outlet': 4963, 'extension': 4964, 'unattended': 4965, 'broiling': 4966, 'simmering': 4967, 'fires': 4968, 'too-high': 4969, 'toward': 4970, 'Handles': 4971, 'spilling': 4972, 'causing': 4973, 'burns': 4974, 'Turning': 4975, 'ensures': 4976, 'purchasing': 4977, 'protects': 4978, 'drag': 4979, 'close-fit': 4980, 'scarves': 4981, 'Long': 4982, 'forget': 4983, 'danger': 4984, 'blinds': 4985, 'Microwaving': 4986, 'silverware': 4987, 'sparks': 4988, 'mitt': 4989, 'smother': 4990, 'remember': 4991, 'technique': 4992, 'suffocate': 4993, 'Roads': 4994, 'paths': 4995, 'eroded': 4996, 'Stay': 4997, 'bridges': 4998, 'routes': 4999, 'muddy': 5000, 'vehicles': 5001, 'alternate': 5002, 'authorities': 5003, 'Buildings': 5004, 'hit': 5005, 'floodwaters': 5006, 'unseen': 5007, 'collapse': 5008, 'flooded': 5009, 'Assume': 5010, 'approach': 5011, 'floodwater': 5012, 'Stillwater': 5013, 'sewage': 5014, 'electrified': 5015, 'buildings': 5016, 'surrounded': 5017, 'suffered': 5018, 'structural': 5019, 'flashlight': 5020, 'hear': 5021, 'hissing': 5022, 'lanterns': 5023, 'snakes': 5024, 'Dangerous': 5025, 'taken': 5026, 'Overturn': 5027, 'pole': 5028, 'member': 5029, 'bitten': 5030, 'insurance': 5031, 'document': 5032, 'capturing': 5033, 'documentation': 5034, 'Doing': 5035, 'claims': 5036, 'disaster': 5037, 'applications': 5038, 'income': 5039, 'tax': 5040, 'deductions': 5041, 'unsafe': 5042, 'occupy': 5043, 'sump': 5044, 'pump': 5045, 'wet-dry': 5046, 'septic': 5047, 'wiring': 5048, 'Mud': 5049, 'heavy-duty': 5050, 'disinfecting': 5051, '10%': 5052, 'Fans': 5053, 'poorly': 5054, 'lit': 5055, 'alert': 5056, 'surroundings': 5057, 'mindful': 5058, 'watching': 5059, 'suspicious': 5060, 'drug': 5061, 'Drug': 5062, 'drugs': 5063, 'bought': 5064, 'isolated': 5065, 'police': 5066, 'Excessive': 5067, 'loitering': 5068, 'visitors': 5069, 'entering': 5070, 'Frequent': 5071, 'stops': 5072, 'cars': 5073, 'Threatening': 5074, 'obvious': 5075, 'intimidation': 5076, 'violence': 5077, 'sudden': 5078, 'criminal': 5079, 'terrorist': 5080, 'strange': 5081, 'crime': 5082, 'involving': 5083, 'terrorism': 5084, 'Unattended': 5085, 'briefcases': 5086, 'suitcases': 5087, 'backpacks': 5088, 'packages': 5089, 'Unusual': 5090, 'fumes': 5091, 'Strangers': 5092, 'asking': 5093, 'security': 5094, 'videos': 5095, 'witness': 5096, 'detail': 5097, 'law': 5098, 'enforcement': 5099, 'committed': 5100, \"crime's\": 5101, 'landmarks': 5102, 'urbanized': 5103, 'nearest': 5104, 'distinguishing': 5105, 'characteristic': 5106, 'height': 5107, 'build': 5108, 'markings': 5109, 'scars': 5110, 'tattoos)': 5111, 'criminals': 5112, 'armed': 5113, 'committing': 5114, 'victim': 5115, 'victimized': 5116, 'vandalism': 5117, 'imminent': 5118, '9-1-1': 5119, 'report': 5120, 'relating': 5121, 'federal': 5122, 'violent': 5123, 'child': 5124, 'pornography': 5125, 'human': 5126, 'trafficking': 5127, 'fraud': 5128, 'FBI': 5129, 'Department': 5130, 'Justice': 5131, 'borate': 5132, 'wherever': 5133, 'Timbor': 5134, 'Boracare': 5135, 'Typically': 5136, 'sprayer': 5137, 'Borate': 5138, 'threat': 5139, 'unlike': 5140, 'fumigants': 5141, 'varnish': 5142, 'stripper': 5143, 'reapplied': 5144, 'washes': 5145, 'dampness': 5146, 'penetrating': 5147, 'Thompson’s': 5148, 'Rainguard': 5149, 'kiln': 5150, 'chamber': 5151, 'freezer': 5152, 'heated': 5153, '150℉': 5154, '(66℃)': 5155, '-4℉': 5156, '(-20℃)': 5157, 'freezing': 5158, 'Extreme': 5159, 'warp': 5160, 'produce': 5161, 'diary': 5162, 'consumes': 5163, 'eaten': 5164, 'eat': 5165, 'inclusion': 5166, 'full-grown': 5167, 'dealer': 5168, 'physically': 5169, 'mature': 5170, 'capable': 5171, 'surviving': 5172, 'thorns': 5173, 'thornless': 5174, 'discount': 5175, 'hesitate': 5176, 'discounted': 5177, 'disappointing': 5178, 'established': 5179, 'seller': 5180, 'replacement': 5181, 'policy': 5182, 'dies': 5183, 'general': 5184, 'swing': 5185, 'Perhaps': 5186, 'fort': 5187, 'pool': 5188, 'splash': 5189, 'guarantee': 5190, 'spends': 5191, 'wildlife': 5192, 'pond': 5193, 'birdfeeder': 5194, 'low-key': 5195, 'owl': 5196, 'birdhouses': 5197, 'modifications': 5198, 'brings': 5199, 'birdwatching': 5200, 'adults': 5201, 'kid-safe': 5202, 'grille': 5203, 'guidance': 5204, 'barrow': 5205, 'readiness': 5206, 'cell': 5207, 'ringing': 5208, 'cordless': 5209, 'preferred)': 5210, 'Plan': 5211, 'concerning': 5212, '(what': 5213, 'neighbors': 5214, 'at)': 5215, 'jungle': 5216, 'tackle': 5217, 'limits': 5218, 'say': 5219, 'Map': 5220, 'breaks': 5221, 'snacks': 5222, 'MP3': 5223, 'radio': 5224, 'podcasts': 5225, 'skip': 5226, 'sounds': 5227, 'study': 5228, 'language': 5229, 'studying': 5230, 'something!': 5231, 'Bucket': 5232, 'compostable': 5233, 'namely': 5234, 'seed-free': 5235, 'dead-headed': 5236, 'mulch-machine': 5237, 'diseased': 5238, 'twigs': 5239, 'seedy': 5240, 'Wheel': 5241, 'required': 5242, '(it': 5243, 'compost': 5244, 'bin': 5245, 'waste)': 5246, 'refresh': 5247, 'inclined': 5248, 'planned': 5249, 'weekend': 5250, 'builds': 5251, 'sessions': 5252, 'enthusiasm': 5253, 'fork': 5254, 'grassy': 5255, 'sandals': 5256, 'spikes': 5257, 'bottoms': 5258, 'Aeration': 5259, 'lawn': 5260, 'eight': 5261, 'centimeters': 5262, 'achieve': 5263, 'aeration': 5264, 'compaction': 5265, 'Loosen': 5266, 'digging': 5267, 'spade': 5268, 'trenches': 5269, 'poor': 5270, 'spade-lengths': 5271, 'aerate': 5272, 'rototiller': 5273, 'Run': 5274, 'tiller': 5275, 'Tillers': 5276, 'coring': 5277, 'machines': 5278, 'Tilling': 5279, 'contributes': 5280, 'tilled': 5281, 'cores': 5282, 'Plug': 5283, 'aerators': 5284, 'lawns': 5285, 'fields': 5286, 'Rent': 5287, 'rolls': 5288, 'core': 5289, 'scattering': 5290, 'Badly': 5291, 'compacted': 5292, 'aerating': 5293, 'Mark': 5294, 'held': 5295, 'manually': 5296, 'intensive': 5297, 'mostly': 5298, 'reintroduce': 5299, 'grass': 5300, 'planting': 5301, 'mound': 5302, 'bury': 5303, 'topsoil': 5304, 'nurture': 5305, 'flourish': 5306, 'Trees': 5307, 'shrubs': 5308, 'purpose': 5309, 'journal': 5310, 'fulfill': 5311, 'repository': 5312, 'gardener': 5313, 'merely': 5314, 'suffice': 5315, 'hoping': 5316, 'format': 5317, 'suits': 5318, 'graph': 5319, 'binder': 5320, 'pouches': 5321, 'Electronic': 5322, 'formats': 5323, 'diary-like': 5324, 'file': 5325, 'entry': 5326, 'measurements': 5327, 'statistics': 5328, 'data': 5329, 'Microsoft': 5330, 'Excel': 5331, 'smartphones': 5332, 'Garden': 5333, 'Pro': 5334, 'Master': 5335, 'Gardener': 5336, 'chronological': 5337, 'topical': 5338, 'folder': 5339, 'non-text': 5340, 'samples': 5341, 'diagrams': 5342, 'lamination': 5343, 'Begin': 5344, \"day's\": 5345, 'edit': 5346, 'evolve': 5347, 'goes': 5348, 'second': 5349, 'volume': 5350, 'inserting': 5351, 'exposure': 5352, 'rash': 5353, 'blisters': 5354, '48': 5355, 'sumac': 5356, 'toxin': 5357, 'urushiol': 5358, 'dissolve': 5359, 'significant': 5360, 'Warning': 5361, 'susceptible': 5362, 'surfactant': 5363, 'bind': 5364, 'dermis': 5365, 'Fels': 5366, 'Naptha': 5367, '(old': 5368, 'fashioned': 5369, 'ole': 5370, 'Spic': 5371, 'n': 5372, 'Span': 5373, 'readily': 5374, 'pores': 5375, 'worse': 5376, 'specialized': 5377, 'Tecnu': 5378, 'picked': 5379, 'antihistamines': 5380, 'lotions': 5381, 'oral': 5382, 'itching': 5383, 'calamine': 5384, 'hydrocortisone': 5385, 'baths': 5386, 'oozing': 5387, 'prescription-strength': 5388, 'Ooze': 5389, 'inhaled': 5390, 'seek': 5391, 'symptoms': 5392, 'developed': 5393, \"doctor's\": 5394, 'genitals': 5395, 'fails': 5396, 'swollen': 5397, 'trouble': 5398, 'initial': 5399, 'transport': 5400, 'Shut': 5401, 'sprinkler': 5402, 'hooked': 5403, 'nozzle': 5404, 'dribbles': 5405, 'Twist': 5406, 'grip': 5407, 'counterclockwise': 5408, 'unscrew': 5409, 'sits': 5410, 'rough': 5411, 'Rain': 5412, 'Bird': 5413, 'models': 5414, 'clockwise': 5415, 'crops': 5416, 'planted': 5417, 'tasks': 5418, 'straighter': 5419, 'sharper': 5420, 'sod': 5421, 'Smooth': 5422, 'mulch': 5423, 'rake': 5424, 'tines': 5425, 'lumps': 5426, 'distribute': 5427, 'stones': 5428, 'pebbles': 5429, 'flexible': 5430, 'compile': 5431, 'Aerate': 5432, 'enlarged': 5433, 'version': 5434, 'hoe': 5435, 'weeder': 5436, 'resembles': 5437, 'breeds': 5438, 'disrupting': 5439, 'high-powered': 5440, 'mower': 5441, 'large-scale': 5442, 'expanse': 5443, 'watered': 5444, 'wand': 5445, 'wheelbarrow': 5446, 'harvesting': 5447, 'clover': 5448, 'consumption': 5449, 'Leaves': 5450, 'clumps': 5451, 'flower)': 5452, 'regrow': 5453, 'ability': 5454, 'discolored': 5455, 'Plants': 5456, 'sickly': 5457, 'Harvesting': 5458, 'sick': 5459, 'recover': 5460, 'harvest)': 5461, 'impact': 5462, 'flavor': 5463, 'uproot': 5464, 'racks': 5465, 'shaded': 5466, 'crisp': 5467, 'humid': 5468, 'dehydrator': 5469, 'appropriately': 5470, 'Tupperware': 5471, '70': 5472, 'Fahrenheit': 5473, '(21': 5474, 'Celsius)': 5475, 'basil': 5476, 'ziplock': 5477, 'Blanch': 5478, 'spoon': 5479, 'freezer-safe': 5480, 'Basil': 5481, 'cupboard': 5482, 'canning': 5483, 'crumble': 5484, 'bail': 5485, 'Dried': 5486, 'distinct': 5487, 'bunches': 5488, 'inch': 5489, '(25': 5490, '(51': 5491, 'sunlight': 5492, '“basil': 5493, 'cubes”': 5494, 'processor': 5495, '(250': 5496, 'grape': 5497, 'Process': 5498, 'tightly': 5499, 'cubes': 5500, 'sauces': 5501, 'soups': 5502, 'curries': 5503, 'seasoning': 5504, 'approximately': 5505, 'During': 5506, 'summer': 5507, 'aloe': 5508, 'fastest': 5509, 'overwater': 5510, 'depth': 5511, '(75': 5512, 'infrequently': 5513, 'Aloe': 5514, 'dormant': 5515, 'prolonged': 5516, 'twice': 5517, 'Fertilize': 5518, 'overuse': 5519, 'unhealthy': 5520, 'manner': 5521, 'nitrogen': 5522, 'phosphorous': 5523, 'potassium': 5524, '104010': 5525, '153015': 5526, 'sandy': 5527, 'vigorous': 5528, 'weed-pulling': 5529, 'Increase': 5530, 'vera': 5531, 'outward': 5532, 'angle': 5533, 'receiving': 5534, 'sunnier': 5535, 'daylight': 5536, 'Decrease': 5537, 'hardier': 5538, 'turns': 5539, 'receives': 5540, 'thincurled': 5541, 'curled': 5542, 'fleshy': 5543, 'drought': 5544, 'curling': 5545, 'overcompensate': 5546, 'Yellowed': 5547, '\"melting\"': 5548, 'suffering': 5549, 'altogether': 5550, 'season)': 5551, 'although': 5552, '(6-9': 5553, 'Flower': 5554, 'laid': 5555, '“island”': 5556, 'clearing': 5557, 'aerated': 5558, '(25-5': 5559, 'rotary': 5560, 'tallest': 5561, 'shortest': 5562, 'to)': 5563, 'natural-looking': 5564, 'groups': 5565, 'spontaneous': 5566, 'rigid': 5567, 'formations': 5568, 'clusters': 5569, '(small': 5570, 'groups)': 5571, 'drifts': 5572, '(long': 5573, 'rows)': 5574, 'intermingling': 5575, 'trellis': 5576, 'wrought-iron': 5577, 'morning': 5578, 'glories': 5579, 'elaborate': 5580, 'trellises': 5581, 'repurposed': 5582, 'wagon': 5583, 'wheels': 5584, 'ground-planted': 5585, 'among': 5586, 'trailing': 5587, 'vines': 5588, 'keg': 5589, 'multipurpose': 5590, 'entertaining': 5591, 'multi': 5592, 'extended': 5593, 'Consolidate': 5594, 'Lightweight': 5595, 'rely': 5596, 'solely': 5597, 'cushioned': 5598, 'raise': 5599, 'needing': 5600, 'brighter': 5601, 'cheerier': 5602, 'airy': 5603, 'inadequate': 5604, 'full-bodied': 5605, 'Glass': 5606, 'topped': 5607, 'Furniture': 5608, 'bodies': 5609, 'Soft': 5610, 'airier': 5611, 'Cushions': 5612, 'cheaply': 5613, 'adjusting': 5614, 'tinsel': 5615, 'expand': 5616, 'beyond': 5617, '(often': 5618, 'boughs': 5619, 'pine-needles)': 5620, 'doorframe': 5621, 'entryway': 5622, 'porch': 5623, 'push-pins': 5624, 'woodwork': 5625, 'paraphernalia': 5626, 'nativity': 5627, 'nutcracker': 5628, 'woodsy': 5629, 'birch': 5630, 'aspen': 5631, 'theme—which': 5632, 'evergreens': 5633, 'look—make': 5634, 'consistent': 5635, 'unlikely': 5636, 'snowfall': 5637, 'Holiday': 5638, 'aerosol': 5639, 'inhale': 5640, 'well-ventilated': 5641, 'Calculate': 5642, 'estimate': 5643, 'purchases': 5644, 'wallcoverings': 5645, 'combining': 5646, 'draws': 5647, 'drawn': 5648, 'particular': 5649, 'trust': 5650, 'gut': 5651, 'Going': 5652, 'gathered': 5653, 'similarities': 5654, 'prints': 5655, 'repeatedly': 5656, 'calmer': 5657, 'wallcovering': 5658, 'resist': 5659, 'high-traffic': 5660, 'switches': 5661, 'fingerprints': 5662, 'scuff': 5663, 'child-friendly': 5664, 'child’s': 5665, 'boring': 5666, 'eliminates': 5667, 'hate': 5668, 'tricks': 5669, 'Lighter': 5670, 'stylish': 5671, '“distressed”': 5672, 'index': 5673, 'petroleum': 5674, 'Upcycle': 5675, 'salvaged': 5676, 'curved': 5677, 'quadrant': 5678, 'lithographs': 5679, 'Tack': 5680, 'mounting': 5681, 'putty': 5682, 'normally': 5683, \"haven't\": 5684, 'jobfinish': 5685, 'endless': 5686, 'Prop': 5687, 'Fasten': 5688, 'mail': 5689, 'cookware': 5690, 'slice': 5691, 'chop': 5692, 'puree': 5693, 'knead': 5694, 'bread': 5695, 'cooks': 5696, 'processors': 5697, 'grind': 5698, 'herbs': 5699, 'nuts': 5700, 'amounts': 5701, 'blender': 5702, 'rated': 5703, '500': 5704, 'watts': 5705, 'crush': 5706, 'smoothies': 5707, 'soup': 5708, 'ingredients': 5709, 'immerse': 5710, 'slots': 5711, 'toast': 5712, 'slices': 5713, 'bagels': 5714, 'pizza': 5715, 'waffles': 5716, 'conventional': 5717, 'mixer': 5718, 'bake': 5719, 'baked': 5720, 'goods': 5721, 'doughs': 5722, 'beating': 5723, 'grinder': 5724, 'commercially-ground': 5725, 'spices': 5726, 'coffeemakers': 5727, 'automatic': 5728, 'manual': 5729, 'cappuccino': 5730, 'latte': 5731, 'drinks': 5732, 'espresso': 5733, 'maker': 5734, 'slow': 5735, 'cooker': 5736, 'pressed': 5737, 'tenderize': 5738, '8': 5739, 'costs': 5740, 'Pressure': 5741, 'cookers': 5742, 'chickens': 5743, 'roasted': 5744, 'waffle': 5745, 'sandwiches': 5746, 'Season': 5747, 'instruction': 5748, 'appliance': 5749, 'rice': 5750, 'perfection': 5751, 'citrus': 5752, 'juicer': 5753, 'juices': 5754, 'division': 5755, 'divides': 5756, 'open-backed': 5757, 'Curtains': 5758, 'divider': 5759, 'benefits': 5760, 'lengths': 5761, 'extra-light': 5762, 'cable': 5763, 'experienced': 5764, 'sliding': 5765, 'installations': 5766, 'semi-opaque': 5767, 'Nevertheless': 5768, 'Sliding': 5769, 'Hiring': 5770, 'alterations': 5771, 'vision': 5772, 'barriers': 5773, 'nonexistent': 5774, 'host': 5775, 'dislike': 5776, 'Similarly': 5777, 'bar': 5778, 'paneled': 5779, 'opaque': 5780, 'Dividers': 5781, 'moved': 5782, 'mood': 5783, 'convey': 5784, 'Colors': 5785, 'rev': 5786, 'soothed': 5787, 'peppy': 5788, 'guide': 5789, 'Save': 5790, 'visualize': 5791, 'Color': 5792, 'tone': 5793, 'factors': 5794, 'range': 5795, 'intensity': 5796, 'families': 5797, 'Warm': 5798, 'tones': 5799, 'dramatic': 5800, 'dynamic': 5801, 'conveying': 5802, 'Reds': 5803, 'Red': 5804, 'terms': 5805, 'lively': 5806, 'exciting': 5807, 'Blue': 5808, 'conveys': 5809, 'Like': 5810, 'perfect': 5811, 'sky': 5812, 'ocean': 5813, 'soothing': 5814, '(yellow)': 5815, '(blue)': 5816, 'excellent': 5817, 'Yellow': 5818, 'comforting': 5819, 'hue': 5820, 'brightening': 5821, 'tranquil': 5822, 'Neutrals': 5823, 'beiges': 5824, 'quiet': 5825, 'introduce': 5826, 'suggestion': 5827, 'warmth': 5828, 'coolness': 5829, 'tints': 5830, 'Stick': 5831, 'evocative': 5832, 'desert': 5833, 'golds': 5834, 'greens': 5835, 'tapestries': 5836, 'richness': 5837, 'freehand': 5838, 'reminiscent': 5839, 'East': 5840, 'closer': 5841, 'drama': 5842, 'architectural': 5843, 'built-in': 5844, 'Moroccan-inspired': 5845, 'Tape': 5846, 'Affix': 5847, 'pre-made': 5848, 'eight-pointed': 5849, 'star': 5850, 'octagon': 5851, 'repeated': 5852, 'geometrical': 5853, 'taping': 5854, 'painter’s': 5855, 'skirt': 5856, 'box-spring': 5857, 'ruffled': 5858, 'protector': 5859, 'fitted': 5860, 'solid-colored': 5861, 'patterned': 5862, 'blend': 5863, 'climate': 5864, 'comforter': 5865, 'duvet': 5866, 'Duvet': 5867, 'band': 5868, 'leaning': 5869, 'velvet': 5870, 'brocade': 5871, 'unified': 5872, 'pillow': 5873, 'experimenting': 5874, 'skill': 5875, 'Stones': 5876, 'beginners': 5877, 'Softer': 5878, 'sedimentary': 5879, 'sandstone': 5880, 'limestone': 5881, 'soapstone)': 5882, 'engraver': 5883, 'chisel': 5884, 'mallet': 5885, 'carbide': 5886, 'soapstone': 5887, 'diamond': 5888, 'harder': 5889, 'Engraving': 5890, 'widths': 5891, 'Over': 5892, 'cone': 5893, 'cylindrical': 5894, 'shading': 5895, 'Electric': 5896, 'engravers': 5897, 'wax-based': 5898, 'marker': 5899, 'Sketching': 5900, 'missteps': 5901, 'Wax-based': 5902, 'acetate': 5903, 'Beeswax': 5904, 'Safety': 5905, 'submerge': 5906, 'lantern': 5907, 'candleholder': 5908, 'Mod': 5909, 'Podge': 5910, 'flake': 5911, \"Here's\": 5912, 'squat': 5913, 'tissue': 5914, 'proportionate': 5915, '(254': 5916, 'centimeters)': 5917, '(762': 5918, 'paintbrushfoam': 5919, 'overlap': 5920, 'okay': 5921, 'past': 5922, 'scissors': 5923, 'edgeneck': 5924, 'symbols': 5925, 'Chinese': 5926, 'characters': 5927, 'yin-yang': 5928, 'forth': 5929, 'sequins': 5930, 'Blow': 5931, 'flakes': 5932, 'Coat': 5933, 'Concrete': 5934, 'comparison': 5935, 'Dry-brushing': 5936, 'coats': 5937, 'swiping': 5938, 'partially-coated': 5939, 'striation': 5940, 'barely-damp': 5941, 'feathery': 5942, 'antiquing': 5943, 'Airbrushing': 5944, 'airbrush': 5945, 'learned': 5946, 'realistic': 5947, 'effects': 5948, 'Detailing': 5949, 'detailed': 5950, 'non-yellowing': 5951, 'sealer': 5952, '(gravel': 5953, 'do)': 5954, 'determining': 5955, 'cubic': 5956, 'Multiply': 5957, 'width': 5958, 'calculator': 5959, 'mortar': 5960, 'tub': 5961, 'independent': 5962, 'shifting': 5963, 'settling': 5964, 'slabs': 5965, 'ramp': 5966, 'Ramps': 5967, '2x4s': 5968, 'Getting': 5969, 'Push': 5970, 'dump': 5971, 'boots': 5972, 'screed': 5973, '(a': 5974, '2x4)': 5975, 'Work': 5976, 'sawing': 5977, 'bull': 5978, 'float': 5979, 'disappears': 5980, 'touches': 5981, 'edger': 5982, 'rounded': 5983, 'grooves': 5984, 'joints': 5985, 'shifts': 5986, 'changes': 5987, 'magnesium': 5988, 'smoothing': 5989, 'cure': 5990, 'curing': 5991, 'compound': 5992, 'stays': 5993, 'cures': 5994, 'chip': 5995, 'newly': 5996, 'pane': 5997, 'Measuring': 5998, 'Subtract': 5999, '1⁄8': 6000, '(032': 6001, 'measurement': 6002, 'crack': 6003, 'expands': 6004, 'Wood': 6005, 'glazing': 6006, 'weather-proofing': 6007, 'came': 6008, 'Reinsert': 6009, 'clips': 6010, 'earlier': 6011, 'tapping': 6012, 'runs': 6013, 'decoupage': 6014, '(some': 6015, 'suggested': 6016, 'next)': 6017, 'Theme': 6018, 'Victoriana': 6019, 'tropical': 6020, 'cats': 6021, 'dogs': 6022, 'moons': 6023, 'cupcakes': 6024, 'doughnuts': 6025, 'woodland': 6026, 'critters': 6027, 'safari': 6028, 'Instagram': 6029, 'Pinterest': 6030, 'sites': 6031, 'relevant': 6032, 'piles': 6033, 'pictureshape': 6034, 'Suitable': 6035, 'specifically': 6036, 'magazine': 6037, 'groceries': 6038, 'printed': 6039, 'quilting': 6040, '(fat': 6041, 'quarters': 6042, 'useful)': 6043, 'CD': 6044, 'decision': 6045, 'affixed': 6046, 'sketch': 6047, 'placement': 6048, 'separated': 6049, 'detect': 6050, '(the': 6051, 'nicely': 6052, 'gaps)': 6053, 'skimping': 6054, 'confine': 6055, 'poster': 6056, 'tack': 6057, 'panel': 6058, 'return-air': 6059, 'blower': 6060, 'tracks': 6061, 'debris)': 6062, 'unsure': 6063, 'usable': 6064, 'furnaces': 6065, 'applies': 6066, 'number)': 6067, 'reapply': 6068, 'furnace’s': 6069, 'double-check': 6070, 'correct': 6071, 'Furnace': 6072, 'reminders': 6073, 'conserve': 6074, 'meeting': 6075, 'employees': 6076, 'kg': 6077, 'carbon': 6078, 'dioxide': 6079, 'emissions': 6080, 'barely': 6081, 'energy-saving': 6082, '(CFL)': 6083, 'escaping': 6084, 'conditioner': 6085, 'regions': 6086, 'shuts': 6087, \"office's\": 6088, 'venting': 6089, 'conditioning': 6090, '(HVAC)': 6091, 'routine': 6092, 'repairman': 6093, 'bills': 6094, 'office’s': 6095, 'files': 6096, 'Blocked': 6097, 'Modify': 6098, 'Conserve': 6099, '68': 6100, '78': 6101, 'Close': 6102, 'via': 6103, 'overheating': 6104, 'weekends': 6105, 'saved': 6106, 'lowering': 6107, 'already-painted': 6108, 'unpainted': 6109, 'varnished': 6110, 'Step': 6111, 'according': 6112, \"manufacturer's\": 6113, 'strippers': 6114, 'turpentine': 6115, 'Sandpaper': 6116, 'uneven': 6117, 'knobs': 6118, 'Handheld': 6119, 'sanders': 6120, 'medium-grit': 6121, '150-grit': 6122, '180': 6123, 'loosened': 6124, 'fine-grit': 6125, 'Sanding': 6126, 'gloss': 6127, 'gives': 6128, '220-grit': 6129, 'grain': 6130, 'open-grained': 6131, 'woods': 6132, 'oak': 6133, 'filler': 6134, \"wood's\": 6135, 'pronounced': 6136, 'manufacturer-recommended': 6137, 'Engineered': 6138, 'engineered': 6139, 'wrong': 6140, 'brand': 6141, 'email': 6142, 'request': 6143, 'liquid-cleaner': 6144, '“Cleaning”': 6145, '“Flooring”': 6146, 'Lowe’s': 6147, 'hard-to-reach': 6148, 'themselves': 6149, 'Liquids': 6150, 'Cleaning': 6151, 'substituting': 6152, 'pressing': 6153, 'crumbly': 6154, 'Popsicle': 6155, 'printing': 6156, 'ruining': 6157, 'tailored': 6158, 'carpeted': 6159, 'store-bought': 6160, 'bad': 6161, 'ingested': 6162, 'carpet-cleaning': 6163, 'continue': 6164, 'Frequently': 6165, 'freshly-cleaned': 6166, 'remnants': 6167, '(49': 6168, 'proving': 6169, 'Quickly': 6170, 'straighten': 6171, 'rising': 6172, 'film': 6173, 'Swirl': 6174, 'agitate': 6175, 'mat': 6176, 'Submerge': 6177, 'You’ll': 6178, 'clanking': 6179, 'chipping': 6180, 'breakage': 6181, 'chandelier': 6182, 'dampen': 6183, 'scratches': 6184, 'etched': 6185, 'soft-bristled': 6186, 'etchings': 6187, 'water-spots': 6188, 'lint-free': 6189, 'smudging': 6190, 'Lemon': 6191, 'bottled': 6192, 'rolling': 6193, 'Squeeze': 6194, 'fair': 6195, 'Essential': 6196, 'increasingly': 6197, 'rumored': 6198, 'Citrus-based': 6199, 'tends': 6200, 'buildups': 6201, 'spreads': 6202, 're-form': 6203, 'bubbling': 6204, 'subside': 6205, 'still-bubbling': 6206, 'resulting': 6207, 'expansion': 6208, 'forcibly': 6209, 'manufacturer’s': 6210, 'everything': 6211, 'post-installation': 6212, 'immediate': 6213, 'approved': 6214, 'warranty': 6215, 'voided': 6216, 'pH': 6217, 'pre-moistened': 6218, 'sustaining': 6219, '“pH-neutral': 6220, 'wipes”': 6221, '“stone': 6222, 'countertop': 6223, '(30': 6224, 'mL)': 6225, 'unsealed': 6226, 'sealant': 6227, 'pads': 6228, 'movements': 6229, 'rewet': 6230, 'set-in': 6231, 'sediment': 6232, 'wash-down': 6233, 'wax': 6234, 'bathmat': 6235, 'poured': 6236, 'Fumes': 6237, 'noxious': 6238, 'dizziness': 6239, 'intensify': 6240, 'indiscriminately': 6241, 'Unequal': 6242, 'potent': 6243, 'follows': 6244, '(49ml)': 6245, '(38L)': 6246, '(240ml)': 6247, '(189L)': 6248, 'bathmats': 6249, '(3-4)': 6250, 'freshen': 6251, 'scouring': 6252, 'Drain': 6253, 'scum': 6254, 'user': 6255, 'agents': 6256, 'manufacturers': 6257, 'proscribe': 6258, 'dilemmas': 6259, 'stickiness': 6260, 'discoloration)': 6261, 'Combine': 6262, 'powdered': 6263, 'sudsy': 6264, 'scrubbie': 6265, 'thereby': 6266, 'preventing': 6267, 'Scoop': 6268, 'smear': 6269, 'dishcloth': 6270, 'cooktop’s': 6271, 'generous': 6272, 'disappear': 6273, 'Popular': 6274, 'CeramaBryte': 6275, 'Sprayway': 6276, 'vital': 6277, 'documents': 6278, 'includes': 6279, 'proves': 6280, 'existence': 6281, 'citizenship': 6282, 'eligibility': 6283, 'vote': 6284, 'indefinitely': 6285, 'exception': 6286, 'expired': 6287, 'voter': 6288, 'registration': 6289, 'precinct': 6290, 'changed': 6291, 'Birth': 6292, 'certificates': 6293, 'Immigration': 6294, 'Social': 6295, 'Security': 6296, 'National': 6297, 'ID': 6298, 'applicable': 6299, 'Military': 6300, 'discharge': 6301, 'papers': 6302, 'pertaining': 6303, 'assets': 6304, 'Asset': 6305, 'prove': 6306, 'owning': 6307, 'outright': 6308, 'asset': 6309, 'Property': 6310, 'deeds': 6311, '(domestic': 6312, 'commercial)': 6313, 'Vehicle': 6314, 'titles': 6315, 'Stock': 6316, 'Unconverted': 6317, 'savings': 6318, 'bonds': 6319, 'liabilities': 6320, 'Liabilities': 6321, 'debts': 6322, 'paying': 6323, 'application': 6324, 'contract': 6325, 'loan': 6326, 'paid': 6327, 'Student': 6328, 'loans': 6329, 'lease': 6330, 'contracts': 6331, 'Mortgages': 6332, 'leases': 6333, 'Payment': 6334, 'plans': 6335, 'services': 6336, 'Installment': 6337, 'marking': 6338, 'mortgages': 6339, 'vehicle': 6340, 'IRS': 6341, 'recommends': 6342, 'accuracy': 6343, 'audited': 6344, 'IRSgov': 6345, 'paperwork': 6346, 'proceedings': 6347, 'lawsuits': 6348, 'seven': 6349, 'estate': 6350, 'marital': 6351, 'status': 6352, 'licensed': 6353, 'proof': 6354, 'ownership': 6355, 'Wills': 6356, 'wills': 6357, 'Marriage': 6358, 'licenses': 6359, 'Divorce': 6360, 'decrees': 6361, 'Adoption': 6362, 'Copyrights': 6363, 'patents': 6364, 'business': 6365, 'three-year': 6366, 'domestic': 6367, 'taxes': 6368, 'account': 6369, 'pertain': 6370, 'Contracts': 6371, 'Client': 6372, 'Payroll': 6373, 'Investment': 6374, 'Bad': 6375, 'investment': 6376, 'write-offs': 6377, 'total': 6378, 'inner': 6379, 'spaced': 6380, 'Rest': 6381, '1”x4”': 6382, '(19x89mm)': 6383, 'laterally': 6384, 'precut': 6385, 'fasten': 6386, 'slat': 6387, \"they'll\": 6388, 'individually': 6389, 'stapling': 6390, 'fourth': 6391, 'orientation': 6392, 'two-paneled': 6393, 'constructing': 6394, 'enclosure': 6395, '“U”': 6396, 'clamp': 6397, 'increased': 6398, 'hinges': 6399, 'receptacle': 6400, 'invisible': 6401, 'onlookers': 6402, 'Jointed': 6403, 'please': 6404, 'beautifying': 6405, 'Offset': 6406, '6-10': 6407, '(approximately': 6408, '15-25': 6409, 'jointed': 6410, 'Larger': 6411, 'swinging': 6412, 'gate': 6413, 'Ornament': 6414, 'pulls': 6415, 'Position': 6416, 'conceal': 6417, 'newfound': 6418, 'curb': 6419, 'offers': 6420, 'ease': 6421, 'seeing': 6422, 'piled': 6423, 'rubbish': 6424, 'Restyle': 6425, 'Dyeing': 6426, 'altering': 6427, 'gown': 6428, 'satin': 6429, 'organza': 6430, 'dyed': 6431, 'polyester': 6432, 'wedding': 6433, 'attempting': 6434, 'dozens': 6435, 'diverse': 6436, 'gowns': 6437, 'invaluable': 6438, 'resource': 6439, 'keepsakes': 6440, 'Wedding': 6441, 'album': 6442, 'lace': 6443, 'overlay': 6444, 'Trinkets': 6445, 'pendant': 6446, 'necklace': 6447, 'locket': 6448, 'christening': 6449, 'traditions': 6450, 'adventurous': 6451, 'wrecking': 6452, 'Salvage': 6453, 'sew': 6454, 'flowing': 6455, 'patchwork': 6456, 'Decorations': 6457, 'banners': 6458, 'strung': 6459, 'skirts': 6460, 'Clothing': 6461, 'headbands': 6462, 'handkerchiefs': 6463, 'Fashionable': 6464, 'portions': 6465, 'Stuffed': 6466, 'bassinet': 6467, 'crib': 6468, 'trace': 6469, 'linens': 6470, 'cabinets': 6471, 'overhaul': 6472, 'all-purpose': 6473, 'donate': 6474, 'thrift': 6475, 'moldy': 6476, 'stagnant': 6477, 'cycle': 6478, 'Soap': 6479, 'scorch': 6480, 'ironed': 6481, 'hydrogen-peroxide': 6482, 'clothesline': 6483, 'Iron': 6484, 'hurting': 6485, 'Starch': 6486, 'formal': 6487, 'Organize': 6488, 'oxygen': 6489, 'garments': 6490, 'sweaters': 6491, 'leggings': 6492, 'Merino': 6493, 'socks': 6494, 'mittens': 6495, 'bleeding': 6496, 'darks': 6497, 'brights': 6498, 'weights': 6499, 'minimizing': 6500, 'pilling': 6501, 'inside-out': 6502, 'wool-specific': 6503, 'necessitates': 6504, 'minimize': 6505, 'fiber-damage': 6506, 'shampoo': 6507, 'softeners': 6508, 'select': 6509, 'rotation': 6510, '(about': 6511, 'C': 6512, '85': 6513, 'F)': 6514, 'garment’s': 6515, 'guidelines': 6516, 'shrinkage': 6517, 'felting': 6518, 'constant': 6519, 'seriously': 6520, 'shrink': 6521, 'Leaving': 6522, 'pile': 6523, 'stretch': 6524, 'misshape': 6525, 'mold-killing': 6526, 'killing': 6527, 'discreet': 6528, 'discolor': 6529, 'adjacent': 6530, 'infected': 6531, 'disposables': 6532, 'duty': 6533, 'vigorously': 6534, 'bristle': 6535, 'footwear': 6536, 'rating': 6537, '3000': 6538, 'psi': 6539, 'gpm': 6540, '(gallons': 6541, 'minute)': 6542, 'van': 6543, 'pickup': 6544, 'truck': 6545, 'SUV': 6546, 'load': 6547, 'unload': 6548, 'rental': 6549, 'nozzles': 6550, 'fifteen': 6551, 'zero': 6552, 'degree': 6553, 'yet': 6554, 'Leather': 6555, 'inorganic': 6556, 'Upholstered': 6557, 'reupholstered': 6558, 'Carpet': 6559, 'shows': 6560, 'pantry': 6561, 'infested': 6562, 'grains': 6563, 'moth': 6564, 'naked': 6565, 'larvae': 6566, 'moths': 6567, 'angular': 6568, 'baseboard': 6569, 'shelving': 6570, 'Focus': 6571, 'webbing': 6572, 'jamb': 6573, 'cabinetry': 6574, 'effectiveness': 6575, 'ventilation': 6576, 'closely': 6577, 'evidence': 6578, 'infestation': 6579, 'returning': 6580, 'Pantry': 6581, 'thrive': 6582, 'containing': 6583, 'soiled': 6584, 'tricky': 6585, 'Pet': 6586, 'blacklight': 6587, 'scent-creating': 6588, 'gloved': 6589, 'urinate': 6590, 'positive': 6591, 'transferring': 6592, 'enzymatic': 6593, 'newspapers': 6594, 'Urine': 6595, 'enzymes': 6596, 'pees': 6597, 'urea': 6598, 'produces': 6599, 'decompose': 6600, 'Fortunately': 6601, 'uric': 6602, 'component': 6603, 'soluble': 6604, 'owners': 6605, 'urinating': 6606, 'remind': 6607, 'Aluminum': 6608, 'crinkly': 6609, 'scare': 6610, 'turf': 6611, 'Fall': 6612, 'seasons': 6613, 'Winter': 6614, 'rainfall': 6615, 'die': 6616, 'amendments': 6617, 'turfgrass': 6618, 'species': 6619, '72': 6620, '(low': 6621, 'pH)': 6622, '(high': 6623, 'sulfur': 6624, 'pine': 6625, 'soil’s': 6626, 'nutrient': 6627, 'levels': 6628, 'Level': 6629, 'mounds': 6630, 'Topsoil': 6631, 'efficiently': 6632, 'water-weighted': 6633, 'roller': 6634, 'pushed': 6635, 'towed': 6636, 'tractor': 6637, 'Adding': 6638, 'granular': 6639, 'non-liquid': 6640, '“starter”': 6641, 'NPK': 6642, '5-10-5': 6643, 'aesthetic': 6644, 'flexibility': 6645, 'property’s': 6646, 'Enhance': 6647, 'Fencing': 6648, 'Being': 6649, 'repaint': 6650, \"borders'\": 6651, 'limiting': 6652, 'Cornus': 6653, 'Mas': 6654, 'Eastern': 6655, 'Redbud': 6656, 'Japanese': 6657, 'Dwarf': 6658, 'Korean': 6659, 'Lilac': 6660, 'Kousa': 6661, 'Dogwood': 6662, 'Serviceberry': 6663, 'crowded': 6664, 'suit': 6665, 'extending': 6666, 'shrub': 6667, 'pizazz': 6668, 'topiary': 6669, 'trimming': 6670, 'eye-catching': 6671, 'region': 6672, 'burkwood': 6673, 'viburnum': 6674, 'India': 6675, 'hawthorn': 6676, '“Pallida”': 6677, 'witch': 6678, 'hazel': 6679, 'Define': 6680, 'hindering': 6681, 'accessibility': 6682, 'Vegetables': 6683, 'exhaust': 6684, 'intend': 6685, 'Opting': 6686, 'creeper': 6687, 'Corsican': 6688, 'mint)': 6689, 'track': 6690, 'curves': 6691, 'curve(s)': 6692, 'viewing': 6693, 'tendency': 6694, 'punch': 6695, 'Planting': 6696, 'Starting': 6697, 'wishing': 6698, 'holds': 6699, '(45': 6700, 'kg)': 6701, 'hiring': 6702, 'movers': 6703, 'separators': 6704, 'crumpled': 6705, 'cushioning': 6706, 'precious': 6707, 'Bubble': 6708, 'Packing': 6709, 'sturdier': 6710, 'wad': 6711, 'crumbled': 6712, 'bulging': 6713, 'overstuff': 6714, 'broad': 6715, 'Draw': 6716, 'arrows': 6717, '\"FRAGILE': 6718, 'plates\"': 6719, '\"FRAGILE--lamp\"': 6720, 'aftershocks': 6721, 'Aftershocks': 6722, 'earthquakes': 6723, 'shock': 6724, 'earthquake': 6725, 'occur': 6726, 'structurally': 6727, 'aftershock': 6728, 'shaking': 6729, 'shoes': 6730, 'long-sleeve': 6731, 'hat': 6732, 'Exit': 6733, 'stopped': 6734, 'weakened': 6735, 'hits': 6736, 'elevators': 6737, 'Slowly': 6738, 'descend': 6739, 'stadium': 6740, 'calmly': 6741, 'deemed': 6742, 'coast': 6743, 'tsunami': 6744, 'roommate': 6745, 'alright': 6746, 'establish': 6747, 'official': 6748, 'knows': 6749, 're-enter': 6750, 'Drive': 6751, 'Traffic': 6752, 'response': 6753, 'encounter': 6754, 'battery-powered': 6755, 'listen': 6756, 'officials': 6757, 'updates': 6758, 'social': 6759, 'alerts': 6760, 'lie': 6761, 'Standing': 6762, 'cigarette': 6763, 'ashtray': 6764, 'water-damp': 6765, 'ashes': 6766, 'airers': 6767, 'dryers': 6768, 'unavoidable': 6769, 'illumination': 6770, 'cage': 6771, 'Extinguish': 6772, 'relight': 6773, 'highly': 6774, 'combustible': 6775, 'combined': 6776, 'video': 6777, 'amazing': 6778, 'extinguish': 6779, 'phones': 6780, 'straighteners': 6781, 'irons': 6782, 'unplug': 6783, 'Talk': 6784, 'burglaries': 6785, 'surveillance': 6786, 'cameras': 6787, 'break-in': 6788, 'anyone': 6789, 'describes': 6790, 'parked': 6791, 'burglary': 6792, 'bank': 6793, 'accounts': 6794, 'statements': 6795, 'stolen': 6796, 'unauthorized': 6797, 'charges': 6798, 'clues': 6799, 'burglar': 6800, 'liability': 6801, 'swiped': 6802, 'footage': 6803, 'suspects': 6804, 'apprehended': 6805, 'tying': 6806, 'probable': 6807, 'lesser': 6808, 'offense': 6809, 'meantime': 6810, 'pawn': 6811, 'serial': 6812, 'databases': 6813, 'Activate': 6814, 'trackers': 6815, 'locate': 6816, 'tracking': 6817, 'technology': 6818, 'GPS': 6819, 'perpetrator': 6820, 'searched': 6821, 'retrieve': 6822, 'Posting': 6823, 'Encourage': 6824, 'Particularly': 6825, \"suspect's\": 6826, 'rural': 6827, 'Prune': 6828, 'sap': 6829, 'pruned': 6830, 'Maples': 6831, 'elms': 6832, 'walnuts': 6833, 'dogwoods': 6834, 'birches': 6835, 'prune': 6836, 'prunes': 6837, 'crab': 6838, 'apples': 6839, 'locust': 6840, 'poplar': 6841, 'juniper': 6842, 'cherry': 6843, 'bald': 6844, 'cypress': 6845, 'Bradford': 6846, 'pears': 6847, 'Callery': 6848, 'camellias': 6849, 'hydrangeas': 6850, 'mallow': 6851, 'barberries': 6852, 'pruning': 6853, 'dying': 6854, 'shears': 6855, 'branch': 6856, 'collar': 6857, 'meets': 6858, 'thirds': 6859, 'rip': 6860, 'infection': 6861, 'percent': 6862, 'spreading': 6863, 'starting': 6864, 'pathways': 6865, 'Places': 6866, 'mowing': 6867, 'breakages': 6868, 'Thin': 6869, 'photosynthesis': 6870, 'scalp': 6871, \"tree's\": 6872, 'Trying': 6873, 'watered)': 6874, 'whom': 6875, 'visits': 6876, 'refill': 6877, 'Group': 6878, 'ivies': 6879, 'assumptions': 6880, 'skills': 6881, \"neighbor's\": 6882, '½': 6883, '(120': 6884, 'milliliters)': 6885, 'Saturday': 6886, 'Tip': 6887, 'Watering': 6888, 'sitter': 6889, 'Checking': 6890, 'succumb': 6891, 'loss': 6892, 'guilty!': 6893, 'Offer': 6894, 'favor': 6895, '\"no\"': 6896, 'polite': 6897, 'reassure': 6898, 'Time': 6899, 'Bend': 6900, 'Burying': 6901, 'slits': 6902, 'hollow': 6903, 'hollowed-out': 6904, 'fresher': 6905, 'meant': 6906, 'Styrofoam': 6907, 'dented': 6908, 'dome': 6909, 'orb': 6910, 'ombre': 6911, 'sorting': 6912, 'overlooked': 6913, '(763': 6914, 'Poke': 6915, 'Insert': 6916, 'sticking': 6917, 'depths': 6918, 'dome-like': 6919, 'lightest': 6920, 'darkest': 6921, 'rings': 6922, 'votive': 6923, 'Scatter': 6924, 'gems': 6925, 'metallic': 6926, 'sheer': 6927, 'lilies': 6928, 'crowns': 6929, 'clippings': 6930, 'crown': 6931, 'incurable': 6932, 'fungal': 6933, 'issues': 6934, '34': 6935, 'aquatic': 6936, 'hessian': 6937, 'labelled': 6938, 'potting': 6939, 'fluffy': 6940, 'rhizome': 6941, 'hair-like': 6942, '45': 6943, \"lily's\": 6944, 'hasn’t': 6945, 'matured': 6946, '51': 6947, 'container’s': 6948, 'Lightly': 6949, 'floating': 6950, '(13': 6951, 'water’s': 6952, 'bricks': 6953, 'supports': 6954, 'sandbox': 6955, '(preferably': 6956, 'BPA': 6957, 'free)': 6958, 'Drill': 6959, 'improve': 6960, 'collecting': 6961, 'liner': 6962, '(76': 6963, 'well-suited': 6964, 'self-watering': 6965, 'two-liter': 6966, 'upper': 6967, 'jugs': 6968, 'slanted': 6969, 'uniform': 6970, 'sloping': 6971, 'equidistant': 6972, 'knot': 6973, 'liter': 6974, 'ounce': 6975, 'sideways': 6976, \"didn't\": 6977, 'squirt': 6978, 'hunt': 6979, 'joined': 6980, 'hoses': 6981, 'somewhere': 6982, 'leaky': 6983, 'Detach': 6984, 'lube': 6985, 'Reattach': 6986, 'none': 6987, 'slipped': 6988, '\"female\"': 6989, 'punctures': 6990, 'dowel': 6991, \"water's\": 6992, 'incentive': 6993, 'burst': 6994, 'tire': 6995, 'puncture': 6996, 'bicycle': 6997, 'car-part': 6998, 'supplier': 6999, 'boot': 7000, '(use': 7001, 'strongly': 7002, 'coupling': 7003, 'couplings': 7004, 're-join': 7005, 'Shutoff': 7006, 'Splice': 7007, 'lilac': 7008, 'Visit': 7009, \"Palibin'\": 7010, 'Superba': 7011, 'lilacs': 7012, 'Others': 7013, 'Syringa': 7014, 'reticulata': 7015, 'bare-root': 7016, 'container-grown': 7017, 'transplant': 7018, 'saplings': 7019, 'bush': 7020, 'opened': 7021, 'sapling': 7022, 'parent': 7023, 'Lilacs': 7024, 'circulation': 7025, 'grown': 7026, 'sunshine': 7027, 'well-drained': 7028, \"lilacs'\": 7029, 'tepid': 7030, '10–15': 7031, 'rootbound': 7032, 'overgrown': 7033, 'halfway': 7034, 'flare': 7035, 'Covering': 7036, 'bone': 7037, '5–15': 7038, '(15–46': 7039, 'hibiscus': 7040, 'southern': 7041, 'sunroom': 7042, '55º': 7043, '85ºF': 7044, '(127': 7045, '294ºC)': 7046, 'Re-pot': 7047, 'soggy': 7048, 'rainwater': 7049, 'flows': 7050, 'saucers': 7051, 'draining': 7052, 'Touch': 7053, 'schedule': 7054, '50-60': 7055, '%': 7056, 'forced': 7057, 'flowering': 7058, 'water-soluble': 7059, 'directs': 7060, 'fertilization': 7061, 'included': 7062, 'fullness': 7063, 're-pot': 7064, 'bypass': 7065, 'node': 7066, 'nodes': 7067, 'Groom': 7068, 'promptly': 7069, 'yellowed': 7070, 'Dust': 7071, 'wrinkled': 7072, 'browning': 7073, 'insecticide': 7074, 'suspected': 7075, 'fades': 7076, 'faded': 7077, 'lump': 7078, 'sag': 7079, 'droop': 7080, 'shortly': 7081, 'bulb': 7082, 'Only': 7083, 'well-watered': 7084, 'amaryllis': 7085, 'indirect': 7086, 'northern': 7087, 'hemisphere': 7088, 'north': 7089, 'east': 7090, 'south': 7091, 'F': 7092, 'fertilize': 7093, 'half-strength': 7094, 'May': 7095, 'June': 7096, 'December': 7097, 'January': 7098, 'hydrangea': 7099, \"soil's\": 7100, 'cultivated': 7101, 'belong': 7102, 'Hydrangea': 7103, 'macrophylla': 7104, 'adjustment': 7105, 'owner': 7106, 'named': 7107, 'Enziandom': 7108, 'Kasteln': 7109, \"Merritt's\": 7110, 'Supreme': 7111, 'Star': 7112, 'Rose': 7113, 'blossoms': 7114, 'affects': 7115, \"hydrangea's\": 7116, 'prediction': 7117, 'predict': 7118, 'blotched': 7119, 'tbsp': 7120, 'sulfate': 7121, '(lowering': 7122, '10–14': 7123, 'phosphorus': 7124, '25530': 7125, 'superphosphates': 7126, 'presence': 7127, 'driveways': 7128, 'mixes': 7129, 'mortars': 7130, 'leach': 7131, 'inhibits': 7132, 'uptake': 7133, '25-10-10': 7134, 'raising': 7135, 'ash': 7136, 'crushed': 7137, '64': 7138, 'Full': 7139, 'requiring': 7140, 'raspberries': 7141, 'aim': 7142, 'raspberry': 7143, 'juicier': 7144, 'flavorful': 7145, 'Fruit': 7146, 'stunted': 7147, 'shriveled': 7148, 'environmentally': 7149, 'stringing': 7150, 'posts': 7151, 'row-end': 7152, 'T-trellises': 7153, 'V-trellises': 7154, '3½': 7155, '(11': 7156, 'spacing': 7157, 'row': 7158, '(61': 7159, 'Black': 7160, '(91': 7161, 'same-row': 7162, 'Distance': 7163, '(152': 7164, '61': 7165, 'Spacing': 7166, 'adequately': 7167, 'drained': 7168, 'Rich': 7169, 'well-draining': 7170, 'loam': 7171, 'Improve': 7172, 'fare': 7173, 'overwatered': 7174, 'Refrain': 7175, 'previously': 7176, 'bramble': 7177, 'potatoes': 7178, 'peppers': 7179, 'eggplants': 7180, 'diseases': 7181, 'Destroy': 7182, 'wild': 7183, 'blackberries': 7184, '600': 7185, '(183': 7186, 'Wild': 7187, 'transmit': 7188, '56': 7189, '62': 7190, 'agricultural': 7191, 'livable': 7192, 'intimidating': 7193, 'Backless': 7194, 'low-backed': 7195, 'distractions': 7196, 'improves': 7197, 'appealing': 7198, \"weren't\": 7199, 'comfortably': 7200, 'oversize': 7201, 'ottoman': 7202, 'Tapestries': 7203, 'sculptures': 7204, 'ceramics': 7205, 'unusable': 7206, 'sufficient': 7207, 'redesign': 7208, 'wainscoting': 7209, 'Drawing': 7210, 'encompassed': 7211, 'intimate': 7212, 'Grab': 7213, 'Wire': 7214, 'hangers': 7215, 'dents': 7216, 'team’s': 7217, 'hanger’s': 7218, 'plaster': 7219, 'sporting': 7220, 'slipping': 7221, 'Dots': 7222, 'Soften': 7223, 'Feng': 7224, 'Shui': 7225, \"'Poison\": 7226, \"Arrows'\": 7227, '(sharp': 7228, 'pointing': 7229, 'person)': 7230, 'preventingeliminating': 7231, 'Chi': 7232, 'Face': 7233, 'scenes': 7234, 'reflection': 7235, 'blankness': 7236, 'unpleasantness': 7237, 'surprised': 7238, 'facilitated': 7239, 'encouraged': 7240, 'central': 7241, 'structured': 7242, 'street': 7243, 'windchimes': 7244, 'archways': 7245, 'facilitate': 7246, '(wood': 7247, 'glass)': 7248, '(candles': 7249, 'incense)': 7250, '(fountains': 7251, 'vases)': 7252, 'loved': 7253, 'proofs': 7254, 'accomplishment': 7255, 'loneliness': 7256, 'off-white': 7257, 'unit': 7258, 'decals': 7259, 'giant': 7260, 'peel': 7261, 'artwork': 7262, 'unable': 7263, 'stick-on': 7264, 'puttytack': 7265, 'fixed': 7266, 'Nightstands': 7267, 'adjustable': 7268, 'fairy': 7269, 'fixtures': 7270, 'darker-colored': 7271, 'uncluttered': 7272, '“romantic”': 7273, 'tulle': 7274, 'muted': 7275, 'overtly': 7276, 'sexy': 7277, 'understated': 7278, 'Will': 7279, 'tv': 7280, 'elsewhere': 7281, 'accomplish': 7282, 'sharing': 7283, 'redoing': 7284, 'sketches': 7285, 'floorplan': 7286, 'drawings': 7287, '3-D': 7288, 'sketching': 7289, 'thinking': 7290, 'significantly': 7291, 'foremost': 7292, 'passionate': 7293, 'Traditional': 7294, 'Beach': 7295, 'getaway': 7296, 'Retro': 7297, 'Dramatic': 7298, 'Country': 7299, 'algae': 7300, 'smears': 7301, 'brick': 7302, 'Wet': 7303, 'pavers)': 7304, 'react': 7305, 'broom-type': 7306, 'rinsing': 7307, 'irrigation': 7308, 'satisfaction': 7309, 'masonry': 7310, 'muriatic': 7311, '(Review': 7312, 'Warnings': 7313, 'solution)': 7314, 'proportion': 7315, 'copious': 7316, 'mentioned': 7317, 'dilute': 7318, 'soiling': 7319, 'siloxane': 7320, 'silicone': 7321, 'courses': 7322, '\"leads\"': 7323, \"builder's\": 7324, 'aligned': 7325, '\"U\"': 7326, 'Stagger': 7327, 'began': 7328, 'staggered': 7329, 'thick)': 7330, 'spirit': 7331, \"carpenter's\": 7332, '(head': 7333, 'joints)': 7334, '3⁄8': 7335, 'preference': 7336, '3⁄4': 7337, '(19': 7338, 'Strike': 7339, '\"jointer\"': 7340, '\"joint': 7341, 'striker\"': 7342, 'begun': 7343, 'jointer': 7344, 'tubing': 7345, 'whose': 7346, '\"S\"': 7347, '\"foxtail\"': 7348, 'strike': 7349, 'basically': 7350, 'rhythm': 7351, 'outsides': 7352, 'breakers': 7353, 'fuses': 7354, 'circuit': 7355, 'breaker': 7356, 'fuse': 7357, 'connects': 7358, 'theirs': 7359, 'era': 7360, 'electrician': 7361, 'remodels': 7362, 'Fuses': 7363, 'sockets': 7364, 'Older': 7365, 'UNK': 7366}\n",
            "word_dim: 50\n",
            "\n",
            "\n",
            " Training... \n",
            "\n",
            "train_text_ind: 0 of 120\n",
            "rec:      0.361111\t pre:      0.074286\t f1:       0.123223\n",
            "max_loss: 43562.062500\t min_loss: 22.234146\t avg_loss: 1302.589292\n",
            "\n",
            "train_text_ind: 1 of 120\n",
            "rec:      0.150000\t pre:      0.037500\t f1:       0.060000\n",
            "max_loss: 2632.637939\t min_loss: 64.655922\t avg_loss: 717.803508\n",
            "\n",
            "train_text_ind: 2 of 120\n",
            "rec:      0.068966\t pre:      0.058824\t f1:       0.063492\n",
            "max_loss: 611253.750000\t min_loss: 26.508398\t avg_loss: 3221.742259\n",
            "\n",
            "train_text_ind: 3 of 120\n",
            "rec:      0.050000\t pre:      0.040000\t f1:       0.044444\n",
            "max_loss: 3195.731201\t min_loss: 11.904871\t avg_loss: 660.626400\n",
            "\n",
            "train_text_ind: 4 of 120\n",
            "rec:      0.000000\t pre:      0.000000\t f1:       0.000000\n",
            "max_loss: 3599.797607\t min_loss: 8.574347\t avg_loss: 628.461835\n",
            "\n",
            "train_text_ind: 5 of 120\n",
            "rec:      0.066667\t pre:      0.040000\t f1:       0.050000\n",
            "max_loss: 2984.584229\t min_loss: 19.608120\t avg_loss: 559.983151\n",
            "\n",
            "train_text_ind: 6 of 120\n",
            "rec:      0.000000\t pre:      0.000000\t f1:       0.000000\n",
            "max_loss: 1714.140503\t min_loss: 22.289505\t avg_loss: 479.972683\n",
            "\n",
            "train_text_ind: 7 of 120\n",
            "rec:      0.037037\t pre:      0.100000\t f1:       0.054054\n",
            "max_loss: 1663.480591\t min_loss: 36.122635\t avg_loss: 471.837071\n",
            "\n",
            "train_text_ind: 8 of 120\n",
            "rec:      0.105263\t pre:      0.076923\t f1:       0.088889\n",
            "max_loss: 1805.198242\t min_loss: 20.209448\t avg_loss: 418.442526\n",
            "\n",
            "train_text_ind: 9 of 120\n",
            "rec:      0.034483\t pre:      0.062500\t f1:       0.044444\n",
            "max_loss: 1746.950562\t min_loss: 28.628450\t avg_loss: 395.632868\n",
            "\n",
            "train_text_ind: 10 of 120\n",
            "rec:      0.235294\t pre:      0.235294\t f1:       0.235294\n",
            "max_loss: 1690.022583\t min_loss: 22.732826\t avg_loss: 357.063467\n",
            "\n",
            "train_text_ind: 11 of 120\n",
            "rec:      0.034483\t pre:      0.058824\t f1:       0.043478\n",
            "max_loss: 2209.870605\t min_loss: 36.946999\t avg_loss: 344.673668\n",
            "\n",
            "train_text_ind: 12 of 120\n",
            "rec:      0.000000\t pre:      0.000000\t f1:       0.000000\n",
            "max_loss: 1631.134277\t min_loss: 26.248152\t avg_loss: 326.615653\n",
            "\n",
            "train_text_ind: 13 of 120\n",
            "rec:      0.000000\t pre:      0.000000\t f1:       0.000000\n",
            "max_loss: 1452.580200\t min_loss: 23.196548\t avg_loss: 263.988114\n",
            "\n",
            "train_text_ind: 14 of 120\n",
            "rec:      0.111111\t pre:      0.096774\t f1:       0.103448\n",
            "max_loss: 1321.648315\t min_loss: 19.684355\t avg_loss: 243.382842\n",
            "\n",
            "train_text_ind: 15 of 120\n",
            "rec:      0.090909\t pre:      0.130435\t f1:       0.107143\n",
            "max_loss: 1517.756958\t min_loss: 21.432133\t avg_loss: 258.432852\n",
            "\n",
            "train_text_ind: 16 of 120\n",
            "rec:      0.000000\t pre:      0.000000\t f1:       0.000000\n",
            "max_loss: 1749.199707\t min_loss: 14.767816\t avg_loss: 217.174943\n",
            "\n",
            "train_text_ind: 17 of 120\n",
            "rec:      0.066667\t pre:      0.052632\t f1:       0.058824\n",
            "max_loss: 1634.298340\t min_loss: 23.368212\t avg_loss: 216.049423\n",
            "\n",
            "train_text_ind: 18 of 120\n",
            "rec:      0.250000\t pre:      0.136364\t f1:       0.176471\n",
            "max_loss: 831.589600\t min_loss: 16.526272\t avg_loss: 181.348078\n",
            "\n",
            "train_text_ind: 19 of 120\n",
            "rec:      0.200000\t pre:      0.200000\t f1:       0.200000\n",
            "max_loss: 1375.192139\t min_loss: 20.098518\t avg_loss: 155.305325\n",
            "\n",
            "train_text_ind: 20 of 120\n",
            "rec:      0.000000\t pre:      0.000000\t f1:       0.000000\n",
            "max_loss: 975.126709\t min_loss: 15.452195\t avg_loss: 168.458921\n",
            "\n",
            "train_text_ind: 21 of 120\n",
            "rec:      0.095238\t pre:      0.086957\t f1:       0.090909\n",
            "max_loss: 1032.705322\t min_loss: 17.184750\t avg_loss: 153.508953\n",
            "\n",
            "train_text_ind: 22 of 120\n",
            "rec:      0.000000\t pre:      0.000000\t f1:       0.000000\n",
            "max_loss: 860.405151\t min_loss: 13.615541\t avg_loss: 139.633349\n",
            "\n",
            "train_text_ind: 23 of 120\n",
            "rec:      0.052632\t pre:      0.045455\t f1:       0.048780\n",
            "max_loss: 1463.731323\t min_loss: 13.293238\t avg_loss: 139.304427\n",
            "\n",
            "train_text_ind: 24 of 120\n",
            "rec:      0.086957\t pre:      0.133333\t f1:       0.105263\n",
            "max_loss: 842.111511\t min_loss: 14.960317\t avg_loss: 141.228918\n",
            "\n",
            "train_text_ind: 25 of 120\n",
            "rec:      0.074074\t pre:      0.095238\t f1:       0.083333\n",
            "max_loss: 1026.648193\t min_loss: 13.711685\t avg_loss: 141.005382\n",
            "\n",
            "train_text_ind: 26 of 120\n",
            "rec:      0.027778\t pre:      0.071429\t f1:       0.040000\n",
            "max_loss: 832.994568\t min_loss: 16.677132\t avg_loss: 150.670302\n",
            "\n",
            "train_text_ind: 27 of 120\n",
            "rec:      0.000000\t pre:      0.000000\t f1:       0.000000\n",
            "max_loss: 642.560364\t min_loss: 18.232910\t avg_loss: 137.624966\n",
            "\n",
            "train_text_ind: 28 of 120\n",
            "rec:      0.000000\t pre:      0.000000\t f1:       0.000000\n",
            "max_loss: 703.510620\t min_loss: 9.469239\t avg_loss: 119.170317\n",
            "\n",
            "train_text_ind: 29 of 120\n",
            "rec:      0.068966\t pre:      0.076923\t f1:       0.072727\n",
            "max_loss: 638.138428\t min_loss: 10.549032\t avg_loss: 103.638907\n",
            "\n",
            "train_text_ind: 30 of 120\n",
            "rec:      0.000000\t pre:      0.000000\t f1:       0.000000\n",
            "max_loss: 751.411377\t min_loss: 12.152910\t avg_loss: 102.147535\n",
            "\n",
            "train_text_ind: 31 of 120\n",
            "rec:      0.000000\t pre:      0.000000\t f1:       0.000000\n",
            "max_loss: 696.192688\t min_loss: 11.436626\t avg_loss: 110.079187\n",
            "\n",
            "train_text_ind: 32 of 120\n",
            "rec:      0.114286\t pre:      0.181818\t f1:       0.140351\n",
            "max_loss: 954.291870\t min_loss: 15.026413\t avg_loss: 131.724692\n",
            "\n",
            "train_text_ind: 33 of 120\n",
            "rec:      0.132075\t pre:      0.259259\t f1:       0.175000\n",
            "max_loss: 7384.998047\t min_loss: 10.080863\t avg_loss: 195.264169\n",
            "\n",
            "train_text_ind: 34 of 120\n",
            "rec:      0.000000\t pre:      0.000000\t f1:       0.000000\n",
            "max_loss: 1273.189331\t min_loss: 15.609713\t avg_loss: 168.886989\n",
            "\n",
            "train_text_ind: 35 of 120\n",
            "rec:      0.000000\t pre:      0.000000\t f1:       0.000000\n",
            "max_loss: 1039.486938\t min_loss: 17.975834\t avg_loss: 137.847128\n",
            "\n",
            "train_text_ind: 36 of 120\n",
            "rec:      0.047619\t pre:      0.100000\t f1:       0.064516\n",
            "max_loss: 1761.242920\t min_loss: 15.289453\t avg_loss: 146.197291\n",
            "\n",
            "train_text_ind: 37 of 120\n",
            "rec:      0.071429\t pre:      0.100000\t f1:       0.083333\n",
            "max_loss: 1430.482056\t min_loss: 15.851525\t avg_loss: 141.309005\n",
            "\n",
            "train_text_ind: 38 of 120\n",
            "rec:      0.093750\t pre:      0.150000\t f1:       0.115385\n",
            "max_loss: 814.642151\t min_loss: 13.595395\t avg_loss: 145.381223\n",
            "\n",
            "train_text_ind: 39 of 120\n",
            "rec:      0.241379\t pre:      0.269231\t f1:       0.254545\n",
            "max_loss: 1340.072998\t min_loss: 12.977919\t avg_loss: 146.552602\n",
            "\n",
            "train_text_ind: 40 of 120\n",
            "rec:      0.416667\t pre:      0.277778\t f1:       0.333333\n",
            "max_loss: 1466.394287\t min_loss: 8.034754\t avg_loss: 130.060806\n",
            "\n",
            "train_text_ind: 41 of 120\n",
            "rec:      0.187500\t pre:      0.375000\t f1:       0.250000\n",
            "max_loss: 747.780945\t min_loss: 17.180622\t avg_loss: 116.269913\n",
            "\n",
            "train_text_ind: 42 of 120\n",
            "rec:      0.000000\t pre:      0.000000\t f1:       0.000000\n",
            "max_loss: 1040.360596\t min_loss: 15.195133\t avg_loss: 136.162175\n",
            "\n",
            "train_text_ind: 43 of 120\n",
            "rec:      0.481481\t pre:      0.282609\t f1:       0.356164\n",
            "max_loss: 1014.349060\t min_loss: 20.236628\t avg_loss: 135.169736\n",
            "\n",
            "train_text_ind: 44 of 120\n",
            "rec:      0.250000\t pre:      0.157895\t f1:       0.193548\n",
            "max_loss: 537.954102\t min_loss: 23.204880\t avg_loss: 139.763171\n",
            "\n",
            "train_text_ind: 45 of 120\n",
            "rec:      0.441176\t pre:      0.483871\t f1:       0.461538\n",
            "max_loss: 1437.644653\t min_loss: 16.181496\t avg_loss: 135.415556\n",
            "\n",
            "train_text_ind: 46 of 120\n",
            "rec:      0.312500\t pre:      0.357143\t f1:       0.333333\n",
            "max_loss: 810.803833\t min_loss: 9.977632\t avg_loss: 139.866119\n",
            "\n",
            "train_text_ind: 47 of 120\n",
            "rec:      0.785714\t pre:      0.500000\t f1:       0.611111\n",
            "max_loss: 693.960693\t min_loss: 16.199196\t avg_loss: 140.836799\n",
            "\n",
            "train_text_ind: 48 of 120\n",
            "rec:      0.761905\t pre:      0.500000\t f1:       0.603774\n",
            "max_loss: 1093.783691\t min_loss: 14.082661\t avg_loss: 133.172702\n",
            "\n",
            "train_text_ind: 49 of 120\n",
            "rec:      0.363636\t pre:      0.242424\t f1:       0.290909\n",
            "max_loss: 662.259705\t min_loss: 15.697293\t avg_loss: 140.929218\n",
            "\n",
            "train_text_ind: 50 of 120\n",
            "loss list[1302.589292090872, 717.8035075759888, 3221.7422590208052, 660.6263998365403, 628.4618346357346, 559.9831505537034, 479.9726825688073, 471.8370711182895, 418.44252594470976, 395.63286796092984, 357.0634667778015, 344.67366827552314, 326.61565268993377, 263.9881140089035, 243.38284235954285, 258.43285171031954, 217.1749433875084, 216.04942280206924, 181.3480777042668, 155.3053249692917, 168.45892116036697, 153.50895341396333, 139.63334865997484, 139.30442727804183, 141.22891796851644, 141.00538180351256, 150.6703024366941, 137.62496581077576, 119.17031655159403, 103.63890678167343, 102.14753484487534, 110.07918706417084, 131.72469202416843, 195.26416917562486, 168.886988820557, 137.84712756633758, 146.19729063749313, 141.3090046286583, 145.38122307300569, 146.55260223945982, 130.06080615997314, 116.26991322009594, 136.1621747636795, 135.1697359085083, 139.76317059234734, 135.4155563066403, 139.86611853977215, 140.8367988097766, 133.1727022668316, 140.9292175745964]\n",
            "\n",
            "\n",
            " Testing ...\n",
            "\n",
            "valid_text_ind: 0 of 30\n",
            "rec:      0.750000\t pre:      0.750000\t f1:       0.750000\n",
            "\n",
            "valid_text_ind: 1 of 30\n",
            "rec:      0.580645\t pre:      0.818182\t f1:       0.679245\n",
            "\n",
            "valid_text_ind: 2 of 30\n",
            "rec:      0.606061\t pre:      0.769231\t f1:       0.677966\n",
            "\n",
            "valid_text_ind: 3 of 30\n",
            "rec:      0.650000\t pre:      0.481481\t f1:       0.553191\n",
            "\n",
            "valid_text_ind: 4 of 30\n",
            "rec:      0.789474\t pre:      1.000000\t f1:       0.882353\n",
            "\n",
            "valid_text_ind: 5 of 30\n",
            "rec:      0.473684\t pre:      0.600000\t f1:       0.529412\n",
            "\n",
            "valid_text_ind: 6 of 30\n",
            "rec:      0.600000\t pre:      0.800000\t f1:       0.685714\n",
            "\n",
            "valid_text_ind: 7 of 30\n",
            "rec:      0.473684\t pre:      0.692308\t f1:       0.562500\n",
            "\n",
            "valid_text_ind: 8 of 30\n",
            "rec:      0.611111\t pre:      0.758621\t f1:       0.676923\n",
            "\n",
            "valid_text_ind: 9 of 30\n",
            "rec:      0.344828\t pre:      0.833333\t f1:       0.487805\n",
            "\n",
            "valid_text_ind: 10 of 30\n",
            "rec:      0.576923\t pre:      0.833333\t f1:       0.681818\n",
            "\n",
            "valid_text_ind: 11 of 30\n",
            "rec:      0.391304\t pre:      1.000000\t f1:       0.562500\n",
            "\n",
            "valid_text_ind: 12 of 30\n",
            "rec:      0.206897\t pre:      0.857143\t f1:       0.333333\n",
            "\n",
            "valid_text_ind: 13 of 30\n",
            "rec:      0.607143\t pre:      0.944444\t f1:       0.739130\n",
            "\n",
            "valid_text_ind: 14 of 30\n",
            "rec:      0.483871\t pre:      0.652174\t f1:       0.555556\n",
            "\n",
            "valid_text_ind: 15 of 30\n",
            "rec:      0.666667\t pre:      0.666667\t f1:       0.666667\n",
            "\n",
            "valid_text_ind: 16 of 30\n",
            "rec:      0.433333\t pre:      0.722222\t f1:       0.541667\n",
            "\n",
            "valid_text_ind: 17 of 30\n",
            "rec:      0.238095\t pre:      0.500000\t f1:       0.322581\n",
            "\n",
            "valid_text_ind: 18 of 30\n",
            "rec:      0.833333\t pre:      0.833333\t f1:       0.833333\n",
            "\n",
            "valid_text_ind: 19 of 30\n",
            "rec:      0.423077\t pre:      0.785714\t f1:       0.550000\n",
            "\n",
            "valid_text_ind: 20 of 30\n",
            "rec:      0.382353\t pre:      0.764706\t f1:       0.509804\n",
            "\n",
            "valid_text_ind: 21 of 30\n",
            "rec:      0.666667\t pre:      0.818182\t f1:       0.734694\n",
            "\n",
            "valid_text_ind: 22 of 30\n",
            "rec:      0.434783\t pre:      0.833333\t f1:       0.571429\n",
            "\n",
            "valid_text_ind: 23 of 30\n",
            "rec:      0.250000\t pre:      0.500000\t f1:       0.333333\n",
            "\n",
            "valid_text_ind: 24 of 30\n",
            "rec:      0.633333\t pre:      0.826087\t f1:       0.716981\n",
            "\n",
            "valid_text_ind: 25 of 30\n",
            "rec:      0.750000\t pre:      0.794118\t f1:       0.771429\n",
            "\n",
            "valid_text_ind: 26 of 30\n",
            "rec:      0.758621\t pre:      0.880000\t f1:       0.814815\n",
            "\n",
            "valid_text_ind: 27 of 30\n",
            "rec:      0.769231\t pre:      0.833333\t f1:       0.800000\n",
            "\n",
            "valid_text_ind: 28 of 30\n",
            "rec:      0.608696\t pre:      0.777778\t f1:       0.682927\n",
            "\n",
            "valid_text_ind: 29 of 30\n",
            "rec:      0.750000\t pre:      0.750000\t f1:       0.750000\n",
            "\n",
            "\n",
            "-----valid_epoch_end_flag = True-----\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Summary:\n",
            "total_act: 766\t right_act: 426\t tagged_act: 554\n",
            "rec: 0.556136\t pre: 0.768953\t f1: 0.645455\n",
            "\n",
            "cumulative reward: 546365.625000\t average reward: 46.329655\n",
            "\n",
            "Saved weights to /content/drive/My Drive/EASDRL/model/wikihow_act_5_fold3.h5 ...\n",
            "\n",
            "\n",
            " Best f1 score: {'rec': [0.0, 0.556135770234987], 'pre': [0.0, 0.7689530685920578], 'f1': [0.0, 0.6454545454545455], 'rw': [0.0, 46.329655303993974]}  best epoch: 0\n",
            "\n",
            "\n",
            "\n",
            " Training... \n",
            "rec:      0.791667\t pre:      0.760000\t f1:       0.775510\n",
            "max_loss: 36.272758\t min_loss: 36.272758\t avg_loss: 36.272758\n",
            "\n",
            "train_text_ind: 51 of 120\n",
            "rec:      0.590909\t pre:      0.302326\t f1:       0.400000\n",
            "max_loss: 905.763672\t min_loss: 14.859327\t avg_loss: 141.870985\n",
            "\n",
            "train_text_ind: 52 of 120\n",
            "rec:      0.476190\t pre:      0.285714\t f1:       0.357143\n",
            "max_loss: 931.128845\t min_loss: 20.175005\t avg_loss: 145.289791\n",
            "\n",
            "train_text_ind: 53 of 120\n",
            "rec:      0.680000\t pre:      0.435897\t f1:       0.531250\n",
            "max_loss: 917.773682\t min_loss: 17.856581\t avg_loss: 127.072910\n",
            "\n",
            "train_text_ind: 54 of 120\n",
            "rec:      0.750000\t pre:      0.367347\t f1:       0.493151\n",
            "max_loss: 700.198792\t min_loss: 13.861000\t avg_loss: 126.165384\n",
            "\n",
            "train_text_ind: 55 of 120\n",
            "rec:      0.434783\t pre:      0.333333\t f1:       0.377358\n",
            "max_loss: 765.393188\t min_loss: 14.903936\t avg_loss: 116.589121\n",
            "\n",
            "train_text_ind: 56 of 120\n",
            "rec:      0.650000\t pre:      0.481481\t f1:       0.553191\n",
            "max_loss: 1195.003052\t min_loss: 19.614786\t avg_loss: 113.561403\n",
            "\n",
            "train_text_ind: 57 of 120\n",
            "rec:      0.380952\t pre:      0.347826\t f1:       0.363636\n",
            "max_loss: 823.748047\t min_loss: 16.286686\t avg_loss: 133.223414\n",
            "\n",
            "train_text_ind: 58 of 120\n",
            "rec:      0.750000\t pre:      0.428571\t f1:       0.545455\n",
            "max_loss: 950.902771\t min_loss: 10.000719\t avg_loss: 133.287728\n",
            "\n",
            "train_text_ind: 59 of 120\n",
            "rec:      0.650000\t pre:      0.393939\t f1:       0.490566\n",
            "max_loss: 909.712524\t min_loss: 14.056172\t avg_loss: 130.748281\n",
            "\n",
            "train_text_ind: 60 of 120\n",
            "rec:      0.785714\t pre:      0.478261\t f1:       0.594595\n",
            "max_loss: 943.518005\t min_loss: 14.103123\t avg_loss: 126.985978\n",
            "\n",
            "train_text_ind: 61 of 120\n",
            "rec:      0.428571\t pre:      0.214286\t f1:       0.285714\n",
            "max_loss: 914.890503\t min_loss: 9.247499\t avg_loss: 151.019577\n",
            "\n",
            "train_text_ind: 62 of 120\n",
            "rec:      0.500000\t pre:      0.307692\t f1:       0.380952\n",
            "max_loss: 794.981140\t min_loss: 13.280745\t avg_loss: 146.613207\n",
            "\n",
            "train_text_ind: 63 of 120\n",
            "rec:      0.694444\t pre:      0.520833\t f1:       0.595238\n",
            "max_loss: 807.956421\t min_loss: 16.282204\t avg_loss: 131.470627\n",
            "\n",
            "train_text_ind: 64 of 120\n",
            "rec:      0.565217\t pre:      0.448276\t f1:       0.500000\n",
            "max_loss: 831.574585\t min_loss: 10.241297\t avg_loss: 117.486092\n",
            "\n",
            "train_text_ind: 65 of 120\n",
            "rec:      0.500000\t pre:      0.320000\t f1:       0.390244\n",
            "max_loss: 592.893433\t min_loss: 12.777256\t avg_loss: 130.683935\n",
            "\n",
            "train_text_ind: 66 of 120\n",
            "rec:      0.733333\t pre:      0.550000\t f1:       0.628571\n",
            "max_loss: 705.512329\t min_loss: 14.627755\t avg_loss: 123.596113\n",
            "\n",
            "train_text_ind: 67 of 120\n",
            "rec:      0.680851\t pre:      0.592593\t f1:       0.633663\n",
            "max_loss: 747.295349\t min_loss: 11.281212\t avg_loss: 122.400958\n",
            "\n",
            "train_text_ind: 68 of 120\n",
            "rec:      0.727273\t pre:      0.307692\t f1:       0.432432\n",
            "max_loss: 909.268188\t min_loss: 15.846697\t avg_loss: 131.613979\n",
            "\n",
            "train_text_ind: 69 of 120\n",
            "rec:      0.645161\t pre:      0.454545\t f1:       0.533333\n",
            "max_loss: 674.682983\t min_loss: 14.900194\t avg_loss: 120.588947\n",
            "\n",
            "train_text_ind: 70 of 120\n",
            "rec:      0.521739\t pre:      0.400000\t f1:       0.452830\n",
            "max_loss: 943.349731\t min_loss: 15.639213\t avg_loss: 118.590854\n",
            "\n",
            "train_text_ind: 71 of 120\n",
            "rec:      0.666667\t pre:      0.384615\t f1:       0.487805\n",
            "max_loss: 968.000427\t min_loss: 11.042389\t avg_loss: 141.340404\n",
            "\n",
            "train_text_ind: 72 of 120\n",
            "rec:      0.772727\t pre:      0.361702\t f1:       0.492754\n",
            "max_loss: 816.247742\t min_loss: 15.806405\t avg_loss: 126.016433\n",
            "\n",
            "train_text_ind: 73 of 120\n",
            "rec:      0.676471\t pre:      0.442308\t f1:       0.534884\n",
            "max_loss: 775.584717\t min_loss: 15.319130\t avg_loss: 131.688430\n",
            "\n",
            "train_text_ind: 74 of 120\n",
            "rec:      0.777778\t pre:      0.311111\t f1:       0.444444\n",
            "max_loss: 1094.556641\t min_loss: 19.675869\t avg_loss: 139.391358\n",
            "\n",
            "train_text_ind: 75 of 120\n",
            "rec:      0.470588\t pre:      0.216216\t f1:       0.296296\n",
            "max_loss: 693.711548\t min_loss: 15.889520\t avg_loss: 125.818764\n",
            "\n",
            "train_text_ind: 76 of 120\n",
            "rec:      0.709677\t pre:      0.647059\t f1:       0.676923\n",
            "max_loss: 812.399902\t min_loss: 13.322591\t avg_loss: 110.674527\n",
            "\n",
            "train_text_ind: 77 of 120\n",
            "rec:      0.708333\t pre:      0.472222\t f1:       0.566667\n",
            "max_loss: 810.655212\t min_loss: 15.230601\t avg_loss: 133.381393\n",
            "\n",
            "train_text_ind: 78 of 120\n",
            "rec:      0.653846\t pre:      0.395349\t f1:       0.492754\n",
            "max_loss: 759.860229\t min_loss: 12.959100\t avg_loss: 132.565574\n",
            "\n",
            "train_text_ind: 79 of 120\n",
            "rec:      0.764706\t pre:      0.481481\t f1:       0.590909\n",
            "max_loss: 738.138000\t min_loss: 11.953189\t avg_loss: 131.018788\n",
            "\n",
            "train_text_ind: 80 of 120\n",
            "rec:      0.823529\t pre:      0.538462\t f1:       0.651163\n",
            "max_loss: 804.861328\t min_loss: 14.625709\t avg_loss: 125.799082\n",
            "\n",
            "train_text_ind: 81 of 120\n",
            "rec:      0.864865\t pre:      0.492308\t f1:       0.627451\n",
            "max_loss: 670.436279\t min_loss: 13.152950\t avg_loss: 125.864638\n",
            "\n",
            "train_text_ind: 82 of 120\n",
            "rec:      0.590909\t pre:      0.325000\t f1:       0.419355\n",
            "max_loss: 710.176636\t min_loss: 15.728011\t avg_loss: 128.303008\n",
            "\n",
            "train_text_ind: 83 of 120\n",
            "rec:      0.705882\t pre:      0.510638\t f1:       0.592593\n",
            "max_loss: 828.117004\t min_loss: 14.506295\t avg_loss: 138.421821\n",
            "\n",
            "train_text_ind: 84 of 120\n",
            "rec:      0.600000\t pre:      0.300000\t f1:       0.400000\n",
            "max_loss: 826.475403\t min_loss: 11.936417\t avg_loss: 123.413083\n",
            "\n",
            "train_text_ind: 85 of 120\n",
            "rec:      0.714286\t pre:      0.392157\t f1:       0.506329\n",
            "max_loss: 802.968506\t min_loss: 12.941950\t avg_loss: 126.191740\n",
            "\n",
            "train_text_ind: 86 of 120\n",
            "rec:      0.742857\t pre:      0.604651\t f1:       0.666667\n",
            "max_loss: 600.956177\t min_loss: 14.003464\t avg_loss: 135.369155\n",
            "\n",
            "train_text_ind: 87 of 120\n",
            "rec:      0.757576\t pre:      0.446429\t f1:       0.561798\n",
            "max_loss: 1047.779663\t min_loss: 15.648886\t avg_loss: 127.136150\n",
            "\n",
            "train_text_ind: 88 of 120\n",
            "rec:      0.806452\t pre:      0.438596\t f1:       0.568182\n",
            "max_loss: 787.423584\t min_loss: 19.932411\t avg_loss: 140.225096\n",
            "\n",
            "train_text_ind: 89 of 120\n",
            "rec:      0.764706\t pre:      0.351351\t f1:       0.481481\n",
            "max_loss: 794.682495\t min_loss: 12.591259\t avg_loss: 124.755120\n",
            "\n",
            "train_text_ind: 90 of 120\n",
            "rec:      0.806452\t pre:      0.480769\t f1:       0.602410\n",
            "max_loss: 834.062805\t min_loss: 15.066964\t avg_loss: 128.412481\n",
            "\n",
            "train_text_ind: 91 of 120\n",
            "rec:      0.633333\t pre:      0.395833\t f1:       0.487179\n",
            "max_loss: 989.825195\t min_loss: 13.012894\t avg_loss: 120.565271\n",
            "\n",
            "train_text_ind: 92 of 120\n",
            "rec:      0.692308\t pre:      0.418605\t f1:       0.521739\n",
            "max_loss: 1336.821655\t min_loss: 14.523554\t avg_loss: 142.812733\n",
            "\n",
            "train_text_ind: 93 of 120\n",
            "rec:      0.882353\t pre:      0.306122\t f1:       0.454545\n",
            "max_loss: 906.797180\t min_loss: 9.848562\t avg_loss: 143.970000\n",
            "\n",
            "train_text_ind: 94 of 120\n",
            "rec:      0.812500\t pre:      0.500000\t f1:       0.619048\n",
            "max_loss: 1104.051025\t min_loss: 9.188067\t avg_loss: 143.688309\n",
            "\n",
            "train_text_ind: 95 of 120\n",
            "rec:      0.689655\t pre:      0.476190\t f1:       0.563380\n",
            "max_loss: 1156.821899\t min_loss: 12.622979\t avg_loss: 153.980963\n",
            "\n",
            "train_text_ind: 96 of 120\n",
            "rec:      0.684211\t pre:      0.371429\t f1:       0.481481\n",
            "max_loss: 780.011719\t min_loss: 16.326326\t avg_loss: 157.976989\n",
            "\n",
            "train_text_ind: 97 of 120\n",
            "rec:      0.736842\t pre:      0.291667\t f1:       0.417910\n",
            "max_loss: 1113.127197\t min_loss: 14.186859\t avg_loss: 143.642400\n",
            "\n",
            "train_text_ind: 98 of 120\n",
            "rec:      0.708333\t pre:      0.472222\t f1:       0.566667\n",
            "max_loss: 844.505859\t min_loss: 11.016758\t avg_loss: 149.264200\n",
            "\n",
            "train_text_ind: 99 of 120\n",
            "rec:      0.720000\t pre:      0.439024\t f1:       0.545455\n",
            "max_loss: 1070.914917\t min_loss: 16.461203\t avg_loss: 140.032351\n",
            "\n",
            "train_text_ind: 100 of 120\n",
            "loss list[36.27275848388672, 141.87098474502562, 145.28979132299395, 127.07291043561031, 126.16538438796997, 116.58912059899532, 113.5614028207187, 133.22341422080993, 133.28772755622865, 130.7482814049721, 126.98597777366638, 151.01957724571227, 146.61320746680047, 131.4706272983551, 117.48609181234282, 130.68393495559692, 123.59611289540275, 122.40095819950103, 131.6139793920517, 120.5889471912384, 118.59085449448288, 141.34040414672538, 126.0164325785637, 131.68842956385257, 139.39135798931122, 125.81876400709152, 110.67452712480888, 133.38139346837997, 132.5655737423765, 131.01878798246383, 125.79908159971237, 125.86463755607605, 128.30300755262374, 138.42182065248488, 123.41308313134599, 126.19173983097076, 135.36915481731455, 127.13615003108978, 140.22509565302934, 124.75511950538272, 128.41248138189314, 120.5652705656191, 142.812732527256, 143.96999991759088, 143.68830852985383, 153.9809628725052, 157.97698865413665, 143.6424004983902, 149.26419966936112, 140.03235082360612]\n",
            "\n",
            "\n",
            " Testing ...\n",
            "\n",
            "valid_text_ind: 0 of 30\n",
            "rec:      0.750000\t pre:      0.562500\t f1:       0.642857\n",
            "\n",
            "valid_text_ind: 1 of 30\n",
            "rec:      0.700000\t pre:      0.552632\t f1:       0.617647\n",
            "\n",
            "valid_text_ind: 2 of 30\n",
            "rec:      0.600000\t pre:      0.656250\t f1:       0.626866\n",
            "\n",
            "valid_text_ind: 3 of 30\n",
            "rec:      0.750000\t pre:      0.576923\t f1:       0.652174\n",
            "\n",
            "valid_text_ind: 4 of 30\n",
            "rec:      0.850000\t pre:      1.000000\t f1:       0.918919\n",
            "\n",
            "valid_text_ind: 5 of 30\n",
            "rec:      0.631579\t pre:      0.705882\t f1:       0.666667\n",
            "\n",
            "valid_text_ind: 6 of 30\n",
            "rec:      0.652174\t pre:      0.681818\t f1:       0.666667\n",
            "\n",
            "valid_text_ind: 7 of 30\n",
            "rec:      0.590909\t pre:      0.590909\t f1:       0.590909\n",
            "\n",
            "valid_text_ind: 8 of 30\n",
            "rec:      0.810811\t pre:      0.731707\t f1:       0.769231\n",
            "\n",
            "valid_text_ind: 9 of 30\n",
            "rec:      0.709677\t pre:      0.814815\t f1:       0.758621\n",
            "\n",
            "valid_text_ind: 10 of 30\n",
            "rec:      0.769231\t pre:      0.769231\t f1:       0.769231\n",
            "\n",
            "valid_text_ind: 11 of 30\n",
            "rec:      0.708333\t pre:      0.894737\t f1:       0.790698\n",
            "\n",
            "valid_text_ind: 12 of 30\n",
            "rec:      0.517241\t pre:      0.882353\t f1:       0.652174\n",
            "\n",
            "valid_text_ind: 13 of 30\n",
            "rec:      0.785714\t pre:      0.687500\t f1:       0.733333\n",
            "\n",
            "valid_text_ind: 14 of 30\n",
            "rec:      0.677419\t pre:      0.677419\t f1:       0.677419\n",
            "\n",
            "valid_text_ind: 15 of 30\n",
            "rec:      0.857143\t pre:      0.782609\t f1:       0.818182\n",
            "\n",
            "valid_text_ind: 16 of 30\n",
            "rec:      0.483871\t pre:      0.714286\t f1:       0.576923\n",
            "\n",
            "valid_text_ind: 17 of 30\n",
            "rec:      0.545455\t pre:      0.631579\t f1:       0.585366\n",
            "\n",
            "valid_text_ind: 18 of 30\n",
            "rec:      0.875000\t pre:      0.724138\t f1:       0.792453\n",
            "\n",
            "valid_text_ind: 19 of 30\n",
            "rec:      0.500000\t pre:      0.565217\t f1:       0.530612\n",
            "\n",
            "valid_text_ind: 20 of 30\n",
            "rec:      0.694444\t pre:      0.657895\t f1:       0.675676\n",
            "\n",
            "valid_text_ind: 21 of 30\n",
            "rec:      0.774194\t pre:      0.800000\t f1:       0.786885\n",
            "\n",
            "valid_text_ind: 22 of 30\n",
            "rec:      0.608696\t pre:      0.700000\t f1:       0.651163\n",
            "\n",
            "valid_text_ind: 23 of 30\n",
            "rec:      0.650000\t pre:      0.684211\t f1:       0.666667\n",
            "\n",
            "valid_text_ind: 24 of 30\n",
            "rec:      0.838710\t pre:      0.896552\t f1:       0.866667\n",
            "\n",
            "valid_text_ind: 25 of 30\n",
            "rec:      0.777778\t pre:      0.823529\t f1:       0.800000\n",
            "\n",
            "valid_text_ind: 26 of 30\n",
            "rec:      0.793103\t pre:      0.821429\t f1:       0.807018\n",
            "\n",
            "valid_text_ind: 27 of 30\n",
            "rec:      0.851852\t pre:      0.766667\t f1:       0.807018\n",
            "\n",
            "valid_text_ind: 28 of 30\n",
            "rec:      0.739130\t pre:      0.653846\t f1:       0.693878\n",
            "\n",
            "valid_text_ind: 29 of 30\n",
            "rec:      0.708333\t pre:      0.653846\t f1:       0.680000\n",
            "\n",
            "\n",
            "-----valid_epoch_end_flag = True-----\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Summary:\n",
            "total_act: 791\t right_act: 559\t tagged_act: 778\n",
            "rec: 0.706700\t pre: 0.718509\t f1: 0.712556\n",
            "\n",
            "cumulative reward: 563918.671875\t average reward: 47.818085\n",
            "\n",
            "Saved weights to /content/drive/My Drive/EASDRL/model/wikihow_act_5_fold3.h5 ...\n",
            "\n",
            "\n",
            " Best f1 score: {'rec': [0.0, 0.556135770234987, 0.706700379266751], 'pre': [0.0, 0.7689530685920578, 0.718508997429306], 'f1': [0.0, 0.6454545454545455, 0.7125557680050988], 'rw': [0.0, 46.329655303993974, 47.81808461587398]}  best epoch: 0\n",
            "\n",
            "\n",
            "\n",
            " Training... \n",
            "rec:      0.708333\t pre:      0.653846\t f1:       0.680000\n",
            "max_loss: 137.294846\t min_loss: 137.294846\t avg_loss: 137.294846\n",
            "\n",
            "train_text_ind: 101 of 120\n",
            "rec:      0.687500\t pre:      0.305556\t f1:       0.423077\n",
            "max_loss: 867.394714\t min_loss: 12.607268\t avg_loss: 154.261235\n",
            "\n",
            "train_text_ind: 102 of 120\n",
            "rec:      0.916667\t pre:      0.564103\t f1:       0.698413\n",
            "max_loss: 735.572266\t min_loss: 16.905800\t avg_loss: 159.098329\n",
            "\n",
            "train_text_ind: 103 of 120\n",
            "rec:      0.769231\t pre:      0.465116\t f1:       0.579710\n",
            "max_loss: 1292.678589\t min_loss: 11.128217\t avg_loss: 161.783504\n",
            "\n",
            "train_text_ind: 104 of 120\n",
            "rec:      0.880000\t pre:      0.550000\t f1:       0.676923\n",
            "max_loss: 823.867065\t min_loss: 17.823309\t avg_loss: 142.012865\n",
            "\n",
            "train_text_ind: 105 of 120\n",
            "rec:      0.818182\t pre:      0.500000\t f1:       0.620690\n",
            "max_loss: 928.359436\t min_loss: 12.231424\t avg_loss: 152.462894\n",
            "\n",
            "train_text_ind: 106 of 120\n",
            "rec:      0.814815\t pre:      0.478261\t f1:       0.602740\n",
            "max_loss: 1170.750732\t min_loss: 13.319757\t avg_loss: 148.246988\n",
            "\n",
            "train_text_ind: 107 of 120\n",
            "rec:      0.677419\t pre:      0.411765\t f1:       0.512195\n",
            "max_loss: 1653.561768\t min_loss: 11.919629\t avg_loss: 137.313452\n",
            "\n",
            "train_text_ind: 108 of 120\n",
            "rec:      0.764706\t pre:      0.472727\t f1:       0.584270\n",
            "max_loss: 778.394287\t min_loss: 15.096779\t avg_loss: 143.008025\n",
            "\n",
            "train_text_ind: 109 of 120\n",
            "rec:      0.812500\t pre:      0.393939\t f1:       0.530612\n",
            "max_loss: 938.757324\t min_loss: 15.776405\t avg_loss: 159.379192\n",
            "\n",
            "train_text_ind: 110 of 120\n",
            "rec:      0.666667\t pre:      0.375000\t f1:       0.480000\n",
            "max_loss: 969.689209\t min_loss: 14.861599\t avg_loss: 154.239497\n",
            "\n",
            "train_text_ind: 111 of 120\n",
            "rec:      0.684211\t pre:      0.302326\t f1:       0.419355\n",
            "max_loss: 853.156006\t min_loss: 10.193571\t avg_loss: 163.703451\n",
            "\n",
            "train_text_ind: 112 of 120\n",
            "rec:      0.789474\t pre:      0.454545\t f1:       0.576923\n",
            "max_loss: 1114.634644\t min_loss: 13.179005\t avg_loss: 140.554567\n",
            "\n",
            "train_text_ind: 113 of 120\n",
            "rec:      0.869565\t pre:      0.487805\t f1:       0.625000\n",
            "max_loss: 929.582458\t min_loss: 12.696898\t avg_loss: 161.164452\n",
            "\n",
            "train_text_ind: 114 of 120\n",
            "rec:      0.714286\t pre:      0.322581\t f1:       0.444444\n",
            "max_loss: 886.347107\t min_loss: 16.613798\t avg_loss: 144.413713\n",
            "\n",
            "train_text_ind: 115 of 120\n",
            "rec:      0.590909\t pre:      0.351351\t f1:       0.440678\n",
            "max_loss: 904.386475\t min_loss: 16.279001\t avg_loss: 144.420219\n",
            "\n",
            "train_text_ind: 116 of 120\n",
            "rec:      0.727273\t pre:      0.228571\t f1:       0.347826\n",
            "max_loss: 633.534668\t min_loss: 10.151346\t avg_loss: 139.174179\n",
            "\n",
            "train_text_ind: 117 of 120\n",
            "rec:      0.787879\t pre:      0.426230\t f1:       0.553191\n",
            "max_loss: 772.996216\t min_loss: 14.617254\t avg_loss: 129.496743\n",
            "\n",
            "train_text_ind: 118 of 120\n",
            "rec:      0.700000\t pre:      0.388889\t f1:       0.500000\n",
            "max_loss: 772.276550\t min_loss: 10.614828\t avg_loss: 150.128731\n",
            "\n",
            "train_text_ind: 119 of 120\n",
            "rec:      0.818182\t pre:      0.409091\t f1:       0.545455\n",
            "max_loss: 814.432495\t min_loss: 18.177589\t avg_loss: 154.978935\n",
            "\n",
            "\n",
            "-----train_epoch_end_flag = True-----\n",
            "\n",
            "\n",
            "loss list[137.2948455810547, 154.26123538017274, 159.09832923611006, 161.7835040974617, 142.01286532339188, 152.46289430426424, 148.24698775053025, 137.31345200300217, 143.00802453525404, 159.37919224875313, 154.2394974565506, 163.70345052719117, 140.5545668250944, 161.16445246899977, 144.41371256729653, 144.42021874427795, 139.17417944431304, 129.49674257370654, 150.12873090982438, 154.97893545281795]\n",
            "\n",
            "\n",
            " Testing ...\n",
            "\n",
            "valid_text_ind: 0 of 30\n",
            "rec:      0.750000\t pre:      0.450000\t f1:       0.562500\n",
            "\n",
            "valid_text_ind: 1 of 30\n",
            "rec:      0.838710\t pre:      0.565217\t f1:       0.675325\n",
            "\n",
            "valid_text_ind: 2 of 30\n",
            "rec:      0.764706\t pre:      0.764706\t f1:       0.764706\n",
            "\n",
            "valid_text_ind: 3 of 30\n",
            "rec:      0.714286\t pre:      0.454545\t f1:       0.555556\n",
            "\n",
            "valid_text_ind: 4 of 30\n",
            "rec:      0.863636\t pre:      1.000000\t f1:       0.926829\n",
            "\n",
            "valid_text_ind: 5 of 30\n",
            "rec:      0.666667\t pre:      0.736842\t f1:       0.700000\n",
            "\n",
            "valid_text_ind: 6 of 30\n",
            "rec:      0.750000\t pre:      0.666667\t f1:       0.705882\n",
            "\n",
            "valid_text_ind: 7 of 30\n",
            "rec:      0.833333\t pre:      0.606061\t f1:       0.701754\n",
            "\n",
            "valid_text_ind: 8 of 30\n",
            "rec:      0.810811\t pre:      0.600000\t f1:       0.689655\n",
            "\n",
            "valid_text_ind: 9 of 30\n",
            "rec:      0.750000\t pre:      0.750000\t f1:       0.750000\n",
            "\n",
            "valid_text_ind: 10 of 30\n",
            "rec:      0.777778\t pre:      0.777778\t f1:       0.777778\n",
            "\n",
            "valid_text_ind: 11 of 30\n",
            "rec:      0.720000\t pre:      0.782609\t f1:       0.750000\n",
            "\n",
            "valid_text_ind: 12 of 30\n",
            "rec:      0.758621\t pre:      0.846154\t f1:       0.800000\n",
            "\n",
            "valid_text_ind: 13 of 30\n",
            "rec:      0.785714\t pre:      0.709677\t f1:       0.745763\n",
            "\n",
            "valid_text_ind: 14 of 30\n",
            "rec:      0.741935\t pre:      0.621622\t f1:       0.676471\n",
            "\n",
            "valid_text_ind: 15 of 30\n",
            "rec:      0.904762\t pre:      0.678571\t f1:       0.775510\n",
            "\n",
            "valid_text_ind: 16 of 30\n",
            "rec:      0.645161\t pre:      0.800000\t f1:       0.714286\n",
            "\n",
            "valid_text_ind: 17 of 30\n",
            "rec:      0.652174\t pre:      0.652174\t f1:       0.652174\n",
            "\n",
            "valid_text_ind: 18 of 30\n",
            "rec:      0.916667\t pre:      0.687500\t f1:       0.785714\n",
            "\n",
            "valid_text_ind: 19 of 30\n",
            "rec:      0.740741\t pre:      0.645161\t f1:       0.689655\n",
            "\n",
            "valid_text_ind: 20 of 30\n",
            "rec:      0.729730\t pre:      0.771429\t f1:       0.750000\n",
            "\n",
            "valid_text_ind: 21 of 30\n",
            "rec:      0.838710\t pre:      0.838710\t f1:       0.838710\n",
            "\n",
            "valid_text_ind: 22 of 30\n",
            "rec:      0.739130\t pre:      0.809524\t f1:       0.772727\n",
            "\n",
            "valid_text_ind: 23 of 30\n",
            "rec:      0.700000\t pre:      0.560000\t f1:       0.622222\n",
            "\n",
            "valid_text_ind: 24 of 30\n",
            "rec:      0.838710\t pre:      0.684211\t f1:       0.753623\n",
            "\n",
            "valid_text_ind: 25 of 30\n",
            "rec:      0.861111\t pre:      0.775000\t f1:       0.815789\n",
            "\n",
            "valid_text_ind: 26 of 30\n",
            "rec:      0.866667\t pre:      0.764706\t f1:       0.812500\n",
            "\n",
            "valid_text_ind: 27 of 30\n",
            "rec:      0.892857\t pre:      0.757576\t f1:       0.819672\n",
            "\n",
            "valid_text_ind: 28 of 30\n",
            "rec:      0.826087\t pre:      0.593750\t f1:       0.690909\n",
            "\n",
            "valid_text_ind: 29 of 30\n",
            "rec:      0.840000\t pre:      0.750000\t f1:       0.792453\n",
            "\n",
            "\n",
            "-----valid_epoch_end_flag = True-----\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Summary:\n",
            "total_act: 808\t right_act: 635\t tagged_act: 913\n",
            "rec: 0.785891\t pre: 0.695509\t f1: 0.737943\n",
            "\n",
            "cumulative reward: 571364.126563\t average reward: 48.449430\n",
            "\n",
            "Saved weights to /content/drive/My Drive/EASDRL/model/wikihow_act_5_fold3.h5 ...\n",
            "\n",
            "\n",
            " Best f1 score: {'rec': [0.0, 0.556135770234987, 0.706700379266751, 0.7858910891089109], 'pre': [0.0, 0.7689530685920578, 0.718508997429306, 0.6955093099671413], 'f1': [0.0, 0.6454545454545455, 0.7125557680050988, 0.7379430563625798], 'rw': [0.0, 46.329655303993974, 47.81808461587398, 48.44942987895383]}  best epoch: 0\n",
            "\n",
            "\n",
            "\n",
            " Training... \n",
            "\n",
            "train_text_ind: 0 of 120\n",
            "rec:      0.837838\t pre:      0.574074\t f1:       0.681319\n",
            "max_loss: 1547.983032\t min_loss: 18.125912\t avg_loss: 144.439010\n",
            "\n",
            "train_text_ind: 1 of 120\n",
            "rec:      0.650000\t pre:      0.481481\t f1:       0.553191\n",
            "max_loss: 657.351318\t min_loss: 15.816555\t avg_loss: 142.798419\n",
            "\n",
            "train_text_ind: 2 of 120\n",
            "rec:      0.931034\t pre:      0.642857\t f1:       0.760563\n",
            "max_loss: 680.339844\t min_loss: 12.295465\t avg_loss: 156.718814\n",
            "\n",
            "train_text_ind: 3 of 120\n",
            "rec:      0.869565\t pre:      0.555556\t f1:       0.677966\n",
            "max_loss: 1056.398560\t min_loss: 11.859787\t avg_loss: 147.485330\n",
            "\n",
            "train_text_ind: 4 of 120\n",
            "rec:      0.900000\t pre:      0.519231\t f1:       0.658537\n",
            "max_loss: 672.251648\t min_loss: 17.232347\t avg_loss: 124.502943\n",
            "\n",
            "train_text_ind: 5 of 120\n",
            "rec:      0.937500\t pre:      0.384615\t f1:       0.545455\n",
            "max_loss: 913.350647\t min_loss: 12.243289\t avg_loss: 147.420730\n",
            "\n",
            "train_text_ind: 6 of 120\n",
            "rec:      0.785714\t pre:      0.578947\t f1:       0.666667\n",
            "max_loss: 1171.338501\t min_loss: 18.698591\t avg_loss: 159.282878\n",
            "\n",
            "train_text_ind: 7 of 120\n",
            "rec:      0.888889\t pre:      0.600000\t f1:       0.716418\n",
            "max_loss: 665.096497\t min_loss: 10.496561\t avg_loss: 150.810759\n",
            "\n",
            "train_text_ind: 8 of 120\n",
            "rec:      0.761905\t pre:      0.421053\t f1:       0.542373\n",
            "max_loss: 1107.579956\t min_loss: 13.678317\t avg_loss: 147.723188\n",
            "\n",
            "train_text_ind: 9 of 120\n",
            "rec:      0.843750\t pre:      0.562500\t f1:       0.675000\n",
            "max_loss: 1462.514648\t min_loss: 10.343678\t avg_loss: 156.231940\n",
            "\n",
            "train_text_ind: 10 of 120\n",
            "rec:      0.850000\t pre:      0.485714\t f1:       0.618182\n",
            "max_loss: 1015.875793\t min_loss: 15.596043\t avg_loss: 141.359626\n",
            "\n",
            "train_text_ind: 11 of 120\n",
            "rec:      0.833333\t pre:      0.543478\t f1:       0.657895\n",
            "max_loss: 803.346313\t min_loss: 12.296571\t avg_loss: 138.976379\n",
            "\n",
            "train_text_ind: 12 of 120\n",
            "rec:      0.846154\t pre:      0.379310\t f1:       0.523810\n",
            "max_loss: 1089.011963\t min_loss: 13.631702\t avg_loss: 140.844696\n",
            "\n",
            "train_text_ind: 13 of 120\n",
            "rec:      0.789474\t pre:      0.340909\t f1:       0.476190\n",
            "max_loss: 1110.077881\t min_loss: 10.621077\t avg_loss: 140.629643\n",
            "\n",
            "train_text_ind: 14 of 120\n",
            "rec:      0.851852\t pre:      0.575000\t f1:       0.686567\n",
            "max_loss: 1437.542969\t min_loss: 20.712505\t avg_loss: 146.863060\n",
            "\n",
            "train_text_ind: 15 of 120\n",
            "rec:      0.823529\t pre:      0.538462\t f1:       0.651163\n",
            "max_loss: 1200.552246\t min_loss: 15.344873\t avg_loss: 152.392100\n",
            "\n",
            "train_text_ind: 16 of 120\n",
            "rec:      0.846154\t pre:      0.478261\t f1:       0.611111\n",
            "max_loss: 950.102539\t min_loss: 18.159483\t avg_loss: 150.515188\n",
            "\n",
            "train_text_ind: 17 of 120\n",
            "rec:      0.800000\t pre:      0.571429\t f1:       0.666667\n",
            "max_loss: 1236.091919\t min_loss: 8.835875\t avg_loss: 164.912461\n",
            "\n",
            "train_text_ind: 18 of 120\n",
            "rec:      0.857143\t pre:      0.500000\t f1:       0.631579\n",
            "max_loss: 1106.215454\t min_loss: 14.938299\t avg_loss: 138.244965\n",
            "\n",
            "train_text_ind: 19 of 120\n",
            "rec:      0.888889\t pre:      0.444444\t f1:       0.592593\n",
            "max_loss: 1105.971680\t min_loss: 13.503392\t avg_loss: 134.024118\n",
            "\n",
            "train_text_ind: 20 of 120\n",
            "rec:      0.857143\t pre:      0.529412\t f1:       0.654545\n",
            "max_loss: 845.963867\t min_loss: 12.758633\t avg_loss: 147.350272\n",
            "\n",
            "train_text_ind: 21 of 120\n",
            "rec:      0.960000\t pre:      0.571429\t f1:       0.716418\n",
            "max_loss: 1119.257324\t min_loss: 13.274408\t avg_loss: 134.702395\n",
            "\n",
            "train_text_ind: 22 of 120\n",
            "rec:      0.703704\t pre:      0.463415\t f1:       0.558824\n",
            "max_loss: 616.687378\t min_loss: 10.951103\t avg_loss: 120.075992\n",
            "\n",
            "train_text_ind: 23 of 120\n",
            "rec:      0.842105\t pre:      0.400000\t f1:       0.542373\n",
            "max_loss: 1034.546143\t min_loss: 16.665949\t avg_loss: 135.550223\n",
            "\n",
            "train_text_ind: 24 of 120\n",
            "rec:      0.916667\t pre:      0.594595\t f1:       0.721311\n",
            "max_loss: 650.852356\t min_loss: 17.568035\t avg_loss: 143.336290\n",
            "\n",
            "train_text_ind: 25 of 120\n",
            "rec:      0.862069\t pre:      0.454545\t f1:       0.595238\n",
            "max_loss: 870.399109\t min_loss: 14.313578\t avg_loss: 138.023277\n",
            "\n",
            "train_text_ind: 26 of 120\n",
            "rec:      0.918919\t pre:      0.607143\t f1:       0.731183\n",
            "max_loss: 1555.351318\t min_loss: 14.927471\t avg_loss: 159.239313\n",
            "\n",
            "train_text_ind: 27 of 120\n",
            "rec:      0.900000\t pre:      0.382979\t f1:       0.537313\n",
            "max_loss: 911.736694\t min_loss: 10.419867\t avg_loss: 135.621177\n",
            "\n",
            "train_text_ind: 28 of 120\n",
            "rec:      0.884615\t pre:      0.560976\t f1:       0.686567\n",
            "max_loss: 842.919189\t min_loss: 17.443048\t avg_loss: 139.669169\n",
            "\n",
            "train_text_ind: 29 of 120\n",
            "rec:      0.916667\t pre:      0.647059\t f1:       0.758621\n",
            "max_loss: 812.751160\t min_loss: 12.377098\t avg_loss: 145.432732\n",
            "\n",
            "train_text_ind: 30 of 120\n",
            "rec:      0.850000\t pre:      0.361702\t f1:       0.507463\n",
            "max_loss: 945.133301\t min_loss: 14.890967\t avg_loss: 136.724800\n",
            "\n",
            "train_text_ind: 31 of 120\n",
            "rec:      0.791667\t pre:      0.475000\t f1:       0.593750\n",
            "max_loss: 901.903381\t min_loss: 17.209469\t avg_loss: 155.612727\n",
            "\n",
            "train_text_ind: 32 of 120\n",
            "rec:      0.916667\t pre:      0.523810\t f1:       0.666667\n",
            "max_loss: 925.022888\t min_loss: 13.154054\t avg_loss: 148.426802\n",
            "\n",
            "train_text_ind: 33 of 120\n",
            "rec:      0.833333\t pre:      0.625000\t f1:       0.714286\n",
            "max_loss: 761.775818\t min_loss: 14.295704\t avg_loss: 127.355296\n",
            "\n",
            "train_text_ind: 34 of 120\n",
            "rec:      0.846154\t pre:      0.578947\t f1:       0.687500\n",
            "max_loss: 1107.763184\t min_loss: 13.184989\t avg_loss: 140.655039\n",
            "\n",
            "train_text_ind: 35 of 120\n",
            "rec:      0.850000\t pre:      0.309091\t f1:       0.453333\n",
            "max_loss: 925.360229\t min_loss: 10.047354\t avg_loss: 149.770983\n",
            "\n",
            "train_text_ind: 36 of 120\n",
            "rec:      0.883721\t pre:      0.703704\t f1:       0.783505\n",
            "max_loss: 1348.686279\t min_loss: 13.413538\t avg_loss: 140.970123\n",
            "\n",
            "train_text_ind: 37 of 120\n",
            "rec:      0.857143\t pre:      0.545455\t f1:       0.666667\n",
            "max_loss: 785.846008\t min_loss: 19.477448\t avg_loss: 145.145497\n",
            "\n",
            "train_text_ind: 38 of 120\n",
            "rec:      0.787879\t pre:      0.490566\t f1:       0.604651\n",
            "max_loss: 1005.362732\t min_loss: 8.005319\t avg_loss: 141.934570\n",
            "\n",
            "train_text_ind: 39 of 120\n",
            "rec:      0.882353\t pre:      0.588235\t f1:       0.705882\n",
            "max_loss: 1253.887695\t min_loss: 18.714020\t avg_loss: 130.171489\n",
            "\n",
            "train_text_ind: 40 of 120\n",
            "rec:      0.846154\t pre:      0.468085\t f1:       0.602740\n",
            "max_loss: 964.714966\t min_loss: 12.764885\t avg_loss: 135.145612\n",
            "\n",
            "train_text_ind: 41 of 120\n",
            "rec:      0.750000\t pre:      0.500000\t f1:       0.600000\n",
            "max_loss: 1040.836060\t min_loss: 13.938943\t avg_loss: 128.014854\n",
            "\n",
            "train_text_ind: 42 of 120\n",
            "rec:      0.750000\t pre:      0.103448\t f1:       0.181818\n",
            "max_loss: 2257.820068\t min_loss: 12.194441\t avg_loss: 139.283865\n",
            "\n",
            "train_text_ind: 43 of 120\n",
            "rec:      0.821429\t pre:      0.489362\t f1:       0.613333\n",
            "max_loss: 717.576294\t min_loss: 13.325101\t avg_loss: 134.392929\n",
            "\n",
            "train_text_ind: 44 of 120\n",
            "rec:      0.785714\t pre:      0.392857\t f1:       0.523810\n",
            "max_loss: 915.776245\t min_loss: 16.153858\t avg_loss: 151.278071\n",
            "\n",
            "train_text_ind: 45 of 120\n",
            "rec:      0.864865\t pre:      0.627451\t f1:       0.727273\n",
            "max_loss: 1530.387695\t min_loss: 14.828466\t avg_loss: 151.345300\n",
            "\n",
            "train_text_ind: 46 of 120\n",
            "rec:      0.906250\t pre:      0.568627\t f1:       0.698795\n",
            "max_loss: 773.169556\t min_loss: 14.665209\t avg_loss: 135.722131\n",
            "\n",
            "train_text_ind: 47 of 120\n",
            "rec:      0.857143\t pre:      0.545455\t f1:       0.666667\n",
            "max_loss: 808.083618\t min_loss: 14.355467\t avg_loss: 123.428057\n",
            "\n",
            "train_text_ind: 48 of 120\n",
            "rec:      0.714286\t pre:      0.483871\t f1:       0.576923\n",
            "max_loss: 979.494080\t min_loss: 20.547869\t avg_loss: 139.089310\n",
            "\n",
            "train_text_ind: 49 of 120\n",
            "rec:      0.809524\t pre:      0.435897\t f1:       0.566667\n",
            "max_loss: 1379.721191\t min_loss: 10.319344\t avg_loss: 136.691719\n",
            "\n",
            "train_text_ind: 50 of 120\n",
            "loss list[144.43901042938234, 142.79841880321501, 156.71881363153457, 147.48532969951629, 124.5029426240921, 147.42072985887526, 159.28287819221157, 150.81075910019547, 147.7231881594658, 156.23194013118743, 141.35962645053863, 138.97637939190406, 140.84469607830047, 140.62964309215545, 146.8630604028702, 152.39210035800934, 150.51518751144408, 164.9124613126119, 138.24496516367284, 134.02411808252336, 147.35027195065064, 134.70239520549774, 120.07599153171401, 135.55022321224212, 143.33629046978595, 138.02327740192413, 159.2393132697425, 135.6211772918701, 139.66916856359927, 145.4327323246002, 136.7248004603386, 155.61272727012636, 148.42680190560597, 127.35529598712921, 140.6550387863515, 149.7709830570221, 140.9701233959198, 145.14549726963043, 141.93457032442092, 130.17148906217713, 135.1456121301651, 128.01485407693045, 139.28386483907698, 134.3929290318489, 151.2780708641595, 151.34529999395212, 135.72213147372602, 123.42805747355311, 139.089309786806, 136.6917189526558]\n",
            "\n",
            "\n",
            " Testing ...\n",
            "\n",
            "valid_text_ind: 0 of 30\n",
            "rec:      0.785714\t pre:      0.611111\t f1:       0.687500\n",
            "\n",
            "valid_text_ind: 1 of 30\n",
            "rec:      0.870968\t pre:      0.729730\t f1:       0.794118\n",
            "\n",
            "valid_text_ind: 2 of 30\n",
            "rec:      0.810811\t pre:      0.789474\t f1:       0.800000\n",
            "\n",
            "valid_text_ind: 3 of 30\n",
            "rec:      0.714286\t pre:      0.600000\t f1:       0.652174\n",
            "\n",
            "valid_text_ind: 4 of 30\n",
            "rec:      0.869565\t pre:      1.000000\t f1:       0.930233\n",
            "\n",
            "valid_text_ind: 5 of 30\n",
            "rec:      0.750000\t pre:      0.937500\t f1:       0.833333\n",
            "\n",
            "valid_text_ind: 6 of 30\n",
            "rec:      0.666667\t pre:      0.636364\t f1:       0.651163\n",
            "\n",
            "valid_text_ind: 7 of 30\n",
            "rec:      0.761905\t pre:      0.592593\t f1:       0.666667\n",
            "\n",
            "valid_text_ind: 8 of 30\n",
            "rec:      0.777778\t pre:      0.666667\t f1:       0.717949\n",
            "\n",
            "valid_text_ind: 9 of 30\n",
            "rec:      0.677419\t pre:      0.807692\t f1:       0.736842\n",
            "\n",
            "valid_text_ind: 10 of 30\n",
            "rec:      0.740741\t pre:      0.800000\t f1:       0.769231\n",
            "\n",
            "valid_text_ind: 11 of 30\n",
            "rec:      0.680000\t pre:      0.809524\t f1:       0.739130\n",
            "\n",
            "valid_text_ind: 12 of 30\n",
            "rec:      0.655172\t pre:      0.950000\t f1:       0.775510\n",
            "\n",
            "valid_text_ind: 13 of 30\n",
            "rec:      0.857143\t pre:      0.727273\t f1:       0.786885\n",
            "\n",
            "valid_text_ind: 14 of 30\n",
            "rec:      0.741935\t pre:      0.638889\t f1:       0.686567\n",
            "\n",
            "valid_text_ind: 15 of 30\n",
            "rec:      0.913043\t pre:      0.724138\t f1:       0.807692\n",
            "\n",
            "valid_text_ind: 16 of 30\n",
            "rec:      0.600000\t pre:      0.857143\t f1:       0.705882\n",
            "\n",
            "valid_text_ind: 17 of 30\n",
            "rec:      0.652174\t pre:      0.681818\t f1:       0.666667\n",
            "\n",
            "valid_text_ind: 18 of 30\n",
            "rec:      0.958333\t pre:      0.575000\t f1:       0.718750\n",
            "\n",
            "valid_text_ind: 19 of 30\n",
            "rec:      0.576923\t pre:      0.555556\t f1:       0.566038\n",
            "\n",
            "valid_text_ind: 20 of 30\n",
            "rec:      0.666667\t pre:      0.685714\t f1:       0.676056\n",
            "\n",
            "valid_text_ind: 21 of 30\n",
            "rec:      0.827586\t pre:      0.827586\t f1:       0.827586\n",
            "\n",
            "valid_text_ind: 22 of 30\n",
            "rec:      0.695652\t pre:      0.761905\t f1:       0.727273\n",
            "\n",
            "valid_text_ind: 23 of 30\n",
            "rec:      0.750000\t pre:      0.319149\t f1:       0.447761\n",
            "\n",
            "valid_text_ind: 24 of 30\n",
            "rec:      0.838710\t pre:      0.866667\t f1:       0.852459\n",
            "\n",
            "valid_text_ind: 25 of 30\n",
            "rec:      0.833333\t pre:      0.833333\t f1:       0.833333\n",
            "\n",
            "valid_text_ind: 26 of 30\n",
            "rec:      0.758621\t pre:      0.733333\t f1:       0.745763\n",
            "\n",
            "valid_text_ind: 27 of 30\n",
            "rec:      0.807692\t pre:      0.913043\t f1:       0.857143\n",
            "\n",
            "valid_text_ind: 28 of 30\n",
            "rec:      0.869565\t pre:      0.588235\t f1:       0.701754\n",
            "\n",
            "valid_text_ind: 29 of 30\n",
            "rec:      0.875000\t pre:      0.807692\t f1:       0.840000\n",
            "\n",
            "\n",
            "-----valid_epoch_end_flag = True-----\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Summary:\n",
            "total_act: 798\t right_act: 611\t tagged_act: 856\n",
            "rec: 0.765664\t pre: 0.713785\t f1: 0.738815\n",
            "\n",
            "cumulative reward: 570851.293750\t average reward: 48.405944\n",
            "\n",
            "Saved weights to /content/drive/My Drive/EASDRL/model/wikihow_act_5_fold3.h5 ...\n",
            "\n",
            "\n",
            " Best f1 score: {'rec': [0.0, 0.556135770234987, 0.706700379266751, 0.7858910891089109, 0.7656641604010025], 'pre': [0.0, 0.7689530685920578, 0.718508997429306, 0.6955093099671413, 0.7137850467289719], 'f1': [0.0, 0.6454545454545455, 0.7125557680050988, 0.7379430563625798, 0.7388149939540508], 'rw': [0.0, 46.329655303993974, 47.81808461587398, 48.44942987895383, 48.405943674213184]}  best epoch: 1\n",
            "\n",
            "\n",
            "\n",
            " Training... \n",
            "rec:      0.875000\t pre:      0.807692\t f1:       0.840000\n",
            "max_loss: 114.778030\t min_loss: 114.778030\t avg_loss: 114.778030\n",
            "\n",
            "train_text_ind: 51 of 120\n",
            "rec:      0.818182\t pre:      0.439024\t f1:       0.571429\n",
            "max_loss: 1078.137939\t min_loss: 19.440029\t avg_loss: 149.682950\n",
            "\n",
            "train_text_ind: 52 of 120\n",
            "rec:      0.857143\t pre:      0.439024\t f1:       0.580645\n",
            "max_loss: 834.966797\t min_loss: 17.451900\t avg_loss: 134.527772\n",
            "\n",
            "train_text_ind: 53 of 120\n",
            "rec:      0.840000\t pre:      0.500000\t f1:       0.626866\n",
            "max_loss: 1258.860840\t min_loss: 15.025233\t avg_loss: 136.191330\n",
            "\n",
            "train_text_ind: 54 of 120\n",
            "rec:      0.833333\t pre:      0.487805\t f1:       0.615385\n",
            "max_loss: 1089.979370\t min_loss: 14.072504\t avg_loss: 148.606120\n",
            "\n",
            "train_text_ind: 55 of 120\n",
            "rec:      0.652174\t pre:      0.535714\t f1:       0.588235\n",
            "max_loss: 1395.188354\t min_loss: 12.604076\t avg_loss: 141.231310\n",
            "\n",
            "train_text_ind: 56 of 120\n",
            "rec:      0.909091\t pre:      0.526316\t f1:       0.666667\n",
            "max_loss: 915.480835\t min_loss: 11.015567\t avg_loss: 149.548514\n",
            "\n",
            "train_text_ind: 57 of 120\n",
            "rec:      0.750000\t pre:      0.418605\t f1:       0.537313\n",
            "max_loss: 1086.043701\t min_loss: 17.133984\t avg_loss: 149.756996\n",
            "\n",
            "train_text_ind: 58 of 120\n",
            "rec:      0.791667\t pre:      0.422222\t f1:       0.550725\n",
            "max_loss: 1066.066895\t min_loss: 10.697886\t avg_loss: 140.123273\n",
            "\n",
            "train_text_ind: 59 of 120\n",
            "rec:      0.900000\t pre:      0.428571\t f1:       0.580645\n",
            "max_loss: 1608.009766\t min_loss: 16.865810\t avg_loss: 129.758762\n",
            "\n",
            "train_text_ind: 60 of 120\n",
            "rec:      0.814815\t pre:      0.448980\t f1:       0.578947\n",
            "max_loss: 712.941345\t min_loss: 12.747580\t avg_loss: 130.280131\n",
            "\n",
            "train_text_ind: 61 of 120\n",
            "rec:      0.842105\t pre:      0.347826\t f1:       0.492308\n",
            "max_loss: 951.399780\t min_loss: 15.113149\t avg_loss: 136.639329\n",
            "\n",
            "train_text_ind: 62 of 120\n",
            "rec:      0.769231\t pre:      0.512821\t f1:       0.615385\n",
            "max_loss: 907.138672\t min_loss: 10.463042\t avg_loss: 145.175084\n",
            "\n",
            "train_text_ind: 63 of 120\n",
            "rec:      0.833333\t pre:      0.576923\t f1:       0.681818\n",
            "max_loss: 1382.390381\t min_loss: 11.721009\t avg_loss: 146.231513\n",
            "\n",
            "train_text_ind: 64 of 120\n",
            "rec:      0.782609\t pre:      0.450000\t f1:       0.571429\n",
            "max_loss: 708.003479\t min_loss: 20.915108\t avg_loss: 139.432626\n",
            "\n",
            "train_text_ind: 65 of 120\n",
            "rec:      0.888889\t pre:      0.410256\t f1:       0.561404\n",
            "max_loss: 851.125244\t min_loss: 17.495102\t avg_loss: 138.411038\n",
            "\n",
            "train_text_ind: 66 of 120\n",
            "rec:      0.870968\t pre:      0.692308\t f1:       0.771429\n",
            "max_loss: 650.507690\t min_loss: 17.511824\t avg_loss: 146.596604\n",
            "\n",
            "train_text_ind: 67 of 120\n",
            "rec:      0.872340\t pre:      0.745455\t f1:       0.803922\n",
            "max_loss: 1348.415771\t min_loss: 15.451368\t avg_loss: 164.155477\n",
            "\n",
            "train_text_ind: 68 of 120\n",
            "rec:      0.857143\t pre:      0.363636\t f1:       0.510638\n",
            "max_loss: 1594.316284\t min_loss: 19.543478\t avg_loss: 172.208790\n",
            "\n",
            "train_text_ind: 69 of 120\n",
            "rec:      0.875000\t pre:      0.682927\t f1:       0.767123\n",
            "max_loss: 1126.307007\t min_loss: 16.563345\t avg_loss: 150.807027\n",
            "\n",
            "train_text_ind: 70 of 120\n",
            "rec:      0.666667\t pre:      0.500000\t f1:       0.571429\n",
            "max_loss: 1178.602295\t min_loss: 16.109638\t avg_loss: 161.832888\n",
            "\n",
            "train_text_ind: 71 of 120\n",
            "rec:      0.928571\t pre:      0.481481\t f1:       0.634146\n",
            "max_loss: 1266.933716\t min_loss: 8.293152\t avg_loss: 160.271772\n",
            "\n",
            "train_text_ind: 72 of 120\n",
            "rec:      0.818182\t pre:      0.439024\t f1:       0.571429\n",
            "max_loss: 1003.415710\t min_loss: 11.845655\t avg_loss: 158.287305\n",
            "\n",
            "train_text_ind: 73 of 120\n",
            "rec:      0.818182\t pre:      0.540000\t f1:       0.650602\n",
            "max_loss: 891.808350\t min_loss: 9.531034\t avg_loss: 149.803870\n",
            "\n",
            "train_text_ind: 74 of 120\n",
            "rec:      0.894737\t pre:      0.395349\t f1:       0.548387\n",
            "max_loss: 912.407349\t min_loss: 17.307961\t avg_loss: 140.102383\n",
            "\n",
            "train_text_ind: 75 of 120\n",
            "rec:      0.947368\t pre:      0.461538\t f1:       0.620690\n",
            "max_loss: 832.591187\t min_loss: 14.865029\t avg_loss: 128.081669\n",
            "\n",
            "train_text_ind: 76 of 120\n",
            "rec:      0.838710\t pre:      0.553191\t f1:       0.666667\n",
            "max_loss: 1156.077393\t min_loss: 13.074539\t avg_loss: 138.109586\n",
            "\n",
            "train_text_ind: 77 of 120\n",
            "rec:      0.875000\t pre:      0.446809\t f1:       0.591549\n",
            "max_loss: 1028.997803\t min_loss: 12.666245\t avg_loss: 154.733174\n",
            "\n",
            "train_text_ind: 78 of 120\n",
            "rec:      0.928571\t pre:      0.684211\t f1:       0.787879\n",
            "max_loss: 1085.200439\t min_loss: 17.041674\t avg_loss: 155.007897\n",
            "\n",
            "train_text_ind: 79 of 120\n",
            "rec:      0.857143\t pre:      0.576923\t f1:       0.689655\n",
            "max_loss: 916.369507\t min_loss: 13.204090\t avg_loss: 154.223675\n",
            "\n",
            "train_text_ind: 80 of 120\n",
            "rec:      0.875000\t pre:      0.595745\t f1:       0.708861\n",
            "max_loss: 739.829285\t min_loss: 16.171923\t avg_loss: 152.599858\n",
            "\n",
            "train_text_ind: 81 of 120\n",
            "rec:      0.842105\t pre:      0.551724\t f1:       0.666667\n",
            "max_loss: 629.789185\t min_loss: 22.058216\t avg_loss: 138.821169\n",
            "\n",
            "train_text_ind: 82 of 120\n",
            "rec:      0.727273\t pre:      0.372093\t f1:       0.492308\n",
            "max_loss: 674.489380\t min_loss: 21.936005\t avg_loss: 141.778428\n",
            "\n",
            "train_text_ind: 83 of 120\n",
            "rec:      0.800000\t pre:      0.538462\t f1:       0.643678\n",
            "max_loss: 1037.595825\t min_loss: 14.860559\t avg_loss: 144.378195\n",
            "\n",
            "train_text_ind: 84 of 120\n",
            "rec:      0.733333\t pre:      0.440000\t f1:       0.550000\n",
            "max_loss: 1487.692383\t min_loss: 20.237619\t avg_loss: 172.370645\n",
            "\n",
            "train_text_ind: 85 of 120\n",
            "rec:      0.740741\t pre:      0.454545\t f1:       0.563380\n",
            "max_loss: 896.007996\t min_loss: 13.982185\t avg_loss: 164.335203\n",
            "\n",
            "train_text_ind: 86 of 120\n",
            "rec:      0.885714\t pre:      0.607843\t f1:       0.720930\n",
            "max_loss: 1024.539673\t min_loss: 15.365302\t avg_loss: 151.805789\n",
            "\n",
            "train_text_ind: 87 of 120\n",
            "rec:      0.848485\t pre:      0.518519\t f1:       0.643678\n",
            "max_loss: 979.635986\t min_loss: 12.583814\t avg_loss: 168.438191\n",
            "\n",
            "train_text_ind: 88 of 120\n",
            "rec:      0.870968\t pre:      0.500000\t f1:       0.635294\n",
            "max_loss: 818.993347\t min_loss: 20.018845\t avg_loss: 148.678206\n",
            "\n",
            "train_text_ind: 89 of 120\n",
            "rec:      0.687500\t pre:      0.379310\t f1:       0.488889\n",
            "max_loss: 1034.647705\t min_loss: 11.948446\t avg_loss: 161.306936\n",
            "\n",
            "train_text_ind: 90 of 120\n",
            "rec:      0.838710\t pre:      0.472727\t f1:       0.604651\n",
            "max_loss: 946.737244\t min_loss: 17.229599\t avg_loss: 157.966574\n",
            "\n",
            "train_text_ind: 91 of 120\n",
            "rec:      0.724138\t pre:      0.488372\t f1:       0.583333\n",
            "max_loss: 1098.531494\t min_loss: 12.447472\t avg_loss: 147.290130\n",
            "\n",
            "train_text_ind: 92 of 120\n",
            "rec:      0.740741\t pre:      0.571429\t f1:       0.645161\n",
            "max_loss: 1320.260376\t min_loss: 19.230316\t avg_loss: 145.774023\n",
            "\n",
            "train_text_ind: 93 of 120\n",
            "rec:      0.764706\t pre:      0.333333\t f1:       0.464286\n",
            "max_loss: 1016.868896\t min_loss: 16.066929\t avg_loss: 154.558422\n",
            "\n",
            "train_text_ind: 94 of 120\n",
            "rec:      0.843750\t pre:      0.586957\t f1:       0.692308\n",
            "max_loss: 918.711121\t min_loss: 21.407135\t avg_loss: 146.474341\n",
            "\n",
            "train_text_ind: 95 of 120\n",
            "rec:      0.821429\t pre:      0.469388\t f1:       0.597403\n",
            "max_loss: 1058.041992\t min_loss: 14.106496\t avg_loss: 143.073554\n",
            "\n",
            "train_text_ind: 96 of 120\n",
            "rec:      0.777778\t pre:      0.368421\t f1:       0.500000\n",
            "max_loss: 1076.849731\t min_loss: 14.311423\t avg_loss: 145.589192\n",
            "\n",
            "train_text_ind: 97 of 120\n",
            "rec:      0.684211\t pre:      0.351351\t f1:       0.464286\n",
            "max_loss: 1165.670654\t min_loss: 11.991802\t avg_loss: 159.079618\n",
            "\n",
            "train_text_ind: 98 of 120\n",
            "rec:      0.750000\t pre:      0.461538\t f1:       0.571429\n",
            "max_loss: 1322.681152\t min_loss: 10.679200\t avg_loss: 175.731671\n",
            "\n",
            "train_text_ind: 99 of 120\n",
            "rec:      0.923077\t pre:      0.413793\t f1:       0.571429\n",
            "max_loss: 936.328857\t min_loss: 10.729627\t avg_loss: 164.702970\n",
            "\n",
            "train_text_ind: 100 of 120\n",
            "loss list[114.77803039550781, 149.68295001506806, 134.5277719935146, 136.19133029315407, 148.6061195755005, 141.2313097635905, 149.54851412279854, 149.7569955730438, 140.12327307224274, 129.7587619113922, 130.28013102054595, 136.6393286061287, 145.17508386862502, 146.23151298761368, 139.432625579834, 138.41103785514832, 146.59660442539902, 164.1554774904251, 172.20878987789155, 150.80702681064605, 161.8328883381491, 160.2717722404454, 158.28730546474458, 149.80386989061222, 140.10238348960877, 128.0816691160202, 138.10958552935807, 154.73317432641983, 155.00789737701416, 154.22367468118668, 152.59985799789428, 138.82116908073425, 141.77842843055726, 144.37819544315337, 172.3706451154735, 164.33520328044892, 151.80578896050812, 168.43819094896315, 148.67820587360038, 161.30693551605822, 157.9665743303299, 147.2901297288217, 145.7740227508545, 154.5584223555548, 146.47434136867523, 143.0735535979271, 145.58919195890428, 159.07961786031723, 175.73167064666748, 164.70297025969893]\n",
            "\n",
            "\n",
            " Testing ...\n",
            "\n",
            "valid_text_ind: 0 of 30\n",
            "rec:      0.833333\t pre:      0.588235\t f1:       0.689655\n",
            "\n",
            "valid_text_ind: 1 of 30\n",
            "rec:      0.821429\t pre:      0.766667\t f1:       0.793103\n",
            "\n",
            "valid_text_ind: 2 of 30\n",
            "rec:      0.828571\t pre:      0.805556\t f1:       0.816901\n",
            "\n",
            "valid_text_ind: 3 of 30\n",
            "rec:      0.809524\t pre:      0.607143\t f1:       0.693878\n",
            "\n",
            "valid_text_ind: 4 of 30\n",
            "rec:      0.913043\t pre:      0.807692\t f1:       0.857143\n",
            "\n",
            "valid_text_ind: 5 of 30\n",
            "rec:      0.619048\t pre:      0.684211\t f1:       0.650000\n",
            "\n",
            "valid_text_ind: 6 of 30\n",
            "rec:      0.714286\t pre:      0.789474\t f1:       0.750000\n",
            "\n",
            "valid_text_ind: 7 of 30\n",
            "rec:      0.681818\t pre:      0.714286\t f1:       0.697674\n",
            "\n",
            "valid_text_ind: 8 of 30\n",
            "rec:      0.888889\t pre:      0.653061\t f1:       0.752941\n",
            "\n",
            "valid_text_ind: 9 of 30\n",
            "rec:      0.741935\t pre:      0.821429\t f1:       0.779661\n",
            "\n",
            "valid_text_ind: 10 of 30\n",
            "rec:      0.640000\t pre:      0.727273\t f1:       0.680851\n",
            "\n",
            "valid_text_ind: 11 of 30\n",
            "rec:      0.680000\t pre:      0.739130\t f1:       0.708333\n",
            "\n",
            "valid_text_ind: 12 of 30\n",
            "rec:      0.633333\t pre:      0.863636\t f1:       0.730769\n",
            "\n",
            "valid_text_ind: 13 of 30\n",
            "rec:      0.785714\t pre:      0.846154\t f1:       0.814815\n",
            "\n",
            "valid_text_ind: 14 of 30\n",
            "rec:      0.645161\t pre:      0.625000\t f1:       0.634921\n",
            "\n",
            "valid_text_ind: 15 of 30\n",
            "rec:      0.960000\t pre:      0.750000\t f1:       0.842105\n",
            "\n",
            "valid_text_ind: 16 of 30\n",
            "rec:      0.466667\t pre:      0.823529\t f1:       0.595745\n",
            "\n",
            "valid_text_ind: 17 of 30\n",
            "rec:      0.791667\t pre:      0.730769\t f1:       0.760000\n",
            "\n",
            "valid_text_ind: 18 of 30\n",
            "rec:      1.000000\t pre:      0.533333\t f1:       0.695652\n",
            "\n",
            "valid_text_ind: 19 of 30\n",
            "rec:      0.807692\t pre:      0.600000\t f1:       0.688525\n",
            "\n",
            "valid_text_ind: 20 of 30\n",
            "rec:      0.666667\t pre:      0.750000\t f1:       0.705882\n",
            "\n",
            "valid_text_ind: 21 of 30\n",
            "rec:      0.846154\t pre:      0.846154\t f1:       0.846154\n",
            "\n",
            "valid_text_ind: 22 of 30\n",
            "rec:      0.739130\t pre:      0.708333\t f1:       0.723404\n",
            "\n",
            "valid_text_ind: 23 of 30\n",
            "rec:      0.600000\t pre:      0.521739\t f1:       0.558140\n",
            "\n",
            "valid_text_ind: 24 of 30\n",
            "rec:      0.733333\t pre:      0.785714\t f1:       0.758621\n",
            "\n",
            "valid_text_ind: 25 of 30\n",
            "rec:      0.888889\t pre:      0.914286\t f1:       0.901408\n",
            "\n",
            "valid_text_ind: 26 of 30\n",
            "rec:      0.821429\t pre:      0.766667\t f1:       0.793103\n",
            "\n",
            "valid_text_ind: 27 of 30\n",
            "rec:      0.740741\t pre:      0.740741\t f1:       0.740741\n",
            "\n",
            "valid_text_ind: 28 of 30\n",
            "rec:      0.913043\t pre:      0.617647\t f1:       0.736842\n",
            "\n",
            "valid_text_ind: 29 of 30\n",
            "rec:      0.875000\t pre:      0.750000\t f1:       0.807692\n",
            "\n",
            "\n",
            "-----valid_epoch_end_flag = True-----\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Summary:\n",
            "total_act: 791\t right_act: 608\t tagged_act: 840\n",
            "rec: 0.768647\t pre: 0.723810\t f1: 0.745555\n",
            "\n",
            "cumulative reward: 571767.090625\t average reward: 48.483600\n",
            "\n",
            "Saved weights to /content/drive/My Drive/EASDRL/model/wikihow_act_5_fold3.h5 ...\n",
            "\n",
            "\n",
            " Best f1 score: {'rec': [0.0, 0.556135770234987, 0.706700379266751, 0.7858910891089109, 0.7656641604010025, 0.7686472819216182], 'pre': [0.0, 0.7689530685920578, 0.718508997429306, 0.6955093099671413, 0.7137850467289719, 0.7238095238095238], 'f1': [0.0, 0.6454545454545455, 0.7125557680050988, 0.7379430563625798, 0.7388149939540508, 0.745554874310239], 'rw': [0.0, 46.329655303993974, 47.81808461587398, 48.44942987895383, 48.405943674213184, 48.483599645976454]}  best epoch: 1\n",
            "\n",
            "\n",
            "\n",
            " Training... \n",
            "rec:      0.875000\t pre:      0.750000\t f1:       0.807692\n",
            "max_loss: 68.066757\t min_loss: 68.066757\t avg_loss: 68.066757\n",
            "\n",
            "train_text_ind: 101 of 120\n",
            "rec:      0.562500\t pre:      0.230769\t f1:       0.327273\n",
            "max_loss: 1316.845703\t min_loss: 19.455906\t avg_loss: 161.162547\n",
            "\n",
            "train_text_ind: 102 of 120\n",
            "rec:      0.909091\t pre:      0.714286\t f1:       0.800000\n",
            "max_loss: 1069.079346\t min_loss: 19.304144\t avg_loss: 171.057814\n",
            "\n",
            "train_text_ind: 103 of 120\n",
            "rec:      0.962963\t pre:      0.520000\t f1:       0.675325\n",
            "max_loss: 1429.882935\t min_loss: 20.129988\t avg_loss: 161.958084\n",
            "\n",
            "train_text_ind: 104 of 120\n",
            "rec:      0.826087\t pre:      0.558824\t f1:       0.666667\n",
            "max_loss: 1098.824341\t min_loss: 17.315737\t avg_loss: 166.061276\n",
            "\n",
            "train_text_ind: 105 of 120\n",
            "rec:      0.969697\t pre:      0.666667\t f1:       0.790123\n",
            "max_loss: 878.015259\t min_loss: 22.046436\t avg_loss: 157.910053\n",
            "\n",
            "train_text_ind: 106 of 120\n",
            "rec:      0.846154\t pre:      0.372881\t f1:       0.517647\n",
            "max_loss: 1169.287598\t min_loss: 17.645071\t avg_loss: 155.319695\n",
            "\n",
            "train_text_ind: 107 of 120\n",
            "rec:      0.900000\t pre:      0.465517\t f1:       0.613636\n",
            "max_loss: 1294.683838\t min_loss: 13.685753\t avg_loss: 147.926934\n",
            "\n",
            "train_text_ind: 108 of 120\n",
            "rec:      0.800000\t pre:      0.608696\t f1:       0.691358\n",
            "max_loss: 1584.603271\t min_loss: 16.157797\t avg_loss: 150.511062\n",
            "\n",
            "train_text_ind: 109 of 120\n",
            "rec:      0.687500\t pre:      0.354839\t f1:       0.468085\n",
            "max_loss: 1376.562256\t min_loss: 14.973330\t avg_loss: 157.478257\n",
            "\n",
            "train_text_ind: 110 of 120\n",
            "rec:      0.705882\t pre:      0.342857\t f1:       0.461538\n",
            "max_loss: 1513.928955\t min_loss: 11.787008\t avg_loss: 160.533832\n",
            "\n",
            "train_text_ind: 111 of 120\n",
            "rec:      0.789474\t pre:      0.319149\t f1:       0.454545\n",
            "max_loss: 1495.965454\t min_loss: 12.207694\t avg_loss: 165.237284\n",
            "\n",
            "train_text_ind: 112 of 120\n",
            "rec:      0.842105\t pre:      0.444444\t f1:       0.581818\n",
            "max_loss: 820.611816\t min_loss: 22.011177\t avg_loss: 184.606152\n",
            "\n",
            "train_text_ind: 113 of 120\n",
            "rec:      0.826087\t pre:      0.703704\t f1:       0.760000\n",
            "max_loss: 765.864075\t min_loss: 24.306679\t avg_loss: 172.296350\n",
            "\n",
            "train_text_ind: 114 of 120\n",
            "rec:      0.800000\t pre:      0.363636\t f1:       0.500000\n",
            "max_loss: 669.264343\t min_loss: 18.698734\t avg_loss: 172.092125\n",
            "\n",
            "train_text_ind: 115 of 120\n",
            "rec:      0.857143\t pre:      0.418605\t f1:       0.562500\n",
            "max_loss: 879.898132\t min_loss: 15.817377\t avg_loss: 145.516812\n",
            "\n",
            "train_text_ind: 116 of 120\n",
            "rec:      0.818182\t pre:      0.236842\t f1:       0.367347\n",
            "max_loss: 1021.655762\t min_loss: 12.494972\t avg_loss: 156.023819\n",
            "\n",
            "train_text_ind: 117 of 120\n",
            "rec:      0.848485\t pre:      0.518519\t f1:       0.643678\n",
            "max_loss: 1176.668945\t min_loss: 14.195756\t avg_loss: 167.393812\n",
            "\n",
            "train_text_ind: 118 of 120\n",
            "rec:      0.750000\t pre:      0.468750\t f1:       0.576923\n",
            "max_loss: 944.736755\t min_loss: 15.294324\t avg_loss: 188.472803\n",
            "\n",
            "train_text_ind: 119 of 120\n",
            "rec:      0.769231\t pre:      0.416667\t f1:       0.540541\n",
            "max_loss: 599.409912\t min_loss: 17.496214\t avg_loss: 145.708564\n",
            "\n",
            "\n",
            "-----train_epoch_end_flag = True-----\n",
            "\n",
            "\n",
            "loss list[68.06675720214844, 161.1625469827652, 171.05781428019205, 161.95808437347412, 166.0612758647405, 157.91005270579862, 155.31969468593599, 147.9269344997406, 150.51106186312532, 157.47825692040578, 160.53383150577545, 165.23728402376176, 184.6061524350219, 172.2963500401964, 172.09212489950247, 145.51681153297423, 156.02381920337677, 167.39381184711408, 188.47280345201492, 145.7085636954271]\n",
            "\n",
            "\n",
            " Testing ...\n",
            "\n",
            "valid_text_ind: 0 of 30\n",
            "rec:      0.583333\t pre:      0.304348\t f1:       0.400000\n",
            "\n",
            "valid_text_ind: 1 of 30\n",
            "rec:      0.896552\t pre:      0.666667\t f1:       0.764706\n",
            "\n",
            "valid_text_ind: 2 of 30\n",
            "rec:      0.833333\t pre:      0.681818\t f1:       0.750000\n",
            "\n",
            "valid_text_ind: 3 of 30\n",
            "rec:      0.700000\t pre:      0.437500\t f1:       0.538462\n",
            "\n",
            "valid_text_ind: 4 of 30\n",
            "rec:      0.916667\t pre:      0.956522\t f1:       0.936170\n",
            "\n",
            "valid_text_ind: 5 of 30\n",
            "rec:      0.761905\t pre:      0.666667\t f1:       0.711111\n",
            "\n",
            "valid_text_ind: 6 of 30\n",
            "rec:      0.760000\t pre:      0.593750\t f1:       0.666667\n",
            "\n",
            "valid_text_ind: 7 of 30\n",
            "rec:      0.869565\t pre:      0.588235\t f1:       0.701754\n",
            "\n",
            "valid_text_ind: 8 of 30\n",
            "rec:      0.891892\t pre:      0.600000\t f1:       0.717391\n",
            "\n",
            "valid_text_ind: 9 of 30\n",
            "rec:      0.666667\t pre:      0.785714\t f1:       0.721311\n",
            "\n",
            "valid_text_ind: 10 of 30\n",
            "rec:      0.653846\t pre:      0.629630\t f1:       0.641509\n",
            "\n",
            "valid_text_ind: 11 of 30\n",
            "rec:      0.695652\t pre:      0.640000\t f1:       0.666667\n",
            "\n",
            "valid_text_ind: 12 of 30\n",
            "rec:      0.666667\t pre:      0.689655\t f1:       0.677966\n",
            "\n",
            "valid_text_ind: 13 of 30\n",
            "rec:      0.896552\t pre:      0.742857\t f1:       0.812500\n",
            "\n",
            "valid_text_ind: 14 of 30\n",
            "rec:      0.709677\t pre:      0.666667\t f1:       0.687500\n",
            "\n",
            "valid_text_ind: 15 of 30\n",
            "rec:      0.904762\t pre:      0.791667\t f1:       0.844444\n",
            "\n",
            "valid_text_ind: 16 of 30\n",
            "rec:      0.633333\t pre:      0.791667\t f1:       0.703704\n",
            "\n",
            "valid_text_ind: 17 of 30\n",
            "rec:      0.727273\t pre:      0.615385\t f1:       0.666667\n",
            "\n",
            "valid_text_ind: 18 of 30\n",
            "rec:      1.000000\t pre:      0.631579\t f1:       0.774194\n",
            "\n",
            "valid_text_ind: 19 of 30\n",
            "rec:      0.814815\t pre:      0.687500\t f1:       0.745763\n",
            "\n",
            "valid_text_ind: 20 of 30\n",
            "rec:      0.694444\t pre:      0.657895\t f1:       0.675676\n",
            "\n",
            "valid_text_ind: 21 of 30\n",
            "rec:      0.866667\t pre:      0.702703\t f1:       0.776119\n",
            "\n",
            "valid_text_ind: 22 of 30\n",
            "rec:      0.608696\t pre:      0.560000\t f1:       0.583333\n",
            "\n",
            "valid_text_ind: 23 of 30\n",
            "rec:      0.600000\t pre:      0.521739\t f1:       0.558140\n",
            "\n",
            "valid_text_ind: 24 of 30\n",
            "rec:      0.935484\t pre:      0.725000\t f1:       0.816901\n",
            "\n",
            "valid_text_ind: 25 of 30\n",
            "rec:      0.857143\t pre:      0.857143\t f1:       0.857143\n",
            "\n",
            "valid_text_ind: 26 of 30\n",
            "rec:      0.870968\t pre:      0.794118\t f1:       0.830769\n",
            "\n",
            "valid_text_ind: 27 of 30\n",
            "rec:      0.888889\t pre:      0.827586\t f1:       0.857143\n",
            "\n",
            "valid_text_ind: 28 of 30\n",
            "rec:      0.913043\t pre:      0.583333\t f1:       0.711864\n",
            "\n",
            "valid_text_ind: 29 of 30\n",
            "rec:      0.833333\t pre:      0.606061\t f1:       0.701754\n",
            "\n",
            "\n",
            "-----valid_epoch_end_flag = True-----\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Summary:\n",
            "total_act: 803\t right_act: 638\t tagged_act: 957\n",
            "rec: 0.794521\t pre: 0.666667\t f1: 0.725000\n",
            "\n",
            "cumulative reward: 569451.243750\t average reward: 48.287225\n",
            "\n",
            "当前最高F1为0.745554874310239\n",
            "\n",
            "\n",
            " Training... \n",
            "\n",
            "train_text_ind: 0 of 120\n",
            "rec:      0.829268\t pre:      0.586207\t f1:       0.686869\n",
            "max_loss: 1446.379517\t min_loss: 15.436111\t avg_loss: 179.744550\n",
            "\n",
            "train_text_ind: 1 of 120\n",
            "rec:      0.750000\t pre:      0.441176\t f1:       0.555556\n",
            "max_loss: 1380.376831\t min_loss: 21.252697\t avg_loss: 167.806185\n",
            "\n",
            "train_text_ind: 2 of 120\n",
            "rec:      0.838710\t pre:      0.472727\t f1:       0.604651\n",
            "max_loss: 977.760925\t min_loss: 20.103155\t avg_loss: 171.538393\n",
            "\n",
            "train_text_ind: 3 of 120\n",
            "rec:      0.869565\t pre:      0.476190\t f1:       0.615385\n",
            "max_loss: 1208.798096\t min_loss: 14.980640\t avg_loss: 171.634315\n",
            "\n",
            "train_text_ind: 4 of 120\n",
            "rec:      0.900000\t pre:      0.658537\t f1:       0.760563\n",
            "max_loss: 1131.252686\t min_loss: 14.319083\t avg_loss: 192.848169\n",
            "\n",
            "train_text_ind: 5 of 120\n",
            "rec:      0.875000\t pre:      0.350000\t f1:       0.500000\n",
            "max_loss: 1029.192993\t min_loss: 6.704450\t avg_loss: 167.948208\n",
            "\n",
            "train_text_ind: 6 of 120\n",
            "rec:      0.777778\t pre:      0.538462\t f1:       0.636364\n",
            "max_loss: 761.407715\t min_loss: 15.972363\t avg_loss: 154.706704\n",
            "\n",
            "train_text_ind: 7 of 120\n",
            "rec:      0.629630\t pre:      0.548387\t f1:       0.586207\n",
            "max_loss: 793.012329\t min_loss: 18.819504\t avg_loss: 183.611937\n",
            "\n",
            "train_text_ind: 8 of 120\n",
            "rec:      0.714286\t pre:      0.500000\t f1:       0.588235\n",
            "max_loss: 1571.262451\t min_loss: 16.850967\t avg_loss: 170.455409\n",
            "\n",
            "train_text_ind: 9 of 120\n",
            "rec:      0.875000\t pre:      0.491228\t f1:       0.629213\n",
            "max_loss: 1412.487671\t min_loss: 18.781685\t avg_loss: 178.543535\n",
            "\n",
            "train_text_ind: 10 of 120\n",
            "rec:      0.800000\t pre:      0.410256\t f1:       0.542373\n",
            "max_loss: 1435.326782\t min_loss: 17.272415\t avg_loss: 193.009269\n",
            "\n",
            "train_text_ind: 11 of 120\n",
            "rec:      0.838710\t pre:      0.577778\t f1:       0.684211\n",
            "max_loss: 1301.553711\t min_loss: 14.409190\t avg_loss: 166.037850\n",
            "\n",
            "train_text_ind: 12 of 120\n",
            "rec:      0.666667\t pre:      0.275862\t f1:       0.390244\n",
            "max_loss: 839.560913\t min_loss: 11.379396\t avg_loss: 168.177965\n",
            "\n",
            "train_text_ind: 13 of 120\n",
            "rec:      0.904762\t pre:      0.441860\t f1:       0.593750\n",
            "max_loss: 815.167603\t min_loss: 17.044430\t avg_loss: 178.151042\n",
            "\n",
            "train_text_ind: 14 of 120\n",
            "rec:      0.703704\t pre:      0.431818\t f1:       0.535211\n",
            "max_loss: 992.242615\t min_loss: 21.856117\t avg_loss: 180.798699\n",
            "\n",
            "train_text_ind: 15 of 120\n",
            "rec:      0.735294\t pre:      0.471698\t f1:       0.574713\n",
            "max_loss: 1324.192383\t min_loss: 15.974039\t avg_loss: 180.793665\n",
            "\n",
            "train_text_ind: 16 of 120\n",
            "rec:      0.923077\t pre:      0.352941\t f1:       0.510638\n",
            "max_loss: 1215.411621\t min_loss: 13.158401\t avg_loss: 167.820465\n",
            "\n",
            "train_text_ind: 17 of 120\n",
            "rec:      0.727273\t pre:      0.533333\t f1:       0.615385\n",
            "max_loss: 1060.587769\t min_loss: 22.016176\t avg_loss: 198.701532\n",
            "\n",
            "train_text_ind: 18 of 120\n",
            "rec:      0.928571\t pre:      0.406250\t f1:       0.565217\n",
            "max_loss: 1184.547729\t min_loss: 16.888844\t avg_loss: 159.582538\n",
            "\n",
            "train_text_ind: 19 of 120\n",
            "rec:      0.935484\t pre:      0.580000\t f1:       0.716049\n",
            "max_loss: 1039.109985\t min_loss: 20.889277\t avg_loss: 174.928340\n",
            "\n",
            "train_text_ind: 20 of 120\n",
            "rec:      0.909091\t pre:      0.465116\t f1:       0.615385\n",
            "max_loss: 1459.018311\t min_loss: 10.858875\t avg_loss: 163.839425\n",
            "\n",
            "train_text_ind: 21 of 120\n",
            "rec:      0.888889\t pre:      0.510638\t f1:       0.648649\n",
            "max_loss: 1072.121826\t min_loss: 19.334915\t avg_loss: 185.416521\n",
            "\n",
            "train_text_ind: 22 of 120\n",
            "rec:      0.862069\t pre:      0.581395\t f1:       0.694444\n",
            "max_loss: 891.011353\t min_loss: 28.863863\t avg_loss: 187.513984\n",
            "\n",
            "train_text_ind: 23 of 120\n",
            "rec:      0.842105\t pre:      0.347826\t f1:       0.492308\n",
            "max_loss: 1161.559326\t min_loss: 15.407944\t avg_loss: 205.537878\n",
            "\n",
            "train_text_ind: 24 of 120\n",
            "rec:      0.826087\t pre:      0.542857\t f1:       0.655172\n",
            "max_loss: 1377.292603\t min_loss: 8.809689\t avg_loss: 186.115605\n",
            "\n",
            "train_text_ind: 25 of 120\n",
            "rec:      0.750000\t pre:      0.512195\t f1:       0.608696\n",
            "max_loss: 998.733215\t min_loss: 17.023724\t avg_loss: 176.051243\n",
            "\n",
            "train_text_ind: 26 of 120\n",
            "rec:      0.868421\t pre:      0.515625\t f1:       0.647059\n",
            "max_loss: 1092.565308\t min_loss: 9.683740\t avg_loss: 171.530796\n",
            "\n",
            "train_text_ind: 27 of 120\n",
            "rec:      0.894737\t pre:      0.404762\t f1:       0.557377\n",
            "max_loss: 1438.078735\t min_loss: 21.591919\t avg_loss: 155.186999\n",
            "\n",
            "train_text_ind: 28 of 120\n",
            "rec:      0.892857\t pre:      0.480769\t f1:       0.625000\n",
            "max_loss: 1583.002686\t min_loss: 17.285469\t avg_loss: 192.630076\n",
            "\n",
            "train_text_ind: 29 of 120\n",
            "rec:      0.911765\t pre:      0.508197\t f1:       0.652632\n",
            "max_loss: 1244.347046\t min_loss: 23.421490\t avg_loss: 190.467587\n",
            "\n",
            "train_text_ind: 30 of 120\n",
            "rec:      0.789474\t pre:      0.428571\t f1:       0.555556\n",
            "max_loss: 866.413330\t min_loss: 13.855007\t avg_loss: 189.599822\n",
            "\n",
            "train_text_ind: 31 of 120\n",
            "rec:      0.807692\t pre:      0.466667\t f1:       0.591549\n",
            "max_loss: 1144.416260\t min_loss: 17.213240\t avg_loss: 178.947222\n",
            "\n",
            "train_text_ind: 32 of 120\n",
            "rec:      0.916667\t pre:      0.733333\t f1:       0.814815\n",
            "max_loss: 1412.109863\t min_loss: 9.484452\t avg_loss: 161.313514\n",
            "\n",
            "train_text_ind: 33 of 120\n",
            "rec:      0.888889\t pre:      0.640000\t f1:       0.744186\n",
            "max_loss: 1569.009888\t min_loss: 15.770025\t avg_loss: 183.295712\n",
            "\n",
            "train_text_ind: 34 of 120\n",
            "rec:      0.840000\t pre:      0.525000\t f1:       0.646154\n",
            "max_loss: 1027.416382\t min_loss: 15.383778\t avg_loss: 195.453836\n",
            "\n",
            "train_text_ind: 35 of 120\n",
            "rec:      0.900000\t pre:      0.352941\t f1:       0.507042\n",
            "max_loss: 957.930298\t min_loss: 20.527229\t avg_loss: 206.114840\n",
            "\n",
            "train_text_ind: 36 of 120\n",
            "rec:      0.813953\t pre:      0.636364\t f1:       0.714286\n",
            "max_loss: 994.500366\t min_loss: 13.455707\t avg_loss: 180.241337\n",
            "\n",
            "train_text_ind: 37 of 120\n",
            "rec:      0.821429\t pre:      0.489362\t f1:       0.613333\n",
            "max_loss: 1005.673340\t min_loss: 25.572853\t avg_loss: 199.974321\n",
            "\n",
            "train_text_ind: 38 of 120\n",
            "rec:      0.882353\t pre:      0.500000\t f1:       0.638298\n",
            "max_loss: 988.326538\t min_loss: 24.961788\t avg_loss: 186.910116\n",
            "\n",
            "train_text_ind: 39 of 120\n",
            "rec:      0.852941\t pre:      0.580000\t f1:       0.690476\n",
            "max_loss: 849.169189\t min_loss: 16.695328\t avg_loss: 200.818693\n",
            "\n",
            "train_text_ind: 40 of 120\n",
            "rec:      0.846154\t pre:      0.440000\t f1:       0.578947\n",
            "max_loss: 919.683105\t min_loss: 14.735065\t avg_loss: 182.180196\n",
            "\n",
            "train_text_ind: 41 of 120\n",
            "rec:      0.906250\t pre:      0.500000\t f1:       0.644444\n",
            "max_loss: 1133.566162\t min_loss: 18.639317\t avg_loss: 192.255591\n",
            "\n",
            "train_text_ind: 42 of 120\n",
            "rec:      0.750000\t pre:      0.125000\t f1:       0.214286\n",
            "max_loss: 1283.549194\t min_loss: 13.380112\t avg_loss: 186.197720\n",
            "\n",
            "train_text_ind: 43 of 120\n",
            "rec:      0.964286\t pre:      0.540000\t f1:       0.692308\n",
            "max_loss: 1095.326172\t min_loss: 9.835999\t avg_loss: 183.031527\n",
            "\n",
            "train_text_ind: 44 of 120\n",
            "rec:      0.692308\t pre:      0.375000\t f1:       0.486486\n",
            "max_loss: 1258.932129\t min_loss: 22.094078\t avg_loss: 201.491547\n",
            "\n",
            "train_text_ind: 45 of 120\n",
            "rec:      0.916667\t pre:      0.568966\t f1:       0.702128\n",
            "max_loss: 1272.215454\t min_loss: 14.370573\t avg_loss: 183.791987\n",
            "\n",
            "train_text_ind: 46 of 120\n",
            "rec:      0.852941\t pre:      0.557692\t f1:       0.674419\n",
            "max_loss: 980.080261\t min_loss: 14.028597\t avg_loss: 177.066458\n",
            "\n",
            "train_text_ind: 47 of 120\n",
            "rec:      0.857143\t pre:      0.545455\t f1:       0.666667\n",
            "max_loss: 920.368652\t min_loss: 12.599646\t avg_loss: 169.816524\n",
            "\n",
            "train_text_ind: 48 of 120\n",
            "rec:      0.952381\t pre:      0.645161\t f1:       0.769231\n",
            "max_loss: 1372.931030\t min_loss: 19.902681\t avg_loss: 170.511322\n",
            "\n",
            "train_text_ind: 49 of 120\n",
            "rec:      0.727273\t pre:      0.390244\t f1:       0.507937\n",
            "max_loss: 1318.114502\t min_loss: 16.810066\t avg_loss: 186.624264\n",
            "\n",
            "train_text_ind: 50 of 120\n",
            "loss list[179.7445501947403, 167.80618502616883, 171.53839303970338, 171.6343153333664, 192.84816883325576, 167.94820773363114, 154.70670382073308, 183.61193700685894, 170.4554092645645, 178.54353490829467, 193.0092693567276, 166.03784971263454, 168.17796484947203, 178.15104206085206, 180.79869899749755, 180.79366461277007, 167.82046470880508, 198.7015324959388, 159.58253766850729, 174.92833983898163, 163.83942527574868, 185.41652133464814, 187.51398360629042, 205.5378779888153, 186.11560523753263, 176.05124348163605, 171.53079611108043, 155.18699851989746, 192.63007633736794, 190.46758713245393, 189.5998218536377, 178.9472224998474, 161.31351438115854, 183.29571210861207, 195.45383560692255, 206.11484049320222, 180.24133723974228, 199.97432065963744, 186.91011573314665, 200.81869250280732, 182.18019609689713, 192.25559121169053, 186.1977197933197, 183.0315270972252, 201.49154727378587, 183.79198697706065, 177.06645778645137, 169.8165237647443, 170.51132183263798, 186.62426391124725]\n",
            "\n",
            "\n",
            " Testing ...\n",
            "\n",
            "valid_text_ind: 0 of 30\n",
            "rec:      0.769231\t pre:      0.625000\t f1:       0.689655\n",
            "\n",
            "valid_text_ind: 1 of 30\n",
            "rec:      0.724138\t pre:      0.656250\t f1:       0.688525\n",
            "\n",
            "valid_text_ind: 2 of 30\n",
            "rec:      0.861111\t pre:      0.738095\t f1:       0.794872\n",
            "\n",
            "valid_text_ind: 3 of 30\n",
            "rec:      0.761905\t pre:      0.551724\t f1:       0.640000\n",
            "\n",
            "valid_text_ind: 4 of 30\n",
            "rec:      0.958333\t pre:      0.958333\t f1:       0.958333\n",
            "\n",
            "valid_text_ind: 5 of 30\n",
            "rec:      0.761905\t pre:      0.615385\t f1:       0.680851\n",
            "\n",
            "valid_text_ind: 6 of 30\n",
            "rec:      0.772727\t pre:      0.653846\t f1:       0.708333\n",
            "\n",
            "valid_text_ind: 7 of 30\n",
            "rec:      0.739130\t pre:      0.653846\t f1:       0.693878\n",
            "\n",
            "valid_text_ind: 8 of 30\n",
            "rec:      0.722222\t pre:      0.604651\t f1:       0.658228\n",
            "\n",
            "valid_text_ind: 9 of 30\n",
            "rec:      0.764706\t pre:      0.838710\t f1:       0.800000\n",
            "\n",
            "valid_text_ind: 10 of 30\n",
            "rec:      0.807692\t pre:      0.750000\t f1:       0.777778\n",
            "\n",
            "valid_text_ind: 11 of 30\n",
            "rec:      0.920000\t pre:      0.696970\t f1:       0.793103\n",
            "\n",
            "valid_text_ind: 12 of 30\n",
            "rec:      0.655172\t pre:      0.678571\t f1:       0.666667\n",
            "\n",
            "valid_text_ind: 13 of 30\n",
            "rec:      0.793103\t pre:      0.851852\t f1:       0.821429\n",
            "\n",
            "valid_text_ind: 14 of 30\n",
            "rec:      0.741935\t pre:      0.676471\t f1:       0.707692\n",
            "\n",
            "valid_text_ind: 15 of 30\n",
            "rec:      0.920000\t pre:      0.718750\t f1:       0.807018\n",
            "\n",
            "valid_text_ind: 16 of 30\n",
            "rec:      0.633333\t pre:      0.863636\t f1:       0.730769\n",
            "\n",
            "valid_text_ind: 17 of 30\n",
            "rec:      0.782609\t pre:      0.642857\t f1:       0.705882\n",
            "\n",
            "valid_text_ind: 18 of 30\n",
            "rec:      0.958333\t pre:      0.589744\t f1:       0.730159\n",
            "\n",
            "valid_text_ind: 19 of 30\n",
            "rec:      0.851852\t pre:      0.605263\t f1:       0.707692\n",
            "\n",
            "valid_text_ind: 20 of 30\n",
            "rec:      0.722222\t pre:      0.787879\t f1:       0.753623\n",
            "\n",
            "valid_text_ind: 21 of 30\n",
            "rec:      0.862069\t pre:      0.806452\t f1:       0.833333\n",
            "\n",
            "valid_text_ind: 22 of 30\n",
            "rec:      0.782609\t pre:      0.600000\t f1:       0.679245\n",
            "\n",
            "valid_text_ind: 23 of 30\n",
            "rec:      0.750000\t pre:      0.576923\t f1:       0.652174\n",
            "\n",
            "valid_text_ind: 24 of 30\n",
            "rec:      0.774194\t pre:      0.750000\t f1:       0.761905\n",
            "\n",
            "valid_text_ind: 25 of 30\n",
            "rec:      0.857143\t pre:      0.937500\t f1:       0.895522\n",
            "\n",
            "valid_text_ind: 26 of 30\n",
            "rec:      0.857143\t pre:      0.774194\t f1:       0.813559\n",
            "\n",
            "valid_text_ind: 27 of 30\n",
            "rec:      0.814815\t pre:      0.647059\t f1:       0.721311\n",
            "\n",
            "valid_text_ind: 28 of 30\n",
            "rec:      0.826087\t pre:      0.593750\t f1:       0.690909\n",
            "\n",
            "valid_text_ind: 29 of 30\n",
            "rec:      0.880000\t pre:      0.578947\t f1:       0.698413\n",
            "\n",
            "\n",
            "-----valid_epoch_end_flag = True-----\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Summary:\n",
            "total_act: 805\t right_act: 643\t tagged_act: 923\n",
            "rec: 0.798758\t pre: 0.696641\t f1: 0.744213\n",
            "\n",
            "cumulative reward: 573907.868750\t average reward: 48.665129\n",
            "\n",
            "当前最高F1为0.745554874310239\n",
            "\n",
            "\n",
            " Training... \n",
            "rec:      0.880000\t pre:      0.578947\t f1:       0.698413\n",
            "max_loss: 219.969238\t min_loss: 219.969238\t avg_loss: 219.969238\n",
            "\n",
            "train_text_ind: 51 of 120\n",
            "rec:      0.818182\t pre:      0.400000\t f1:       0.537313\n",
            "max_loss: 920.233887\t min_loss: 20.945160\t avg_loss: 166.170669\n",
            "\n",
            "train_text_ind: 52 of 120\n",
            "rec:      0.761905\t pre:      0.410256\t f1:       0.533333\n",
            "max_loss: 1171.228638\t min_loss: 20.929157\t avg_loss: 177.595288\n",
            "\n",
            "train_text_ind: 53 of 120\n",
            "rec:      0.782609\t pre:      0.545455\t f1:       0.642857\n",
            "max_loss: 1558.365234\t min_loss: 20.604191\t avg_loss: 173.047813\n",
            "\n",
            "train_text_ind: 54 of 120\n",
            "rec:      0.791667\t pre:      0.475000\t f1:       0.593750\n",
            "max_loss: 1064.627930\t min_loss: 10.847532\t avg_loss: 194.534463\n",
            "\n",
            "train_text_ind: 55 of 120\n",
            "rec:      0.818182\t pre:      0.400000\t f1:       0.537313\n",
            "max_loss: 967.996826\t min_loss: 17.152496\t avg_loss: 184.949729\n",
            "\n",
            "train_text_ind: 56 of 120\n",
            "rec:      0.761905\t pre:      0.516129\t f1:       0.615385\n",
            "max_loss: 906.085632\t min_loss: 19.882759\t avg_loss: 189.970439\n",
            "\n",
            "train_text_ind: 57 of 120\n",
            "rec:      0.782609\t pre:      0.367347\t f1:       0.500000\n",
            "max_loss: 1043.171265\t min_loss: 22.746874\t avg_loss: 174.433868\n",
            "\n",
            "train_text_ind: 58 of 120\n",
            "rec:      0.833333\t pre:      0.454545\t f1:       0.588235\n",
            "max_loss: 1644.827271\t min_loss: 16.333403\t avg_loss: 176.336706\n",
            "\n",
            "train_text_ind: 59 of 120\n",
            "rec:      0.863636\t pre:      0.527778\t f1:       0.655172\n",
            "max_loss: 926.508545\t min_loss: 19.839405\t avg_loss: 186.936425\n",
            "\n",
            "train_text_ind: 60 of 120\n",
            "rec:      0.740741\t pre:      0.454545\t f1:       0.563380\n",
            "max_loss: 1009.528748\t min_loss: 18.595966\t avg_loss: 199.124854\n",
            "\n",
            "train_text_ind: 61 of 120\n",
            "rec:      0.388889\t pre:      0.233333\t f1:       0.291667\n",
            "max_loss: 1037.837402\t min_loss: 17.397545\t avg_loss: 192.070780\n",
            "\n",
            "train_text_ind: 62 of 120\n",
            "rec:      0.720000\t pre:      0.400000\t f1:       0.514286\n",
            "max_loss: 910.628906\t min_loss: 18.925835\t avg_loss: 180.891952\n",
            "\n",
            "train_text_ind: 63 of 120\n",
            "rec:      0.916667\t pre:      0.550000\t f1:       0.687500\n",
            "max_loss: 1050.962646\t min_loss: 17.417627\t avg_loss: 187.750791\n",
            "\n",
            "train_text_ind: 64 of 120\n",
            "rec:      0.826087\t pre:      0.441860\t f1:       0.575758\n",
            "max_loss: 940.432739\t min_loss: 14.631559\t avg_loss: 170.424297\n",
            "\n",
            "train_text_ind: 65 of 120\n",
            "rec:      0.888889\t pre:      0.390244\t f1:       0.542373\n",
            "max_loss: 1028.282715\t min_loss: 15.869142\t avg_loss: 206.836375\n",
            "\n",
            "train_text_ind: 66 of 120\n",
            "rec:      0.903226\t pre:      0.700000\t f1:       0.788732\n",
            "max_loss: 1148.247314\t min_loss: 21.873125\t avg_loss: 192.384704\n",
            "\n",
            "train_text_ind: 67 of 120\n",
            "rec:      0.829787\t pre:      0.573529\t f1:       0.678261\n",
            "max_loss: 2189.998779\t min_loss: 21.578802\t avg_loss: 206.155297\n",
            "\n",
            "train_text_ind: 68 of 120\n",
            "rec:      0.812500\t pre:      0.270833\t f1:       0.406250\n",
            "max_loss: 1097.028564\t min_loss: 18.699493\t avg_loss: 180.362645\n",
            "\n",
            "train_text_ind: 69 of 120\n",
            "rec:      0.906250\t pre:      0.630435\t f1:       0.743590\n",
            "max_loss: 996.326965\t min_loss: 14.181524\t avg_loss: 204.819276\n",
            "\n",
            "train_text_ind: 70 of 120\n",
            "rec:      0.750000\t pre:      0.461538\t f1:       0.571429\n",
            "max_loss: 919.166748\t min_loss: 24.754757\t avg_loss: 204.006540\n",
            "\n",
            "train_text_ind: 71 of 120\n",
            "rec:      0.800000\t pre:      0.324324\t f1:       0.461538\n",
            "max_loss: 1291.047729\t min_loss: 17.426535\t avg_loss: 192.597307\n",
            "\n",
            "train_text_ind: 72 of 120\n",
            "rec:      0.772727\t pre:      0.395349\t f1:       0.523077\n",
            "max_loss: 1394.774048\t min_loss: 18.048061\t avg_loss: 176.948563\n",
            "\n",
            "train_text_ind: 73 of 120\n",
            "rec:      0.781250\t pre:      0.431034\t f1:       0.555556\n",
            "max_loss: 951.950989\t min_loss: 21.600683\t avg_loss: 189.651579\n",
            "\n",
            "train_text_ind: 74 of 120\n",
            "rec:      0.882353\t pre:      0.340909\t f1:       0.491803\n",
            "max_loss: 1566.318481\t min_loss: 19.482826\t avg_loss: 196.914870\n",
            "\n",
            "train_text_ind: 75 of 120\n",
            "rec:      0.777778\t pre:      0.388889\t f1:       0.518519\n",
            "max_loss: 935.740662\t min_loss: 14.250685\t avg_loss: 176.255778\n",
            "\n",
            "train_text_ind: 76 of 120\n",
            "rec:      0.806452\t pre:      0.500000\t f1:       0.617284\n",
            "max_loss: 809.887939\t min_loss: 23.906815\t avg_loss: 167.456667\n",
            "\n",
            "train_text_ind: 77 of 120\n",
            "rec:      0.791667\t pre:      0.513514\t f1:       0.622951\n",
            "max_loss: 885.119019\t min_loss: 18.658812\t avg_loss: 185.090232\n",
            "\n",
            "train_text_ind: 78 of 120\n",
            "rec:      0.821429\t pre:      0.469388\t f1:       0.597403\n",
            "max_loss: 1061.686401\t min_loss: 16.006258\t avg_loss: 194.590861\n",
            "\n",
            "train_text_ind: 79 of 120\n",
            "rec:      0.857143\t pre:      0.535714\t f1:       0.659341\n",
            "max_loss: 1504.420898\t min_loss: 17.570982\t avg_loss: 179.313777\n",
            "\n",
            "train_text_ind: 80 of 120\n",
            "rec:      0.911765\t pre:      0.584906\t f1:       0.712644\n",
            "max_loss: 955.616455\t min_loss: 22.284042\t avg_loss: 176.913938\n",
            "\n",
            "train_text_ind: 81 of 120\n",
            "rec:      0.800000\t pre:      0.516129\t f1:       0.627451\n",
            "max_loss: 1071.727417\t min_loss: 20.407108\t avg_loss: 195.298548\n",
            "\n",
            "train_text_ind: 82 of 120\n",
            "rec:      0.920000\t pre:      0.500000\t f1:       0.647887\n",
            "max_loss: 976.912292\t min_loss: 19.064777\t avg_loss: 203.178231\n",
            "\n",
            "train_text_ind: 83 of 120\n",
            "rec:      0.742857\t pre:      0.500000\t f1:       0.597701\n",
            "max_loss: 1064.584595\t min_loss: 14.659849\t avg_loss: 199.680339\n",
            "\n",
            "train_text_ind: 84 of 120\n",
            "rec:      0.750000\t pre:      0.315789\t f1:       0.444444\n",
            "max_loss: 1133.508301\t min_loss: 11.074163\t avg_loss: 208.398916\n",
            "\n",
            "train_text_ind: 85 of 120\n",
            "rec:      0.892857\t pre:      0.446429\t f1:       0.595238\n",
            "max_loss: 885.314941\t min_loss: 21.745003\t avg_loss: 188.720086\n",
            "\n",
            "train_text_ind: 86 of 120\n",
            "rec:      0.787879\t pre:      0.684211\t f1:       0.732394\n",
            "max_loss: 1033.784424\t min_loss: 18.276791\t avg_loss: 230.867374\n",
            "\n",
            "train_text_ind: 87 of 120\n",
            "rec:      0.757576\t pre:      0.480769\t f1:       0.588235\n",
            "max_loss: 1282.303955\t min_loss: 22.134356\t avg_loss: 230.758898\n",
            "\n",
            "train_text_ind: 88 of 120\n",
            "rec:      0.870968\t pre:      0.500000\t f1:       0.635294\n",
            "max_loss: 1044.652588\t min_loss: 17.157457\t avg_loss: 196.602725\n",
            "\n",
            "train_text_ind: 89 of 120\n",
            "rec:      0.764706\t pre:      0.295455\t f1:       0.426230\n",
            "max_loss: 1124.091919\t min_loss: 18.116661\t avg_loss: 211.565634\n",
            "\n",
            "train_text_ind: 90 of 120\n",
            "rec:      0.838710\t pre:      0.619048\t f1:       0.712329\n",
            "max_loss: 1059.896851\t min_loss: 18.560455\t avg_loss: 205.701569\n",
            "\n",
            "train_text_ind: 91 of 120\n",
            "rec:      0.655172\t pre:      0.463415\t f1:       0.542857\n",
            "max_loss: 1064.931519\t min_loss: 21.107868\t avg_loss: 223.153189\n",
            "\n",
            "train_text_ind: 92 of 120\n",
            "rec:      0.840000\t pre:      0.525000\t f1:       0.646154\n",
            "max_loss: 2010.237427\t min_loss: 15.514987\t avg_loss: 238.840478\n",
            "\n",
            "train_text_ind: 93 of 120\n",
            "rec:      0.882353\t pre:      0.357143\t f1:       0.508475\n",
            "max_loss: 1131.393311\t min_loss: 18.599588\t avg_loss: 215.917369\n",
            "\n",
            "train_text_ind: 94 of 120\n",
            "rec:      0.870968\t pre:      0.482143\t f1:       0.620690\n",
            "max_loss: 1011.661072\t min_loss: 24.044296\t avg_loss: 230.371539\n",
            "\n",
            "train_text_ind: 95 of 120\n",
            "rec:      0.821429\t pre:      0.560976\t f1:       0.666667\n",
            "max_loss: 1035.935669\t min_loss: 24.509296\t avg_loss: 220.459126\n",
            "\n",
            "train_text_ind: 96 of 120\n",
            "rec:      0.842105\t pre:      0.326531\t f1:       0.470588\n",
            "max_loss: 1265.339478\t min_loss: 19.442097\t avg_loss: 225.667956\n",
            "\n",
            "train_text_ind: 97 of 120\n",
            "rec:      0.833333\t pre:      0.416667\t f1:       0.555556\n",
            "max_loss: 1157.805298\t min_loss: 18.849230\t avg_loss: 203.889294\n",
            "\n",
            "train_text_ind: 98 of 120\n",
            "rec:      0.826087\t pre:      0.404255\t f1:       0.542857\n",
            "max_loss: 1020.437622\t min_loss: 14.227566\t avg_loss: 222.600932\n",
            "\n",
            "train_text_ind: 99 of 120\n",
            "rec:      0.884615\t pre:      0.522727\t f1:       0.657143\n",
            "max_loss: 852.338135\t min_loss: 30.529493\t avg_loss: 209.656853\n",
            "\n",
            "train_text_ind: 100 of 120\n",
            "loss list[219.96923828125, 166.17066850185395, 177.59528804044112, 173.04781314469537, 194.53446298599243, 184.9497288790616, 189.97043873359416, 174.43386772632599, 176.33670565605163, 186.93642510890962, 199.12485445976256, 192.07077998161316, 180.891951507282, 187.75079102993013, 170.4242968990378, 206.83637462377547, 192.38470409580918, 206.1552971315384, 180.36264462947847, 204.81927588939666, 204.00654004706354, 192.59730662185302, 176.94856285095216, 189.65157879477016, 196.91487028598786, 176.25577790260314, 167.45666677766448, 185.09023186683655, 194.59086077252803, 179.3137769651413, 176.9139378833771, 195.2985475254059, 203.17823055267334, 199.68033874988555, 208.3989156592382, 188.72008588314057, 230.86737411765642, 230.75889823913573, 196.60272547303055, 211.56563390737153, 205.70156909465788, 223.15318887840291, 238.84047778367997, 215.91736894466797, 230.37153854846954, 220.45912611484528, 225.66795641899108, 203.8892943239212, 222.6009320497513, 209.65685270445266]\n",
            "\n",
            "\n",
            " Testing ...\n",
            "\n",
            "valid_text_ind: 0 of 30\n",
            "rec:      0.846154\t pre:      0.550000\t f1:       0.666667\n",
            "\n",
            "valid_text_ind: 1 of 30\n",
            "rec:      0.709677\t pre:      0.687500\t f1:       0.698413\n",
            "\n",
            "valid_text_ind: 2 of 30\n",
            "rec:      0.918919\t pre:      0.666667\t f1:       0.772727\n",
            "\n",
            "valid_text_ind: 3 of 30\n",
            "rec:      0.800000\t pre:      0.500000\t f1:       0.615385\n",
            "\n",
            "valid_text_ind: 4 of 30\n",
            "rec:      0.920000\t pre:      0.920000\t f1:       0.920000\n",
            "\n",
            "valid_text_ind: 5 of 30\n",
            "rec:      0.900000\t pre:      0.600000\t f1:       0.720000\n",
            "\n",
            "valid_text_ind: 6 of 30\n",
            "rec:      0.772727\t pre:      0.653846\t f1:       0.708333\n",
            "\n",
            "valid_text_ind: 7 of 30\n",
            "rec:      0.708333\t pre:      0.680000\t f1:       0.693878\n",
            "\n",
            "valid_text_ind: 8 of 30\n",
            "rec:      0.837838\t pre:      0.645833\t f1:       0.729412\n",
            "\n",
            "valid_text_ind: 9 of 30\n",
            "rec:      0.806452\t pre:      0.781250\t f1:       0.793651\n",
            "\n",
            "valid_text_ind: 10 of 30\n",
            "rec:      0.807692\t pre:      0.700000\t f1:       0.750000\n",
            "\n",
            "valid_text_ind: 11 of 30\n",
            "rec:      0.840000\t pre:      0.656250\t f1:       0.736842\n",
            "\n",
            "valid_text_ind: 12 of 30\n",
            "rec:      0.600000\t pre:      0.750000\t f1:       0.666667\n",
            "\n",
            "valid_text_ind: 13 of 30\n",
            "rec:      0.821429\t pre:      0.851852\t f1:       0.836364\n",
            "\n",
            "valid_text_ind: 14 of 30\n",
            "rec:      0.677419\t pre:      0.636364\t f1:       0.656250\n",
            "\n",
            "valid_text_ind: 15 of 30\n",
            "rec:      0.925926\t pre:      0.862069\t f1:       0.892857\n",
            "\n",
            "valid_text_ind: 16 of 30\n",
            "rec:      0.645161\t pre:      0.833333\t f1:       0.727273\n",
            "\n",
            "valid_text_ind: 17 of 30\n",
            "rec:      0.916667\t pre:      0.785714\t f1:       0.846154\n",
            "\n",
            "valid_text_ind: 18 of 30\n",
            "rec:      0.958333\t pre:      0.676471\t f1:       0.793103\n",
            "\n",
            "valid_text_ind: 19 of 30\n",
            "rec:      0.740741\t pre:      0.588235\t f1:       0.655738\n",
            "\n",
            "valid_text_ind: 20 of 30\n",
            "rec:      0.675676\t pre:      0.735294\t f1:       0.704225\n",
            "\n",
            "valid_text_ind: 21 of 30\n",
            "rec:      0.933333\t pre:      0.717949\t f1:       0.811594\n",
            "\n",
            "valid_text_ind: 22 of 30\n",
            "rec:      0.869565\t pre:      0.689655\t f1:       0.769231\n",
            "\n",
            "valid_text_ind: 23 of 30\n",
            "rec:      0.600000\t pre:      0.500000\t f1:       0.545455\n",
            "\n",
            "valid_text_ind: 24 of 30\n",
            "rec:      0.862069\t pre:      0.781250\t f1:       0.819672\n",
            "\n",
            "valid_text_ind: 25 of 30\n",
            "rec:      0.833333\t pre:      0.857143\t f1:       0.845070\n",
            "\n",
            "valid_text_ind: 26 of 30\n",
            "rec:      0.857143\t pre:      0.750000\t f1:       0.800000\n",
            "\n",
            "valid_text_ind: 27 of 30\n",
            "rec:      0.814815\t pre:      0.814815\t f1:       0.814815\n",
            "\n",
            "valid_text_ind: 28 of 30\n",
            "rec:      0.869565\t pre:      0.666667\t f1:       0.754717\n",
            "\n",
            "valid_text_ind: 29 of 30\n",
            "rec:      0.960000\t pre:      0.666667\t f1:       0.786885\n",
            "\n",
            "\n",
            "-----valid_epoch_end_flag = True-----\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Summary:\n",
            "total_act: 811\t right_act: 658\t tagged_act: 934\n",
            "rec: 0.811344\t pre: 0.704497\t f1: 0.754155\n",
            "\n",
            "cumulative reward: 575774.892188\t average reward: 48.823445\n",
            "\n",
            "Saved weights to /content/drive/My Drive/EASDRL/model/wikihow_act_5_fold3.h5 ...\n",
            "\n",
            "\n",
            " Best f1 score: {'rec': [0.0, 0.556135770234987, 0.706700379266751, 0.7858910891089109, 0.7656641604010025, 0.7686472819216182, 0.8113440197287299], 'pre': [0.0, 0.7689530685920578, 0.718508997429306, 0.6955093099671413, 0.7137850467289719, 0.7238095238095238, 0.7044967880085653], 'f1': [0.0, 0.6454545454545455, 0.7125557680050988, 0.7379430563625798, 0.7388149939540508, 0.745554874310239, 0.7541547277936963], 'rw': [0.0, 46.329655303993974, 47.81808461587398, 48.44942987895383, 48.405943674213184, 48.483599645976454, 48.823445449631166]}  best epoch: 2\n",
            "\n",
            "\n",
            "\n",
            " Training... \n",
            "rec:      0.920000\t pre:      0.657143\t f1:       0.766667\n",
            "max_loss: 357.009491\t min_loss: 357.009491\t avg_loss: 357.009491\n",
            "\n",
            "train_text_ind: 101 of 120\n",
            "rec:      0.812500\t pre:      0.342105\t f1:       0.481481\n",
            "max_loss: 1153.162842\t min_loss: 24.494717\t avg_loss: 236.922022\n",
            "\n",
            "train_text_ind: 102 of 120\n",
            "rec:      0.875000\t pre:      0.677419\t f1:       0.763636\n",
            "max_loss: 1119.480225\t min_loss: 35.763840\t avg_loss: 303.070870\n",
            "\n",
            "train_text_ind: 103 of 120\n",
            "rec:      0.840000\t pre:      0.368421\t f1:       0.512195\n",
            "max_loss: 1172.758301\t min_loss: 17.373173\t avg_loss: 254.526686\n",
            "\n",
            "train_text_ind: 104 of 120\n",
            "rec:      0.916667\t pre:      0.440000\t f1:       0.594595\n",
            "max_loss: 1464.389404\t min_loss: 28.977186\t avg_loss: 297.890493\n",
            "\n",
            "train_text_ind: 105 of 120\n",
            "rec:      0.909091\t pre:      0.612245\t f1:       0.731707\n",
            "max_loss: 1911.529907\t min_loss: 34.474072\t avg_loss: 348.281188\n",
            "\n",
            "train_text_ind: 106 of 120\n",
            "rec:      0.800000\t pre:      0.454545\t f1:       0.579710\n",
            "max_loss: 1212.182373\t min_loss: 29.881142\t avg_loss: 321.005134\n",
            "\n",
            "train_text_ind: 107 of 120\n",
            "rec:      0.900000\t pre:      0.473684\t f1:       0.620690\n",
            "max_loss: 1041.343506\t min_loss: 17.996098\t avg_loss: 307.159680\n",
            "\n",
            "train_text_ind: 108 of 120\n",
            "rec:      0.828571\t pre:      0.630435\t f1:       0.716049\n",
            "max_loss: 1647.719360\t min_loss: 22.557972\t avg_loss: 286.962168\n",
            "\n",
            "train_text_ind: 109 of 120\n",
            "rec:      0.937500\t pre:      0.441176\t f1:       0.600000\n",
            "max_loss: 1290.115234\t min_loss: 26.530590\t avg_loss: 283.687719\n",
            "\n",
            "train_text_ind: 110 of 120\n",
            "rec:      0.705882\t pre:      0.400000\t f1:       0.510638\n",
            "max_loss: 1930.809570\t min_loss: 22.093782\t avg_loss: 265.914261\n",
            "\n",
            "train_text_ind: 111 of 120\n",
            "rec:      0.736842\t pre:      0.358974\t f1:       0.482759\n",
            "max_loss: 1010.850708\t min_loss: 30.396904\t avg_loss: 277.814479\n",
            "\n",
            "train_text_ind: 112 of 120\n",
            "rec:      0.736842\t pre:      0.424242\t f1:       0.538462\n",
            "max_loss: 1122.064575\t min_loss: 19.829239\t avg_loss: 254.961831\n",
            "\n",
            "train_text_ind: 113 of 120\n",
            "rec:      0.826087\t pre:      0.487179\t f1:       0.612903\n",
            "max_loss: 1457.760986\t min_loss: 27.103771\t avg_loss: 274.918515\n",
            "\n",
            "train_text_ind: 114 of 120\n",
            "rec:      0.615385\t pre:      0.333333\t f1:       0.432432\n",
            "max_loss: 1341.935059\t min_loss: 27.926058\t avg_loss: 258.752280\n",
            "\n",
            "train_text_ind: 115 of 120\n",
            "rec:      0.727273\t pre:      0.372093\t f1:       0.492308\n",
            "max_loss: 1041.945068\t min_loss: 25.149574\t avg_loss: 262.485544\n",
            "\n",
            "train_text_ind: 116 of 120\n",
            "rec:      0.909091\t pre:      0.333333\t f1:       0.487805\n",
            "max_loss: 1637.971680\t min_loss: 15.475640\t avg_loss: 236.126994\n",
            "\n",
            "train_text_ind: 117 of 120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8uE5uTF-ksS"
      },
      "source": [
        "import gensim\n",
        "import re\n",
        "model=gensim.models.KeyedVectors.load_word2vec_format('/content/drive/My Drive/EASDRL/data/wordvec_dim50',binary=True)\n",
        "import pickle\n",
        "with open(\"/content/drive/My Drive/EASDRL/Glove_emb/cooking_word2ids.pkl\",\"rb\") as f:\n",
        "    dic=pickle.load(f)\n",
        "word_list=list(dic.keys())\n",
        "word_matrix=[]\n",
        "\n",
        "for word in word_list:\n",
        "    word=re.sub(u'\\(','',word)\n",
        "    word=re.sub(u'\\)','',word)\n",
        "    if word not in model.vocab:\n",
        "        word_matrix.append([0]*50)\n",
        "    else:\n",
        "        word_matrix.append(list(model[word]))\n",
        "f = open(\"/content/drive/My Drive/EASDRL/data/wordvec_dim50_matrix.pkl\",\"wb\")\n",
        "pickle.dump(word_matrix,f)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}